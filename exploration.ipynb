{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General\n",
    "Let's start with the basics:\n",
    "Given a function that we know has a compositionally sparse representation S(x), define a dense (fully-connected) neural network F(x). Then the optimal solution of this dense NN would be sparse. \n",
    "We will focus on SGD at the moment. \n",
    "\n",
    "Davide has shown that when the teacher and student have the same activation function, it can be learned well.\n",
    "\n",
    "### Goal\n",
    "Familiarize myself with the concept of learning compsitionally sparse function of FCNN and SNN. Duplicate the preliminary results from Davide that L1 induces sparsity which in some way helps. \n",
    "\n",
    "### What do we expect and want to achieve?\n",
    "- Show that when using L1, the dense model achieves sparsity.\n",
    "\n",
    "### TODO\n",
    "- automate to see the results for different values of weight decay \n",
    "\n",
    "### Open Questions\n",
    "- What network structure to use for the teacher? CNN or sparse connected network?\n",
    "- What happens in teacher and student have different activation function?\n",
    "- Role of Initialization\n",
    "- Compute the absolute difference or not? If one whole layer is flipped  and activation function is symmetric, it's still kinda correct, no?\n",
    "\n",
    "### Findings\n",
    "- It does seem like L1 norm is better, also the bigger the normalizing value, the smaller the difference between L1 and L2 -> don't see this anymore\n",
    "- Now it seemsl like L2 is better, test low does get quite low\n",
    "- Use direct output, no binary output -> binary output doesn't give a lot of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7f22f1a990>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "TEST_SET_SIZE = 1024\n",
    "BATCH_SIZE = 8\n",
    "PATIENCE = 20  # Early stopping patience\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseNN Teacher and Student, both have exact same architecture\n",
    "\n",
    "Define a sparse connected network as the \"teacher\" that generated a Dataset X, y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target function parameters:\n",
      "[[[ 2.59 -2.83  0.87]]]\n",
      "[[[-1.38  1.29]]]\n",
      "[[[ 0.86 -0.84]]]\n"
     ]
    }
   ],
   "source": [
    "class SparseCNN_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SparseCNN_1, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=3, padding=0, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2, padding=0, bias = False)\n",
    "        self.conv3 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2, padding=0, bias = False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for CNN\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = torch.tanh(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "teacher_model = SparseCNN_1()\n",
    "\n",
    "with torch.no_grad():\n",
    "    teacher_model.conv1.weight.copy_(torch.tensor([[[2.59, -2.83, 0.87]]]))\n",
    "    teacher_model.conv2.weight.copy_(torch.tensor([[[-1.38, 1.29]]]))\n",
    "    teacher_model.conv3.weight.copy_(torch.tensor([[[0.86, -0.84]]]))\n",
    "\n",
    "print(\"\\nTarget function parameters:\")\n",
    "for param in teacher_model.parameters():\n",
    "    print(param.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Teacher CNN to generate a new dataset\n",
    "X_generated = torch.tensor(np.random.randn(TEST_SET_SIZE, 12), dtype=torch.float32)\n",
    "y_generated = teacher_model(X_generated).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the student with the exact same structure and train it on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_model(model, X_train, y_train, optimizer, loss_fn, l1_lambda=0, batch_size=32):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Create a DataLoader to handle batching and shuffling\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(10000):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_X)\n",
    "            loss = loss_fn(y_pred, batch_y)\n",
    "\n",
    "            # Apply L1 regularization if l1_lambda > 0\n",
    "            if l1_lambda > 0:\n",
    "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss += l1_norm * l1_lambda\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()  # Accumulate batch loss\n",
    "\n",
    "        # Compute average loss for the epoch\n",
    "        epoch_loss /= len(dataloader)\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= 20:\n",
    "                print(f\"Early stopping at epoch {epoch}, best loss: {best_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    return model, best_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with SGD, Student and Teacher identical, no Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target function parameters:\n",
      "[[[ 2.59 -2.83  0.87]]]\n",
      "[[[-1.38  1.29]]]\n",
      "[[[ 0.86 -0.84]]]\n",
      "\n",
      "Student function parameters BEFORE training:\n",
      "[[[ 0.33906782  0.50895905 -0.4235604 ]]]\n",
      "[[[0.61461455 0.13234162]]]\n",
      "[[[0.5224168  0.09576386]]]\n",
      "Epoch 0, Loss: 0.2719\n",
      "Early stopping at epoch 367, best loss: 0.0000\n",
      "\n",
      "Student function parameters AFTER training:\n",
      "[[[-2.5899653  2.8299623 -0.8699878]]]\n",
      "[[[ 1.3799883 -1.2899888]]]\n",
      "[[[ 0.860001   -0.84000134]]]\n"
     ]
    }
   ],
   "source": [
    "student_model = SparseCNN_1()\n",
    "#student_model.apply(init_weights)\n",
    "\n",
    "print(\"\\nTarget function parameters:\")\n",
    "for param in teacher_model.parameters():\n",
    "    print(param.data.numpy())\n",
    "\n",
    "print(\"\\nStudent function parameters BEFORE training:\")\n",
    "for param in student_model.parameters():\n",
    "    print(param.data.numpy())\n",
    "\n",
    "sgd_optim = optim.SGD(student_model.parameters(), lr=0.05, momentum=0.9)\n",
    "loss_fn = nn.MSELoss()\n",
    "student_model, final_loss = train_model(student_model, X_train=X_generated, y_train=y_generated, optimizer=sgd_optim, loss_fn=loss_fn)\n",
    "\n",
    "print(\"\\nStudent function parameters AFTER training:\")\n",
    "for param in student_model.parameters():\n",
    "    print(param.data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with SGD, Student and Teacher identical, L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target function parameters:\n",
      "[[[ 2.59 -2.83  0.87]]]\n",
      "[[[-1.38  1.29]]]\n",
      "[[[ 0.86 -0.84]]]\n",
      "\n",
      "Student function parameters BEFORE training:\n",
      "[[[ 0.27839142 -0.08151665  0.4450711 ]]]\n",
      "[[[ 0.10451669 -0.33010566]]]\n",
      "[[[ 0.18024033 -0.32579   ]]]\n",
      "Epoch 0, Loss: 0.4138\n",
      "Epoch 100, Loss: 0.0215\n",
      "Epoch 200, Loss: 0.0050\n",
      "Epoch 300, Loss: 0.0028\n",
      "Epoch 400, Loss: 0.0017\n",
      "Epoch 500, Loss: 0.0012\n",
      "Epoch 600, Loss: 0.0009\n",
      "Epoch 700, Loss: 0.0006\n",
      "Epoch 800, Loss: 0.0005\n",
      "Epoch 900, Loss: 0.0004\n",
      "Epoch 1000, Loss: 0.0003\n",
      "Epoch 1100, Loss: 0.0003\n",
      "Epoch 1200, Loss: 0.0002\n",
      "Epoch 1300, Loss: 0.0002\n",
      "Epoch 1400, Loss: 0.0002\n",
      "Epoch 1500, Loss: 0.0002\n",
      "Epoch 1600, Loss: 0.0002\n",
      "Epoch 1700, Loss: 0.0001\n",
      "Epoch 1800, Loss: 0.0001\n",
      "Epoch 1900, Loss: 0.0001\n",
      "Epoch 2000, Loss: 0.0001\n",
      "Epoch 2100, Loss: 0.0001\n",
      "Epoch 2200, Loss: 0.0001\n",
      "Epoch 2300, Loss: 0.0001\n",
      "Epoch 2400, Loss: 0.0001\n",
      "Epoch 2500, Loss: 0.0001\n",
      "Epoch 2600, Loss: 0.0001\n",
      "Epoch 2700, Loss: 0.0001\n",
      "Epoch 2800, Loss: 0.0001\n",
      "Epoch 2900, Loss: 0.0001\n",
      "Epoch 3000, Loss: 0.0001\n",
      "Epoch 3100, Loss: 0.0001\n",
      "Epoch 3200, Loss: 0.0001\n",
      "Epoch 3300, Loss: 0.0001\n",
      "Epoch 3400, Loss: 0.0001\n",
      "Epoch 3500, Loss: 0.0001\n",
      "Epoch 3600, Loss: 0.0001\n",
      "Epoch 3700, Loss: 0.0001\n",
      "Epoch 3800, Loss: 0.0001\n",
      "Epoch 3900, Loss: 0.0001\n",
      "Epoch 4000, Loss: 0.0001\n",
      "Epoch 4100, Loss: 0.0001\n",
      "Epoch 4200, Loss: 0.0001\n",
      "Epoch 4300, Loss: 0.0001\n",
      "Epoch 4400, Loss: 0.0001\n",
      "Epoch 4500, Loss: 0.0001\n",
      "Epoch 4600, Loss: 0.0001\n",
      "Epoch 4700, Loss: 0.0001\n",
      "Epoch 4800, Loss: 0.0001\n",
      "Epoch 4900, Loss: 0.0001\n",
      "Epoch 5000, Loss: 0.0001\n",
      "Epoch 5100, Loss: 0.0001\n",
      "Epoch 5200, Loss: 0.0001\n",
      "Epoch 5300, Loss: 0.0001\n",
      "Epoch 5400, Loss: 0.0001\n",
      "Epoch 5500, Loss: 0.0001\n",
      "Epoch 5600, Loss: 0.0001\n",
      "Epoch 5700, Loss: 0.0001\n",
      "Epoch 5800, Loss: 0.0001\n",
      "Epoch 5900, Loss: 0.0001\n",
      "Epoch 6000, Loss: 0.0001\n",
      "Epoch 6100, Loss: 0.0001\n",
      "Epoch 6200, Loss: 0.0001\n",
      "Epoch 6300, Loss: 0.0001\n",
      "Epoch 6400, Loss: 0.0001\n",
      "Epoch 6500, Loss: 0.0001\n",
      "Epoch 6600, Loss: 0.0001\n",
      "Epoch 6700, Loss: 0.0001\n",
      "Epoch 6800, Loss: 0.0001\n",
      "Epoch 6900, Loss: 0.0001\n",
      "Epoch 7000, Loss: 0.0001\n",
      "Epoch 7100, Loss: 0.0001\n",
      "Early stopping at epoch 7176, best loss: 0.0001\n",
      "\n",
      "Student function parameters AFTER training:\n",
      "[[[ 2.583873  -2.8233025  0.8678153]]]\n",
      "[[[-1.3768517  1.2869046]]]\n",
      "[[[ 0.8602242  -0.84023046]]]\n"
     ]
    }
   ],
   "source": [
    "student_model = SparseCNN_1()\n",
    "#student_model.apply(init_weights)\n",
    "\n",
    "print(\"\\nTarget function parameters:\")\n",
    "for param in teacher_model.parameters():\n",
    "    print(param.data.numpy())\n",
    "\n",
    "print(\"\\nStudent function parameters BEFORE training:\")\n",
    "for param in student_model.parameters():\n",
    "    print(param.data.numpy())\n",
    "\n",
    "sgd_optim = optim.SGD(student_model.parameters(), lr=0.05, momentum=0.9)\n",
    "loss_fn = nn.MSELoss()\n",
    "student_model, final_loss = train_model(student_model, X_train=X_generated, y_train=y_generated, optimizer=sgd_optim, loss_fn=loss_fn, l1_lambda=1e-5)\n",
    "\n",
    "print(\"\\nStudent function parameters AFTER training:\")\n",
    "for param in student_model.parameters():\n",
    "    print(param.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance metric between teacher and student model parameters: 0.0141\n"
     ]
    }
   ],
   "source": [
    "# Compute distance metric: how different are the entries of the teacher and student parameters\n",
    "def compute_distance_metric(teacher_model, student_model):\n",
    "    distance = 0.0\n",
    "    for teacher_param, student_param in zip(teacher_model.parameters(), student_model.parameters()):\n",
    "        distance += torch.norm(torch.abs(teacher_param) - torch.abs(student_param)).item() #TODO: maybe not do absolute values / make this smarter\n",
    "    return distance\n",
    "\n",
    "distance_metric = compute_distance_metric(teacher_model, student_model)\n",
    "print(f\"Distance metric between teacher and student model parameters: {distance_metric:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3855\n",
      "Epoch 100, Loss: 0.0089\n",
      "Epoch 200, Loss: 0.0038\n",
      "Epoch 300, Loss: 0.0022\n",
      "Epoch 400, Loss: 0.0015\n",
      "Epoch 500, Loss: 0.0010\n",
      "Epoch 600, Loss: 0.0008\n",
      "Epoch 700, Loss: 0.0006\n",
      "Epoch 800, Loss: 0.0005\n",
      "Epoch 900, Loss: 0.0004\n",
      "Epoch 1000, Loss: 0.0003\n",
      "Epoch 1100, Loss: 0.0003\n",
      "Epoch 1200, Loss: 0.0002\n",
      "Epoch 1300, Loss: 0.0002\n",
      "Epoch 1400, Loss: 0.0002\n",
      "Epoch 1500, Loss: 0.0002\n",
      "Epoch 1600, Loss: 0.0001\n",
      "Epoch 1700, Loss: 0.0001\n",
      "Epoch 1800, Loss: 0.0001\n",
      "Epoch 1900, Loss: 0.0001\n",
      "Epoch 2000, Loss: 0.0001\n",
      "Epoch 2100, Loss: 0.0001\n",
      "Epoch 2200, Loss: 0.0001\n",
      "Epoch 2300, Loss: 0.0001\n",
      "Epoch 2400, Loss: 0.0001\n",
      "Epoch 2500, Loss: 0.0001\n",
      "Epoch 2600, Loss: 0.0001\n",
      "Epoch 2700, Loss: 0.0001\n",
      "Epoch 2800, Loss: 0.0001\n",
      "Epoch 2900, Loss: 0.0001\n",
      "Epoch 3000, Loss: 0.0001\n",
      "Epoch 3100, Loss: 0.0001\n",
      "Epoch 3200, Loss: 0.0001\n",
      "Epoch 3300, Loss: 0.0001\n",
      "Epoch 3400, Loss: 0.0001\n",
      "Epoch 3500, Loss: 0.0001\n",
      "Epoch 3600, Loss: 0.0001\n",
      "Epoch 3700, Loss: 0.0001\n",
      "Epoch 3800, Loss: 0.0001\n",
      "Epoch 3900, Loss: 0.0001\n",
      "Epoch 4000, Loss: 0.0001\n",
      "Epoch 4100, Loss: 0.0001\n",
      "Epoch 4200, Loss: 0.0001\n",
      "Epoch 4300, Loss: 0.0001\n",
      "Epoch 4400, Loss: 0.0001\n",
      "Epoch 4500, Loss: 0.0001\n",
      "Epoch 4600, Loss: 0.0001\n",
      "Epoch 4700, Loss: 0.0001\n",
      "Epoch 4800, Loss: 0.0001\n",
      "Epoch 4900, Loss: 0.0001\n",
      "Epoch 5000, Loss: 0.0001\n",
      "Epoch 5100, Loss: 0.0001\n",
      "Epoch 5200, Loss: 0.0001\n",
      "Epoch 5300, Loss: 0.0001\n",
      "Epoch 5400, Loss: 0.0001\n",
      "Epoch 5500, Loss: 0.0001\n",
      "Epoch 5600, Loss: 0.0001\n",
      "Epoch 5700, Loss: 0.0001\n",
      "Epoch 5800, Loss: 0.0001\n",
      "Epoch 5900, Loss: 0.0001\n",
      "Epoch 6000, Loss: 0.0001\n",
      "Epoch 6100, Loss: 0.0001\n",
      "Epoch 6200, Loss: 0.0001\n",
      "Epoch 6300, Loss: 0.0001\n",
      "Epoch 6400, Loss: 0.0001\n",
      "Epoch 6500, Loss: 0.0001\n",
      "Epoch 6600, Loss: 0.0001\n",
      "Epoch 6700, Loss: 0.0001\n",
      "Epoch 6800, Loss: 0.0001\n",
      "Epoch 6900, Loss: 0.0001\n",
      "Epoch 7000, Loss: 0.0001\n",
      "Epoch 7100, Loss: 0.0001\n",
      "Early stopping at epoch 7133, best loss: 0.0001\n",
      "Epoch 0, Loss: 0.3152\n",
      "Epoch 100, Loss: 0.0068\n",
      "Epoch 200, Loss: 0.0034\n",
      "Epoch 300, Loss: 0.0021\n",
      "Epoch 400, Loss: 0.0013\n",
      "Epoch 500, Loss: 0.0009\n",
      "Epoch 600, Loss: 0.0006\n",
      "Epoch 700, Loss: 0.0005\n",
      "Epoch 800, Loss: 0.0003\n",
      "Epoch 900, Loss: 0.0003\n",
      "Epoch 1000, Loss: 0.0002\n",
      "Epoch 1100, Loss: 0.0002\n",
      "Epoch 1200, Loss: 0.0001\n",
      "Epoch 1300, Loss: 0.0001\n",
      "Epoch 1400, Loss: 0.0001\n",
      "Epoch 1500, Loss: 0.0001\n",
      "Epoch 1600, Loss: 0.0000\n",
      "Epoch 1700, Loss: 0.0000\n",
      "Epoch 1800, Loss: 0.0000\n",
      "Epoch 1900, Loss: 0.0000\n",
      "Epoch 2000, Loss: 0.0000\n",
      "Epoch 2100, Loss: 0.0000\n",
      "Epoch 2200, Loss: 0.0000\n",
      "Epoch 2300, Loss: 0.0000\n",
      "Epoch 2400, Loss: 0.0000\n",
      "Epoch 2500, Loss: 0.0000\n",
      "Epoch 2600, Loss: 0.0000\n",
      "Epoch 2700, Loss: 0.0000\n",
      "Epoch 2800, Loss: 0.0000\n",
      "Epoch 2900, Loss: 0.0000\n",
      "Epoch 3000, Loss: 0.0000\n",
      "Epoch 3100, Loss: 0.0000\n",
      "Epoch 3200, Loss: 0.0000\n",
      "Epoch 3300, Loss: 0.0000\n",
      "Epoch 3400, Loss: 0.0000\n",
      "Epoch 3500, Loss: 0.0000\n",
      "Epoch 3600, Loss: 0.0000\n",
      "Epoch 3700, Loss: 0.0000\n",
      "Epoch 3800, Loss: 0.0000\n",
      "Epoch 3900, Loss: 0.0000\n",
      "Epoch 4000, Loss: 0.0000\n",
      "Epoch 4100, Loss: 0.0000\n",
      "Epoch 4200, Loss: 0.0000\n",
      "Epoch 4300, Loss: 0.0000\n",
      "Epoch 4400, Loss: 0.0000\n",
      "Epoch 4500, Loss: 0.0000\n",
      "Epoch 4600, Loss: 0.0000\n",
      "Epoch 4700, Loss: 0.0000\n",
      "Epoch 4800, Loss: 0.0000\n",
      "Epoch 4900, Loss: 0.0000\n",
      "Epoch 5000, Loss: 0.0000\n",
      "Epoch 5100, Loss: 0.0000\n",
      "Epoch 5200, Loss: 0.0000\n",
      "Epoch 5300, Loss: 0.0000\n",
      "Epoch 5400, Loss: 0.0000\n",
      "Epoch 5500, Loss: 0.0000\n",
      "Epoch 5600, Loss: 0.0000\n",
      "Epoch 5700, Loss: 0.0000\n",
      "Epoch 5800, Loss: 0.0000\n",
      "Epoch 5900, Loss: 0.0000\n",
      "Epoch 6000, Loss: 0.0000\n",
      "Epoch 6100, Loss: 0.0000\n",
      "Epoch 6200, Loss: 0.0000\n",
      "Epoch 6300, Loss: 0.0000\n",
      "Epoch 6400, Loss: 0.0000\n",
      "Epoch 6500, Loss: 0.0000\n",
      "Epoch 6600, Loss: 0.0000\n",
      "Epoch 6700, Loss: 0.0000\n",
      "Epoch 6800, Loss: 0.0000\n",
      "Epoch 6900, Loss: 0.0000\n",
      "Epoch 7000, Loss: 0.0000\n",
      "Epoch 7100, Loss: 0.0000\n",
      "Epoch 7200, Loss: 0.0000\n",
      "Epoch 7300, Loss: 0.0000\n",
      "Epoch 7400, Loss: 0.0000\n",
      "Epoch 7500, Loss: 0.0000\n",
      "Epoch 7600, Loss: 0.0000\n",
      "Epoch 7700, Loss: 0.0000\n",
      "Epoch 7800, Loss: 0.0000\n",
      "Epoch 7900, Loss: 0.0000\n",
      "Epoch 8000, Loss: 0.0000\n",
      "Epoch 8100, Loss: 0.0000\n",
      "Epoch 8200, Loss: 0.0000\n",
      "Early stopping at epoch 8213, best loss: 0.0000\n",
      "Epoch 0, Loss: 0.3929\n",
      "Epoch 100, Loss: 0.0156\n",
      "Epoch 200, Loss: 0.0045\n",
      "Epoch 300, Loss: 0.0026\n",
      "Epoch 400, Loss: 0.0017\n",
      "Epoch 500, Loss: 0.0011\n",
      "Epoch 600, Loss: 0.0008\n",
      "Epoch 700, Loss: 0.0006\n",
      "Epoch 800, Loss: 0.0005\n",
      "Epoch 900, Loss: 0.0004\n",
      "Epoch 1000, Loss: 0.0003\n",
      "Epoch 1100, Loss: 0.0003\n",
      "Epoch 1200, Loss: 0.0002\n",
      "Epoch 1300, Loss: 0.0002\n",
      "Epoch 1400, Loss: 0.0002\n",
      "Epoch 1500, Loss: 0.0002\n",
      "Epoch 1600, Loss: 0.0002\n",
      "Epoch 1700, Loss: 0.0001\n",
      "Epoch 1800, Loss: 0.0001\n",
      "Epoch 1900, Loss: 0.0001\n",
      "Epoch 2000, Loss: 0.0001\n",
      "Epoch 2100, Loss: 0.0001\n",
      "Epoch 2200, Loss: 0.0001\n",
      "Epoch 2300, Loss: 0.0001\n",
      "Epoch 2400, Loss: 0.0001\n",
      "Epoch 2500, Loss: 0.0001\n",
      "Epoch 2600, Loss: 0.0001\n",
      "Epoch 2700, Loss: 0.0001\n",
      "Epoch 2800, Loss: 0.0001\n",
      "Epoch 2900, Loss: 0.0001\n",
      "Epoch 3000, Loss: 0.0001\n",
      "Epoch 3100, Loss: 0.0001\n",
      "Epoch 3200, Loss: 0.0001\n",
      "Epoch 3300, Loss: 0.0001\n",
      "Epoch 3400, Loss: 0.0001\n",
      "Epoch 3500, Loss: 0.0001\n",
      "Epoch 3600, Loss: 0.0001\n",
      "Epoch 3700, Loss: 0.0001\n",
      "Epoch 3800, Loss: 0.0001\n",
      "Epoch 3900, Loss: 0.0001\n",
      "Epoch 4000, Loss: 0.0001\n",
      "Epoch 4100, Loss: 0.0001\n",
      "Epoch 4200, Loss: 0.0001\n",
      "Epoch 4300, Loss: 0.0001\n",
      "Epoch 4400, Loss: 0.0001\n",
      "Epoch 4500, Loss: 0.0001\n",
      "Epoch 4600, Loss: 0.0001\n",
      "Epoch 4700, Loss: 0.0001\n",
      "Epoch 4800, Loss: 0.0001\n",
      "Epoch 4900, Loss: 0.0001\n",
      "Epoch 5000, Loss: 0.0001\n",
      "Epoch 5100, Loss: 0.0001\n",
      "Epoch 5200, Loss: 0.0001\n",
      "Epoch 5300, Loss: 0.0001\n",
      "Epoch 5400, Loss: 0.0001\n",
      "Epoch 5500, Loss: 0.0001\n",
      "Epoch 5600, Loss: 0.0001\n",
      "Epoch 5700, Loss: 0.0001\n",
      "Epoch 5800, Loss: 0.0001\n",
      "Epoch 5900, Loss: 0.0001\n",
      "Epoch 6000, Loss: 0.0001\n",
      "Epoch 6100, Loss: 0.0001\n",
      "Epoch 6200, Loss: 0.0001\n",
      "Epoch 6300, Loss: 0.0001\n",
      "Epoch 6400, Loss: 0.0001\n",
      "Epoch 6500, Loss: 0.0001\n",
      "Epoch 6600, Loss: 0.0001\n",
      "Epoch 6700, Loss: 0.0001\n",
      "Epoch 6800, Loss: 0.0001\n",
      "Epoch 6900, Loss: 0.0001\n",
      "Epoch 7000, Loss: 0.0001\n",
      "Epoch 7100, Loss: 0.0001\n",
      "Epoch 7200, Loss: 0.0001\n",
      "Epoch 7300, Loss: 0.0001\n",
      "Early stopping at epoch 7314, best loss: 0.0001\n",
      "Epoch 0, Loss: 0.3776\n",
      "Epoch 100, Loss: 0.0072\n",
      "Epoch 200, Loss: 0.0036\n",
      "Epoch 300, Loss: 0.0021\n",
      "Epoch 400, Loss: 0.0014\n",
      "Epoch 500, Loss: 0.0009\n",
      "Epoch 600, Loss: 0.0007\n",
      "Epoch 700, Loss: 0.0005\n",
      "Epoch 800, Loss: 0.0004\n",
      "Epoch 900, Loss: 0.0003\n",
      "Epoch 1000, Loss: 0.0002\n",
      "Epoch 1100, Loss: 0.0002\n",
      "Epoch 1200, Loss: 0.0001\n",
      "Epoch 1300, Loss: 0.0001\n",
      "Epoch 1400, Loss: 0.0001\n",
      "Epoch 1500, Loss: 0.0001\n",
      "Epoch 1600, Loss: 0.0000\n",
      "Epoch 1700, Loss: 0.0000\n",
      "Epoch 1800, Loss: 0.0000\n",
      "Epoch 1900, Loss: 0.0000\n",
      "Epoch 2000, Loss: 0.0000\n",
      "Epoch 2100, Loss: 0.0000\n",
      "Epoch 2200, Loss: 0.0000\n",
      "Epoch 2300, Loss: 0.0000\n",
      "Epoch 2400, Loss: 0.0000\n",
      "Epoch 2500, Loss: 0.0000\n",
      "Epoch 2600, Loss: 0.0000\n",
      "Epoch 2700, Loss: 0.0000\n",
      "Epoch 2800, Loss: 0.0000\n",
      "Epoch 2900, Loss: 0.0000\n",
      "Epoch 3000, Loss: 0.0000\n",
      "Epoch 3100, Loss: 0.0000\n",
      "Epoch 3200, Loss: 0.0000\n",
      "Epoch 3300, Loss: 0.0000\n",
      "Epoch 3400, Loss: 0.0000\n",
      "Epoch 3500, Loss: 0.0000\n",
      "Epoch 3600, Loss: 0.0000\n",
      "Epoch 3700, Loss: 0.0000\n",
      "Epoch 3800, Loss: 0.0000\n",
      "Epoch 3900, Loss: 0.0000\n",
      "Epoch 4000, Loss: 0.0000\n",
      "Epoch 4100, Loss: 0.0000\n",
      "Epoch 4200, Loss: 0.0000\n",
      "Epoch 4300, Loss: 0.0000\n",
      "Epoch 4400, Loss: 0.0000\n",
      "Epoch 4500, Loss: 0.0000\n",
      "Epoch 4600, Loss: 0.0000\n",
      "Epoch 4700, Loss: 0.0000\n",
      "Epoch 4800, Loss: 0.0000\n",
      "Epoch 4900, Loss: 0.0000\n",
      "Epoch 5000, Loss: 0.0000\n",
      "Epoch 5100, Loss: 0.0000\n",
      "Epoch 5200, Loss: 0.0000\n",
      "Epoch 5300, Loss: 0.0000\n",
      "Epoch 5400, Loss: 0.0000\n",
      "Epoch 5500, Loss: 0.0000\n",
      "Epoch 5600, Loss: 0.0000\n",
      "Epoch 5700, Loss: 0.0000\n",
      "Epoch 5800, Loss: 0.0000\n",
      "Epoch 5900, Loss: 0.0000\n",
      "Epoch 6000, Loss: 0.0000\n",
      "Epoch 6100, Loss: 0.0000\n",
      "Epoch 6200, Loss: 0.0000\n",
      "Epoch 6300, Loss: 0.0000\n",
      "Epoch 6400, Loss: 0.0000\n",
      "Epoch 6500, Loss: 0.0000\n",
      "Epoch 6600, Loss: 0.0000\n",
      "Epoch 6700, Loss: 0.0000\n",
      "Epoch 6800, Loss: 0.0000\n",
      "Epoch 6900, Loss: 0.0000\n",
      "Epoch 7000, Loss: 0.0000\n",
      "Epoch 7100, Loss: 0.0000\n",
      "Epoch 7200, Loss: 0.0000\n",
      "Epoch 7300, Loss: 0.0000\n",
      "Epoch 7400, Loss: 0.0000\n",
      "Epoch 7500, Loss: 0.0000\n",
      "Epoch 7600, Loss: 0.0000\n",
      "Epoch 7700, Loss: 0.0000\n",
      "Epoch 7800, Loss: 0.0000\n",
      "Epoch 7900, Loss: 0.0000\n",
      "Epoch 8000, Loss: 0.0000\n",
      "Epoch 8100, Loss: 0.0000\n",
      "Epoch 8200, Loss: 0.0000\n",
      "Early stopping at epoch 8206, best loss: 0.0000\n",
      "Epoch 0, Loss: 0.4036\n",
      "Epoch 100, Loss: 0.0078\n",
      "Epoch 200, Loss: 0.0038\n",
      "Epoch 300, Loss: 0.0023\n",
      "Epoch 400, Loss: 0.0015\n",
      "Epoch 500, Loss: 0.0010\n",
      "Epoch 600, Loss: 0.0008\n",
      "Epoch 700, Loss: 0.0006\n",
      "Epoch 800, Loss: 0.0005\n",
      "Epoch 900, Loss: 0.0004\n",
      "Epoch 1000, Loss: 0.0003\n",
      "Epoch 1100, Loss: 0.0003\n",
      "Epoch 1200, Loss: 0.0002\n",
      "Epoch 1300, Loss: 0.0002\n",
      "Epoch 1400, Loss: 0.0002\n",
      "Epoch 1500, Loss: 0.0002\n",
      "Epoch 1600, Loss: 0.0001\n",
      "Epoch 1700, Loss: 0.0001\n",
      "Epoch 1800, Loss: 0.0001\n",
      "Epoch 1900, Loss: 0.0001\n",
      "Epoch 2000, Loss: 0.0001\n",
      "Epoch 2100, Loss: 0.0001\n",
      "Epoch 2200, Loss: 0.0001\n",
      "Epoch 2300, Loss: 0.0001\n",
      "Epoch 2400, Loss: 0.0001\n",
      "Epoch 2500, Loss: 0.0001\n",
      "Epoch 2600, Loss: 0.0001\n",
      "Epoch 2700, Loss: 0.0001\n",
      "Epoch 2800, Loss: 0.0001\n",
      "Epoch 2900, Loss: 0.0001\n",
      "Epoch 3000, Loss: 0.0001\n",
      "Epoch 3100, Loss: 0.0001\n",
      "Epoch 3200, Loss: 0.0001\n",
      "Epoch 3300, Loss: 0.0001\n",
      "Epoch 3400, Loss: 0.0001\n",
      "Epoch 3500, Loss: 0.0001\n",
      "Epoch 3600, Loss: 0.0001\n",
      "Epoch 3700, Loss: 0.0001\n",
      "Epoch 3800, Loss: 0.0001\n",
      "Epoch 3900, Loss: 0.0001\n",
      "Epoch 4000, Loss: 0.0001\n",
      "Epoch 4100, Loss: 0.0001\n",
      "Epoch 4200, Loss: 0.0001\n",
      "Epoch 4300, Loss: 0.0001\n",
      "Epoch 4400, Loss: 0.0001\n",
      "Epoch 4500, Loss: 0.0001\n",
      "Epoch 4600, Loss: 0.0001\n",
      "Epoch 4700, Loss: 0.0001\n",
      "Epoch 4800, Loss: 0.0001\n",
      "Epoch 4900, Loss: 0.0001\n",
      "Epoch 5000, Loss: 0.0001\n",
      "Epoch 5100, Loss: 0.0001\n",
      "Epoch 5200, Loss: 0.0001\n",
      "Epoch 5300, Loss: 0.0001\n",
      "Epoch 5400, Loss: 0.0001\n",
      "Epoch 5500, Loss: 0.0001\n",
      "Epoch 5600, Loss: 0.0001\n",
      "Epoch 5700, Loss: 0.0001\n",
      "Epoch 5800, Loss: 0.0001\n",
      "Epoch 5900, Loss: 0.0001\n",
      "Epoch 6000, Loss: 0.0001\n",
      "Epoch 6100, Loss: 0.0001\n",
      "Epoch 6200, Loss: 0.0001\n",
      "Epoch 6300, Loss: 0.0001\n",
      "Epoch 6400, Loss: 0.0001\n",
      "Epoch 6500, Loss: 0.0001\n",
      "Epoch 6600, Loss: 0.0001\n",
      "Epoch 6700, Loss: 0.0001\n",
      "Epoch 6800, Loss: 0.0001\n",
      "Epoch 6900, Loss: 0.0001\n",
      "Epoch 7000, Loss: 0.0001\n",
      "Epoch 7100, Loss: 0.0001\n",
      "Early stopping at epoch 7110, best loss: 0.0001\n",
      "Epoch 0, Loss: 0.4324\n",
      "Epoch 100, Loss: 0.0086\n",
      "Epoch 200, Loss: 0.0038\n",
      "Epoch 300, Loss: 0.0022\n",
      "Epoch 400, Loss: 0.0014\n",
      "Epoch 500, Loss: 0.0010\n",
      "Epoch 600, Loss: 0.0007\n",
      "Epoch 700, Loss: 0.0005\n",
      "Epoch 800, Loss: 0.0004\n",
      "Epoch 900, Loss: 0.0003\n",
      "Epoch 1000, Loss: 0.0002\n",
      "Epoch 1100, Loss: 0.0002\n",
      "Epoch 1200, Loss: 0.0001\n",
      "Epoch 1300, Loss: 0.0001\n",
      "Epoch 1400, Loss: 0.0001\n",
      "Epoch 1500, Loss: 0.0001\n",
      "Epoch 1600, Loss: 0.0001\n",
      "Epoch 1700, Loss: 0.0000\n",
      "Epoch 1800, Loss: 0.0000\n",
      "Epoch 1900, Loss: 0.0000\n",
      "Epoch 2000, Loss: 0.0000\n",
      "Epoch 2100, Loss: 0.0000\n",
      "Epoch 2200, Loss: 0.0000\n",
      "Epoch 2300, Loss: 0.0000\n",
      "Epoch 2400, Loss: 0.0000\n",
      "Epoch 2500, Loss: 0.0000\n",
      "Epoch 2600, Loss: 0.0000\n",
      "Epoch 2700, Loss: 0.0000\n",
      "Epoch 2800, Loss: 0.0000\n",
      "Epoch 2900, Loss: 0.0000\n",
      "Epoch 3000, Loss: 0.0000\n",
      "Epoch 3100, Loss: 0.0000\n",
      "Epoch 3200, Loss: 0.0000\n",
      "Epoch 3300, Loss: 0.0000\n",
      "Epoch 3400, Loss: 0.0000\n",
      "Epoch 3500, Loss: 0.0000\n",
      "Epoch 3600, Loss: 0.0000\n",
      "Epoch 3700, Loss: 0.0000\n",
      "Epoch 3800, Loss: 0.0000\n",
      "Epoch 3900, Loss: 0.0000\n",
      "Epoch 4000, Loss: 0.0000\n",
      "Epoch 4100, Loss: 0.0000\n",
      "Epoch 4200, Loss: 0.0000\n",
      "Epoch 4300, Loss: 0.0000\n",
      "Epoch 4400, Loss: 0.0000\n",
      "Epoch 4500, Loss: 0.0000\n",
      "Epoch 4600, Loss: 0.0000\n",
      "Epoch 4700, Loss: 0.0000\n",
      "Epoch 4800, Loss: 0.0000\n",
      "Epoch 4900, Loss: 0.0000\n",
      "Epoch 5000, Loss: 0.0000\n",
      "Epoch 5100, Loss: 0.0000\n",
      "Epoch 5200, Loss: 0.0000\n",
      "Epoch 5300, Loss: 0.0000\n",
      "Epoch 5400, Loss: 0.0000\n",
      "Epoch 5500, Loss: 0.0000\n",
      "Epoch 5600, Loss: 0.0000\n",
      "Epoch 5700, Loss: 0.0000\n",
      "Epoch 5800, Loss: 0.0000\n",
      "Epoch 5900, Loss: 0.0000\n",
      "Epoch 6000, Loss: 0.0000\n",
      "Epoch 6100, Loss: 0.0000\n",
      "Epoch 6200, Loss: 0.0000\n",
      "Epoch 6300, Loss: 0.0000\n",
      "Epoch 6400, Loss: 0.0000\n",
      "Epoch 6500, Loss: 0.0000\n",
      "Epoch 6600, Loss: 0.0000\n",
      "Epoch 6700, Loss: 0.0000\n",
      "Epoch 6800, Loss: 0.0000\n",
      "Epoch 6900, Loss: 0.0000\n",
      "Epoch 7000, Loss: 0.0000\n",
      "Epoch 7100, Loss: 0.0000\n",
      "Epoch 7200, Loss: 0.0000\n",
      "Epoch 7300, Loss: 0.0000\n",
      "Epoch 7400, Loss: 0.0000\n",
      "Epoch 7500, Loss: 0.0000\n",
      "Epoch 7600, Loss: 0.0000\n",
      "Epoch 7700, Loss: 0.0000\n",
      "Epoch 7800, Loss: 0.0000\n",
      "Epoch 7900, Loss: 0.0000\n",
      "Epoch 8000, Loss: 0.0000\n",
      "Epoch 8100, Loss: 0.0000\n",
      "Epoch 8200, Loss: 0.0000\n",
      "Early stopping at epoch 8206, best loss: 0.0000\n",
      "Epoch 0, Loss: 0.3933\n",
      "Epoch 100, Loss: 0.0089\n",
      "Epoch 200, Loss: 0.0043\n",
      "Epoch 300, Loss: 0.0025\n",
      "Epoch 400, Loss: 0.0016\n",
      "Epoch 500, Loss: 0.0011\n",
      "Epoch 600, Loss: 0.0008\n",
      "Epoch 700, Loss: 0.0006\n",
      "Epoch 800, Loss: 0.0005\n",
      "Epoch 900, Loss: 0.0004\n",
      "Epoch 1000, Loss: 0.0003\n",
      "Epoch 1100, Loss: 0.0003\n",
      "Epoch 1200, Loss: 0.0002\n",
      "Epoch 1300, Loss: 0.0002\n",
      "Epoch 1400, Loss: 0.0002\n",
      "Epoch 1500, Loss: 0.0002\n",
      "Epoch 1600, Loss: 0.0002\n",
      "Epoch 1700, Loss: 0.0001\n",
      "Epoch 1800, Loss: 0.0001\n",
      "Epoch 1900, Loss: 0.0001\n",
      "Epoch 2000, Loss: 0.0001\n",
      "Epoch 2100, Loss: 0.0001\n",
      "Epoch 2200, Loss: 0.0001\n",
      "Epoch 2300, Loss: 0.0001\n",
      "Epoch 2400, Loss: 0.0001\n",
      "Epoch 2500, Loss: 0.0001\n",
      "Epoch 2600, Loss: 0.0001\n",
      "Epoch 2700, Loss: 0.0001\n",
      "Epoch 2800, Loss: 0.0001\n",
      "Epoch 2900, Loss: 0.0001\n",
      "Epoch 3000, Loss: 0.0001\n",
      "Epoch 3100, Loss: 0.0001\n",
      "Epoch 3200, Loss: 0.0001\n",
      "Epoch 3300, Loss: 0.0001\n",
      "Epoch 3400, Loss: 0.0001\n",
      "Epoch 3500, Loss: 0.0001\n",
      "Epoch 3600, Loss: 0.0001\n",
      "Epoch 3700, Loss: 0.0001\n",
      "Epoch 3800, Loss: 0.0001\n",
      "Epoch 3900, Loss: 0.0001\n",
      "Epoch 4000, Loss: 0.0001\n",
      "Epoch 4100, Loss: 0.0001\n",
      "Epoch 4200, Loss: 0.0001\n",
      "Epoch 4300, Loss: 0.0001\n",
      "Epoch 4400, Loss: 0.0001\n",
      "Epoch 4500, Loss: 0.0001\n",
      "Epoch 4600, Loss: 0.0001\n",
      "Epoch 4700, Loss: 0.0001\n",
      "Epoch 4800, Loss: 0.0001\n",
      "Epoch 4900, Loss: 0.0001\n",
      "Epoch 5000, Loss: 0.0001\n",
      "Epoch 5100, Loss: 0.0001\n",
      "Epoch 5200, Loss: 0.0001\n",
      "Epoch 5300, Loss: 0.0001\n",
      "Epoch 5400, Loss: 0.0001\n",
      "Epoch 5500, Loss: 0.0001\n",
      "Epoch 5600, Loss: 0.0001\n",
      "Epoch 5700, Loss: 0.0001\n",
      "Epoch 5800, Loss: 0.0001\n",
      "Epoch 5900, Loss: 0.0001\n",
      "Epoch 6000, Loss: 0.0001\n",
      "Epoch 6100, Loss: 0.0001\n",
      "Epoch 6200, Loss: 0.0001\n",
      "Epoch 6300, Loss: 0.0001\n",
      "Epoch 6400, Loss: 0.0001\n",
      "Epoch 6500, Loss: 0.0001\n",
      "Epoch 6600, Loss: 0.0001\n",
      "Epoch 6700, Loss: 0.0001\n",
      "Epoch 6800, Loss: 0.0001\n",
      "Epoch 6900, Loss: 0.0001\n",
      "Epoch 7000, Loss: 0.0001\n",
      "Epoch 7100, Loss: 0.0001\n",
      "Epoch 7200, Loss: 0.0001\n",
      "Early stopping at epoch 7239, best loss: 0.0001\n",
      "Epoch 0, Loss: 0.4750\n",
      "Epoch 100, Loss: 0.0261\n",
      "Epoch 200, Loss: 0.0052\n",
      "Epoch 300, Loss: 0.0028\n",
      "Epoch 400, Loss: 0.0017\n",
      "Epoch 500, Loss: 0.0011\n",
      "Epoch 600, Loss: 0.0008\n",
      "Epoch 700, Loss: 0.0006\n",
      "Epoch 800, Loss: 0.0004\n",
      "Epoch 900, Loss: 0.0003\n",
      "Epoch 1000, Loss: 0.0002\n",
      "Epoch 1100, Loss: 0.0002\n",
      "Epoch 1200, Loss: 0.0001\n",
      "Epoch 1300, Loss: 0.0001\n",
      "Epoch 1400, Loss: 0.0001\n",
      "Epoch 1500, Loss: 0.0001\n",
      "Epoch 1600, Loss: 0.0001\n",
      "Epoch 1700, Loss: 0.0000\n",
      "Epoch 1800, Loss: 0.0000\n",
      "Epoch 1900, Loss: 0.0000\n",
      "Epoch 2000, Loss: 0.0000\n",
      "Epoch 2100, Loss: 0.0000\n",
      "Epoch 2200, Loss: 0.0000\n",
      "Epoch 2300, Loss: 0.0000\n",
      "Epoch 2400, Loss: 0.0000\n",
      "Epoch 2500, Loss: 0.0000\n",
      "Epoch 2600, Loss: 0.0000\n",
      "Epoch 2700, Loss: 0.0000\n",
      "Epoch 2800, Loss: 0.0000\n",
      "Epoch 2900, Loss: 0.0000\n",
      "Epoch 3000, Loss: 0.0000\n",
      "Epoch 3100, Loss: 0.0000\n",
      "Epoch 3200, Loss: 0.0000\n",
      "Epoch 3300, Loss: 0.0000\n",
      "Epoch 3400, Loss: 0.0000\n",
      "Epoch 3500, Loss: 0.0000\n",
      "Epoch 3600, Loss: 0.0000\n",
      "Epoch 3700, Loss: 0.0000\n",
      "Epoch 3800, Loss: 0.0000\n",
      "Epoch 3900, Loss: 0.0000\n",
      "Epoch 4000, Loss: 0.0000\n",
      "Epoch 4100, Loss: 0.0000\n",
      "Epoch 4200, Loss: 0.0000\n",
      "Epoch 4300, Loss: 0.0000\n",
      "Epoch 4400, Loss: 0.0000\n",
      "Epoch 4500, Loss: 0.0000\n",
      "Epoch 4600, Loss: 0.0000\n",
      "Epoch 4700, Loss: 0.0000\n",
      "Epoch 4800, Loss: 0.0000\n",
      "Epoch 4900, Loss: 0.0000\n",
      "Epoch 5000, Loss: 0.0000\n",
      "Epoch 5100, Loss: 0.0000\n",
      "Epoch 5200, Loss: 0.0000\n",
      "Epoch 5300, Loss: 0.0000\n",
      "Epoch 5400, Loss: 0.0000\n",
      "Epoch 5500, Loss: 0.0000\n",
      "Epoch 5600, Loss: 0.0000\n",
      "Epoch 5700, Loss: 0.0000\n",
      "Epoch 5800, Loss: 0.0000\n",
      "Epoch 5900, Loss: 0.0000\n",
      "Epoch 6000, Loss: 0.0000\n",
      "Epoch 6100, Loss: 0.0000\n",
      "Epoch 6200, Loss: 0.0000\n",
      "Epoch 6300, Loss: 0.0000\n",
      "Epoch 6400, Loss: 0.0000\n",
      "Epoch 6500, Loss: 0.0000\n",
      "Epoch 6600, Loss: 0.0000\n",
      "Epoch 6700, Loss: 0.0000\n",
      "Epoch 6800, Loss: 0.0000\n",
      "Epoch 6900, Loss: 0.0000\n",
      "Epoch 7000, Loss: 0.0000\n",
      "Epoch 7100, Loss: 0.0000\n",
      "Epoch 7200, Loss: 0.0000\n",
      "Epoch 7300, Loss: 0.0000\n",
      "Epoch 7400, Loss: 0.0000\n",
      "Epoch 7500, Loss: 0.0000\n",
      "Epoch 7600, Loss: 0.0000\n",
      "Epoch 7700, Loss: 0.0000\n",
      "Epoch 7800, Loss: 0.0000\n",
      "Epoch 7900, Loss: 0.0000\n",
      "Epoch 8000, Loss: 0.0000\n",
      "Epoch 8100, Loss: 0.0000\n",
      "Epoch 8200, Loss: 0.0000\n",
      "Early stopping at epoch 8238, best loss: 0.0000\n",
      "Epoch 0, Loss: 0.3944\n",
      "Epoch 100, Loss: 0.0114\n",
      "Epoch 200, Loss: 0.0043\n",
      "Epoch 300, Loss: 0.0024\n",
      "Epoch 400, Loss: 0.0016\n",
      "Epoch 500, Loss: 0.0011\n",
      "Epoch 600, Loss: 0.0008\n",
      "Epoch 700, Loss: 0.0006\n",
      "Epoch 800, Loss: 0.0005\n",
      "Epoch 900, Loss: 0.0004\n",
      "Epoch 1000, Loss: 0.0003\n",
      "Epoch 1100, Loss: 0.0003\n",
      "Epoch 1200, Loss: 0.0002\n",
      "Epoch 1300, Loss: 0.0002\n",
      "Epoch 1400, Loss: 0.0002\n",
      "Epoch 1500, Loss: 0.0002\n",
      "Epoch 1600, Loss: 0.0002\n",
      "Epoch 1700, Loss: 0.0001\n",
      "Epoch 1800, Loss: 0.0001\n",
      "Epoch 1900, Loss: 0.0001\n",
      "Epoch 2000, Loss: 0.0001\n",
      "Epoch 2100, Loss: 0.0001\n",
      "Epoch 2200, Loss: 0.0001\n",
      "Epoch 2300, Loss: 0.0001\n",
      "Epoch 2400, Loss: 0.0001\n",
      "Epoch 2500, Loss: 0.0001\n",
      "Epoch 2600, Loss: 0.0001\n",
      "Epoch 2700, Loss: 0.0001\n",
      "Epoch 2800, Loss: 0.0001\n",
      "Epoch 2900, Loss: 0.0001\n",
      "Epoch 3000, Loss: 0.0001\n",
      "Epoch 3100, Loss: 0.0001\n",
      "Epoch 3200, Loss: 0.0001\n",
      "Epoch 3300, Loss: 0.0001\n",
      "Epoch 3400, Loss: 0.0001\n",
      "Epoch 3500, Loss: 0.0001\n",
      "Epoch 3600, Loss: 0.0001\n",
      "Epoch 3700, Loss: 0.0001\n",
      "Epoch 3800, Loss: 0.0001\n",
      "Epoch 3900, Loss: 0.0001\n",
      "Epoch 4000, Loss: 0.0001\n",
      "Epoch 4100, Loss: 0.0001\n",
      "Epoch 4200, Loss: 0.0001\n",
      "Epoch 4300, Loss: 0.0001\n",
      "Epoch 4400, Loss: 0.0001\n",
      "Epoch 4500, Loss: 0.0001\n",
      "Epoch 4600, Loss: 0.0001\n",
      "Epoch 4700, Loss: 0.0001\n",
      "Epoch 4800, Loss: 0.0001\n",
      "Epoch 4900, Loss: 0.0001\n",
      "Epoch 5000, Loss: 0.0001\n",
      "Epoch 5100, Loss: 0.0001\n",
      "Epoch 5200, Loss: 0.0001\n",
      "Epoch 5300, Loss: 0.0001\n",
      "Epoch 5400, Loss: 0.0001\n",
      "Epoch 5500, Loss: 0.0001\n",
      "Epoch 5600, Loss: 0.0001\n",
      "Epoch 5700, Loss: 0.0001\n",
      "Epoch 5800, Loss: 0.0001\n",
      "Epoch 5900, Loss: 0.0001\n",
      "Epoch 6000, Loss: 0.0001\n",
      "Epoch 6100, Loss: 0.0001\n",
      "Epoch 6200, Loss: 0.0001\n",
      "Epoch 6300, Loss: 0.0001\n",
      "Epoch 6400, Loss: 0.0001\n",
      "Epoch 6500, Loss: 0.0001\n",
      "Epoch 6600, Loss: 0.0001\n",
      "Epoch 6700, Loss: 0.0001\n",
      "Epoch 6800, Loss: 0.0001\n",
      "Epoch 6900, Loss: 0.0001\n",
      "Epoch 7000, Loss: 0.0001\n",
      "Epoch 7100, Loss: 0.0001\n",
      "Epoch 7200, Loss: 0.0001\n",
      "Early stopping at epoch 7263, best loss: 0.0001\n",
      "Epoch 0, Loss: 0.3759\n",
      "Epoch 100, Loss: 0.0097\n",
      "Epoch 200, Loss: 0.0038\n",
      "Epoch 300, Loss: 0.0021\n",
      "Epoch 400, Loss: 0.0014\n",
      "Epoch 500, Loss: 0.0009\n",
      "Epoch 600, Loss: 0.0007\n",
      "Epoch 700, Loss: 0.0005\n",
      "Epoch 800, Loss: 0.0004\n",
      "Epoch 900, Loss: 0.0003\n",
      "Epoch 1000, Loss: 0.0002\n",
      "Epoch 1100, Loss: 0.0002\n",
      "Epoch 1200, Loss: 0.0001\n",
      "Epoch 1300, Loss: 0.0001\n",
      "Epoch 1400, Loss: 0.0001\n",
      "Epoch 1500, Loss: 0.0001\n",
      "Epoch 1600, Loss: 0.0000\n",
      "Epoch 1700, Loss: 0.0000\n",
      "Epoch 1800, Loss: 0.0000\n",
      "Epoch 1900, Loss: 0.0000\n",
      "Epoch 2000, Loss: 0.0000\n",
      "Epoch 2100, Loss: 0.0000\n",
      "Epoch 2200, Loss: 0.0000\n",
      "Epoch 2300, Loss: 0.0000\n",
      "Epoch 2400, Loss: 0.0000\n",
      "Epoch 2500, Loss: 0.0000\n",
      "Epoch 2600, Loss: 0.0000\n",
      "Epoch 2700, Loss: 0.0000\n",
      "Epoch 2800, Loss: 0.0000\n",
      "Epoch 2900, Loss: 0.0000\n",
      "Epoch 3000, Loss: 0.0000\n",
      "Epoch 3100, Loss: 0.0000\n",
      "Epoch 3200, Loss: 0.0000\n",
      "Epoch 3300, Loss: 0.0000\n",
      "Epoch 3400, Loss: 0.0000\n",
      "Epoch 3500, Loss: 0.0000\n",
      "Epoch 3600, Loss: 0.0000\n",
      "Epoch 3700, Loss: 0.0000\n",
      "Epoch 3800, Loss: 0.0000\n",
      "Epoch 3900, Loss: 0.0000\n",
      "Epoch 4000, Loss: 0.0000\n",
      "Epoch 4100, Loss: 0.0000\n",
      "Epoch 4200, Loss: 0.0000\n",
      "Epoch 4300, Loss: 0.0000\n",
      "Epoch 4400, Loss: 0.0000\n",
      "Epoch 4500, Loss: 0.0000\n",
      "Epoch 4600, Loss: 0.0000\n",
      "Epoch 4700, Loss: 0.0000\n",
      "Epoch 4800, Loss: 0.0000\n",
      "Epoch 4900, Loss: 0.0000\n",
      "Epoch 5000, Loss: 0.0000\n",
      "Epoch 5100, Loss: 0.0000\n",
      "Epoch 5200, Loss: 0.0000\n",
      "Epoch 5300, Loss: 0.0000\n",
      "Epoch 5400, Loss: 0.0000\n",
      "Epoch 5500, Loss: 0.0000\n",
      "Epoch 5600, Loss: 0.0000\n",
      "Epoch 5700, Loss: 0.0000\n",
      "Epoch 5800, Loss: 0.0000\n",
      "Epoch 5900, Loss: 0.0000\n",
      "Epoch 6000, Loss: 0.0000\n",
      "Epoch 6100, Loss: 0.0000\n",
      "Epoch 6200, Loss: 0.0000\n",
      "Epoch 6300, Loss: 0.0000\n",
      "Epoch 6400, Loss: 0.0000\n",
      "Epoch 6500, Loss: 0.0000\n",
      "Epoch 6600, Loss: 0.0000\n",
      "Epoch 6700, Loss: 0.0000\n",
      "Epoch 6800, Loss: 0.0000\n",
      "Epoch 6900, Loss: 0.0000\n",
      "Epoch 7000, Loss: 0.0000\n",
      "Epoch 7100, Loss: 0.0000\n",
      "Epoch 7200, Loss: 0.0000\n",
      "Epoch 7300, Loss: 0.0000\n",
      "Epoch 7400, Loss: 0.0000\n",
      "Epoch 7500, Loss: 0.0000\n",
      "Epoch 7600, Loss: 0.0000\n",
      "Epoch 7700, Loss: 0.0000\n",
      "Epoch 7800, Loss: 0.0000\n",
      "Epoch 7900, Loss: 0.0000\n",
      "Epoch 8000, Loss: 0.0000\n",
      "Epoch 8100, Loss: 0.0000\n",
      "Early stopping at epoch 8187, best loss: 0.0000\n",
      "Avg. distances using L1 norm 0.007063907360861777\n",
      "Avg. distances using L2 norm 0.014163865290987588\n"
     ]
    }
   ],
   "source": [
    "#Run code 5 times and then average the results of the distance metric\n",
    "def run_student_model(l1_lambda = 0, l2_lambda = 0, print_params = False):\n",
    "    student_model = SparseCNN_1()\n",
    "\n",
    "    # print(\"\\nStudent function parameters BEFORE training:\")\n",
    "    # for param in student_model.parameters():\n",
    "    #     print(param.data.numpy())\n",
    "\n",
    "    sgd_optim = optim.SGD(student_model.parameters(), lr=0.05, momentum=0.9, weight_decay=l2_lambda)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    student_model, final_loss = train_model(student_model, X_train=X_generated, y_train=y_generated, optimizer=sgd_optim, loss_fn=loss_fn, l1_lambda=l1_lambda)\n",
    "    \n",
    "    if print_params == True:\n",
    "        print(\"\\nTarget function parameters:\")\n",
    "        for param in teacher_model.parameters():\n",
    "            print(param.data.numpy())\n",
    "        \n",
    "        print(\"\\nStudent function parameters AFTER training:\")\n",
    "        for param in student_model.parameters():\n",
    "            print(param.data.numpy())\n",
    "\n",
    "    distance_metric = compute_distance_metric(teacher_model=teacher_model, student_model=student_model)\n",
    "    return distance_metric, final_loss\n",
    "\n",
    "\n",
    "# distances_l1 = []\n",
    "# losses = []\n",
    "# for _ in range(5):\n",
    "#     distance_metric, final_loss = run_student_model()\n",
    "#     distances_l1.append(distance_metric)\n",
    "#     losses.append(round(final_loss,4))\n",
    "\n",
    "# print(\"Avg. distances no weight decay\", np.mean(distances_l1))\n",
    "# print(\"Losses\", losses)\n",
    "\n",
    "distances_l1 = []\n",
    "distances_l2 = []\n",
    "for _ in range(5):\n",
    "    distances_l1.append(run_student_model(l1_lambda=1e-5))\n",
    "    distances_l2.append(run_student_model(l2_lambda=1e-5))\n",
    "\n",
    "print(\"Avg. distances using L1 norm\", np.mean(distances_l1))\n",
    "print(\"Avg. distances using L2 norm\", np.mean(distances_l2))\n",
    "\n",
    "\n",
    "# distances_l1 = []\n",
    "# distances_l2 = []\n",
    "# for _ in range(5):\n",
    "#     distances_l1.append(run_student_model(l1_lambda=0.1))\n",
    "#     distances_l2.append(run_student_model(l2_lambda=0.1))\n",
    "\n",
    "# print(\"Avg. distances using L1 norm\", np.mean(distances_l1))\n",
    "# print(\"Avg. distances using L2 norm\", np.mean(distances_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a different student architecture than teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student_sparse(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Student_sparse, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=3, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for CNN\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "teacher_model = SparseCNN_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_generated = torch.tensor(np.random.randn(TEST_SET_SIZE, 12), dtype=torch.float32)\n",
    "y_generated = teacher_model(X_generated).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0054\n",
      "Early stopping at epoch 20, best loss: 0.0054\n",
      "\n",
      "Target function parameters:\n",
      "[[[-0.01213434 -0.22194281  0.19934896]]]\n",
      "[0.0353068]\n",
      "[[[ 0.47101322 -0.05958339]]]\n",
      "[-0.423429]\n",
      "[[[0.47402352 0.09368998]]]\n",
      "[-0.19214378]\n",
      "\n",
      "Student function parameters AFTER training:\n",
      "[[[-0.01213434 -0.22194281  0.19934896]]]\n",
      "[0.0353068]\n",
      "[[[ 0.47101322 -0.05958339]]]\n",
      "[-0.423429]\n",
      "[[[0.47402352 0.09368998]]]\n",
      "[-0.19214378]\n"
     ]
    }
   ],
   "source": [
    "student_model = Student_sparse()\n",
    "\n",
    "sgd_optim = optim.SGD(student_model.parameters(), lr=0.05, weight_decay=0.05)\n",
    "loss_fn = nn.MSELoss()\n",
    "student_model, final_loss = train_model(student_model, X_train=X_generated, y_train=y_generated, optimizer=sgd_optim, loss_fn=loss_fn) \n",
    "\n",
    "print(\"\\nTarget function parameters:\")\n",
    "for param in student_model.parameters():\n",
    "    print(param.data.numpy())\n",
    "    \n",
    "print(\"\\nStudent function parameters AFTER training:\")\n",
    "for param in student_model.parameters():\n",
    "    print(param.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3389\n",
      "Epoch 100, Loss: 0.0498\n",
      "Epoch 200, Loss: 0.0111\n",
      "Epoch 300, Loss: 0.0058\n",
      "Epoch 400, Loss: 0.0051\n",
      "Epoch 500, Loss: 0.0050\n",
      "Epoch 600, Loss: 0.0050\n",
      "Epoch 700, Loss: 0.0050\n",
      "Epoch 800, Loss: 0.0050\n",
      "Epoch 900, Loss: 0.0050\n",
      "Epoch 0, Loss: 0.6455\n",
      "Epoch 100, Loss: 0.0203\n",
      "Epoch 200, Loss: 0.0063\n",
      "Epoch 300, Loss: 0.0058\n",
      "Epoch 400, Loss: 0.0057\n",
      "Epoch 500, Loss: 0.0057\n",
      "Epoch 600, Loss: 0.0057\n",
      "Epoch 700, Loss: 0.0057\n",
      "Epoch 800, Loss: 0.0056\n",
      "Epoch 900, Loss: 0.0056\n",
      "Epoch 0, Loss: 0.0054\n",
      "Early stopping at epoch 20, best loss: 0.0054\n",
      "Epoch 0, Loss: 0.0054\n",
      "Early stopping at epoch 20, best loss: 0.0054\n",
      "Epoch 0, Loss: 0.0050\n",
      "Epoch 100, Loss: 0.0049\n",
      "Epoch 200, Loss: 0.0049\n",
      "Epoch 300, Loss: 0.0049\n",
      "Epoch 400, Loss: 0.0049\n",
      "Epoch 500, Loss: 0.0049\n",
      "Early stopping at epoch 593, best loss: 0.0049\n",
      "Avg. distances no weight decay 1.8564770758152007\n",
      "Losses [0.005, 0.0056, 0.0054, 0.0054, 0.0049]\n",
      "Epoch 0, Loss: 0.1495\n",
      "Epoch 100, Loss: 0.0732\n",
      "Epoch 200, Loss: 0.0555\n",
      "Epoch 300, Loss: 0.0478\n",
      "Epoch 400, Loss: 0.0424\n",
      "Epoch 500, Loss: 0.0374\n",
      "Epoch 600, Loss: 0.0324\n",
      "Epoch 700, Loss: 0.0279\n",
      "Epoch 800, Loss: 0.0241\n",
      "Epoch 900, Loss: 0.0204\n",
      "Epoch 0, Loss: 0.0054\n",
      "Early stopping at epoch 20, best loss: 0.0054\n",
      "Epoch 0, Loss: 0.2754\n",
      "Epoch 100, Loss: 0.1961\n",
      "Epoch 200, Loss: 0.1709\n",
      "Epoch 300, Loss: 0.1566\n",
      "Epoch 400, Loss: 0.1447\n",
      "Epoch 500, Loss: 0.1336\n",
      "Epoch 600, Loss: 0.1233\n",
      "Epoch 700, Loss: 0.1142\n",
      "Epoch 800, Loss: 0.1054\n",
      "Epoch 900, Loss: 0.0967\n",
      "Epoch 0, Loss: 0.0054\n",
      "Early stopping at epoch 20, best loss: 0.0054\n",
      "Epoch 0, Loss: 0.1737\n",
      "Epoch 100, Loss: 0.1620\n",
      "Epoch 200, Loss: 0.1507\n",
      "Epoch 300, Loss: 0.1395\n",
      "Epoch 400, Loss: 0.1282\n",
      "Epoch 500, Loss: 0.1170\n",
      "Epoch 600, Loss: 0.1070\n",
      "Epoch 700, Loss: 0.0975\n",
      "Epoch 800, Loss: 0.0887\n",
      "Epoch 900, Loss: 0.0800\n",
      "Epoch 0, Loss: 0.6353\n",
      "Epoch 100, Loss: 0.0463\n",
      "Epoch 200, Loss: 0.0097\n",
      "Epoch 300, Loss: 0.0062\n",
      "Epoch 400, Loss: 0.0056\n",
      "Epoch 500, Loss: 0.0054\n",
      "Epoch 600, Loss: 0.0053\n",
      "Epoch 700, Loss: 0.0053\n",
      "Epoch 800, Loss: 0.0052\n",
      "Epoch 900, Loss: 0.0052\n",
      "Epoch 0, Loss: 0.1181\n",
      "Epoch 100, Loss: 0.1067\n",
      "Epoch 200, Loss: 0.0960\n",
      "Epoch 300, Loss: 0.0861\n",
      "Epoch 400, Loss: 0.0773\n",
      "Epoch 500, Loss: 0.0692\n",
      "Epoch 600, Loss: 0.0629\n",
      "Epoch 700, Loss: 0.0567\n",
      "Epoch 800, Loss: 0.0504\n",
      "Epoch 900, Loss: 0.0442\n",
      "Epoch 0, Loss: 0.5592\n",
      "Epoch 100, Loss: 0.0581\n",
      "Epoch 200, Loss: 0.0114\n",
      "Epoch 300, Loss: 0.0061\n",
      "Epoch 400, Loss: 0.0055\n",
      "Epoch 500, Loss: 0.0053\n",
      "Epoch 600, Loss: 0.0053\n",
      "Epoch 700, Loss: 0.0052\n",
      "Epoch 800, Loss: 0.0052\n",
      "Epoch 900, Loss: 0.0052\n",
      "Epoch 0, Loss: 0.6277\n",
      "Epoch 100, Loss: 0.2330\n",
      "Epoch 200, Loss: 0.1703\n",
      "Epoch 300, Loss: 0.1522\n",
      "Epoch 400, Loss: 0.1400\n",
      "Epoch 500, Loss: 0.1287\n",
      "Epoch 600, Loss: 0.1174\n",
      "Epoch 700, Loss: 0.1062\n",
      "Epoch 800, Loss: 0.0957\n",
      "Epoch 900, Loss: 0.0857\n",
      "Epoch 0, Loss: 0.1225\n",
      "Epoch 100, Loss: 0.0140\n",
      "Epoch 200, Loss: 0.0055\n",
      "Epoch 300, Loss: 0.0048\n",
      "Epoch 400, Loss: 0.0047\n",
      "Epoch 500, Loss: 0.0047\n",
      "Early stopping at epoch 558, best loss: 0.0047\n",
      "Avg. distances using L1 norm 0.7656030703336001\n",
      "Avg. distances using L2 norm 0.6980651324149221\n"
     ]
    }
   ],
   "source": [
    "#Run code 5 times and then average the results of the distance metric\n",
    "def run_student_model(l1_lambda = 0, l2_lambda = 0, print_params = False):\n",
    "    student_model = Student_sparse()\n",
    "\n",
    "    sgd_optim = optim.SGD(student_model.parameters(), lr=0.005, weight_decay=l2_lambda)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    student_model, final_loss = train_model(student_model, X_train=X_generated, y_train=y_generated, optimizer=sgd_optim, loss_fn=loss_fn, l1_lambda=l1_lambda)\n",
    "    \n",
    "    if print_params == True:\n",
    "        print(\"\\nTarget function parameters:\")\n",
    "        for param in teacher_model.parameters():\n",
    "            print(param.data.numpy())\n",
    "        \n",
    "        print(\"\\nStudent function parameters AFTER training:\")\n",
    "        for param in student_model.parameters():\n",
    "            print(param.data.numpy())\n",
    "\n",
    "    distance_metric = compute_distance_metric(teacher_model=teacher_model, student_model=student_model)\n",
    "    return distance_metric, final_loss\n",
    "\n",
    "\n",
    "distances_l1 = []\n",
    "losses = []\n",
    "for _ in range(5):\n",
    "    distance_metric, final_loss = run_student_model()\n",
    "    distances_l1.append(distance_metric)\n",
    "    losses.append(round(final_loss,4))\n",
    "\n",
    "print(\"Avg. distances no weight decay\", np.mean(distances_l1))\n",
    "print(\"Losses\", losses)\n",
    "\n",
    "distances_l1 = []\n",
    "distances_l2 = []\n",
    "for _ in range(5):\n",
    "    distances_l1.append(run_student_model(l1_lambda=0.05))\n",
    "    distances_l2.append(run_student_model(l2_lambda=0.05))\n",
    "\n",
    "print(\"Avg. distances using L1 norm\", np.mean(distances_l1))\n",
    "print(\"Avg. distances using L2 norm\", np.mean(distances_l2))\n",
    "\n",
    "\n",
    "# distances_l1 = []\n",
    "# distances_l2 = []\n",
    "# for _ in range(5):\n",
    "#     distances_l1.append(run_student_model(l1_lambda=0.1))\n",
    "#     distances_l2.append(run_student_model(l2_lambda=0.1))\n",
    "\n",
    "# print(\"Avg. distances using L1 norm\", np.mean(distances_l1))\n",
    "# print(\"Avg. distances using L2 norm\", np.mean(distances_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseNN Teacher, DenseNN Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse NN defines the sparse function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define \"teacher\" model that generates a Dataset X, y\n",
    "\n",
    "QUESTION: why use a CNN? Couldn't I also manually add sparsity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCNN_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SparseCNN_1, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=2, stride=1, padding=0, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=4, out_channels=2, kernel_size=2, stride=1, padding=0, bias=False)\n",
    "        self.fc = nn.Linear(2, 1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for CNN\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.tanh(self.fc(x))  # Apply tanh activation for positive/negative output\n",
    "        return x\n",
    "\n",
    "# Generate initial training data\n",
    "X_train = torch.tensor(np.random.randn(TEST_SET_SIZE, 3), dtype=torch.float32)\n",
    "y_train = (torch.randn(TEST_SET_SIZE) > 0).float().unsqueeze(1)  # Random labels for initial training\n",
    "\n",
    "# Train the CNN model for one epoch\n",
    "teacher_model = SparseCNN_1()\n",
    "optimizer = optim.SGD(teacher_model.parameters(), lr=0.01)\n",
    "loss_fn = nn.HingeEmbeddingLoss()\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = torch.tanh(teacher_model(X_train))\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16926415]\n",
      " [-0.31554717]\n",
      " [-0.2561143 ]\n",
      " [-0.2923395 ]\n",
      " [-0.13538507]\n",
      " [-0.31636113]\n",
      " [-0.3006882 ]\n",
      " [-0.23830342]\n",
      " [-0.17360625]\n",
      " [-0.06610367]\n",
      " [-0.25173333]\n",
      " [-0.17179686]\n",
      " [-0.12237118]\n",
      " [-0.17486657]\n",
      " [-0.20299232]\n",
      " [-0.3394934 ]\n",
      " [-0.27969295]\n",
      " [-0.0836275 ]\n",
      " [-0.18193364]\n",
      " [-0.12299453]\n",
      " [-0.03974314]\n",
      " [-0.16686611]\n",
      " [-0.21119726]\n",
      " [-0.3473768 ]\n",
      " [-0.3602215 ]\n",
      " [-0.35807398]\n",
      " [-0.2445311 ]\n",
      " [-0.18066874]\n",
      " [-0.07693703]\n",
      " [-0.23945016]\n",
      " [-0.2826171 ]\n",
      " [-0.26390702]\n",
      " [-0.19646007]\n",
      " [-0.18455684]\n",
      " [-0.3121959 ]\n",
      " [-0.17985696]\n",
      " [-0.17452414]\n",
      " [-0.13670464]\n",
      " [-0.07541537]\n",
      " [-0.43158647]\n",
      " [-0.10061846]\n",
      " [-0.16781142]\n",
      " [-0.38297015]\n",
      " [-0.03491351]\n",
      " [-0.3891763 ]\n",
      " [-0.21923736]\n",
      " [-0.19632228]\n",
      " [-0.20115846]\n",
      " [-0.13250977]\n",
      " [-0.11153005]\n",
      " [-0.37487736]\n",
      " [-0.14422806]\n",
      " [-0.36020735]\n",
      " [-0.10803255]\n",
      " [-0.09847488]\n",
      " [-0.1610655 ]\n",
      " [-0.09989656]\n",
      " [-0.20776303]\n",
      " [-0.11780811]\n",
      " [-0.29866627]\n",
      " [-0.21608327]\n",
      " [-0.2601084 ]\n",
      " [-0.3309865 ]\n",
      " [-0.29319105]\n",
      " [-0.2750859 ]\n",
      " [-0.34698918]\n",
      " [-0.13343854]\n",
      " [-0.30138943]\n",
      " [-0.07547183]\n",
      " [-0.2863685 ]\n",
      " [-0.34916174]\n",
      " [-0.49372098]\n",
      " [-0.12910543]\n",
      " [-0.18556456]\n",
      " [-0.28310385]\n",
      " [-0.26687014]\n",
      " [-0.10229284]\n",
      " [-0.4424137 ]\n",
      " [-0.31018904]\n",
      " [-0.29516274]\n",
      " [-0.5760557 ]\n",
      " [-0.24578333]\n",
      " [-0.09705468]\n",
      " [-0.2977218 ]\n",
      " [-0.3024286 ]\n",
      " [-0.08459336]\n",
      " [-0.35045543]\n",
      " [-0.23689617]\n",
      " [-0.41954288]\n",
      " [-0.16977853]\n",
      " [-0.17521827]\n",
      " [-0.2836486 ]\n",
      " [-0.14409213]\n",
      " [-0.10661058]\n",
      " [-0.35639822]\n",
      " [-0.15670149]\n",
      " [-0.3083758 ]\n",
      " [-0.44570738]\n",
      " [-0.34677345]\n",
      " [-0.2217973 ]\n",
      " [-0.11710986]\n",
      " [-0.3753629 ]\n",
      " [-0.40167385]\n",
      " [-0.15068367]\n",
      " [-0.22746864]\n",
      " [-0.12782978]\n",
      " [-0.5563763 ]\n",
      " [-0.16151519]\n",
      " [-0.19380018]\n",
      " [-0.17299539]\n",
      " [-0.33122623]\n",
      " [-0.3059117 ]\n",
      " [-0.19810793]\n",
      " [-0.27790913]\n",
      " [-0.27286804]\n",
      " [-0.4360329 ]\n",
      " [-0.09232482]\n",
      " [-0.12092274]\n",
      " [-0.05822061]\n",
      " [-0.28524464]\n",
      " [-0.13828106]\n",
      " [-0.42676613]\n",
      " [-0.13454844]\n",
      " [-0.16144346]\n",
      " [-0.3921438 ]\n",
      " [-0.12280561]\n",
      " [-0.42371088]\n",
      " [-0.3206773 ]\n",
      " [-0.1069535 ]\n",
      " [-0.23591915]\n",
      " [-0.15263943]\n",
      " [-0.41947943]\n",
      " [-0.56118   ]\n",
      " [-0.1830207 ]\n",
      " [-0.10060105]\n",
      " [-0.3476136 ]\n",
      " [-0.44577453]\n",
      " [-0.43302014]\n",
      " [-0.20196544]\n",
      " [-0.12837255]\n",
      " [-0.12899888]\n",
      " [-0.27152705]\n",
      " [-0.3858307 ]\n",
      " [-0.38001418]\n",
      " [-0.4689724 ]\n",
      " [-0.22435181]\n",
      " [-0.1784625 ]\n",
      " [-0.36503235]\n",
      " [-0.1350254 ]\n",
      " [-0.06487364]\n",
      " [-0.12486006]\n",
      " [-0.43654996]\n",
      " [-0.12649354]\n",
      " [-0.5590734 ]\n",
      " [-0.2787973 ]\n",
      " [-0.12620012]\n",
      " [-0.04146038]\n",
      " [-0.14220141]\n",
      " [-0.20183757]\n",
      " [-0.47888026]\n",
      " [-0.26382348]\n",
      " [-0.09296268]\n",
      " [-0.26419187]\n",
      " [-0.13180378]\n",
      " [-0.11417434]\n",
      " [-0.32442242]\n",
      " [-0.23336647]\n",
      " [-0.06462824]\n",
      " [-0.15210256]\n",
      " [-0.117323  ]\n",
      " [-0.37763372]\n",
      " [-0.27226856]\n",
      " [-0.33073452]\n",
      " [-0.39608234]\n",
      " [-0.36553255]\n",
      " [-0.46289334]\n",
      " [-0.35989806]\n",
      " [-0.25623566]\n",
      " [-0.51543283]\n",
      " [-0.07129167]\n",
      " [-0.43452862]\n",
      " [-0.37707594]\n",
      " [-0.3170583 ]\n",
      " [-0.17858246]\n",
      " [-0.5126392 ]\n",
      " [-0.17896286]\n",
      " [-0.14845653]\n",
      " [-0.29707444]\n",
      " [-0.13962084]\n",
      " [-0.2883343 ]\n",
      " [-0.36080635]\n",
      " [-0.15283331]\n",
      " [-0.06301634]\n",
      " [-0.59846836]\n",
      " [-0.22421046]\n",
      " [-0.09827621]\n",
      " [-0.3081418 ]\n",
      " [-0.47970524]\n",
      " [-0.28545266]\n",
      " [-0.14711583]\n",
      " [-0.40744978]\n",
      " [-0.17877142]\n",
      " [-0.26522136]\n",
      " [-0.05812428]\n",
      " [-0.18566266]\n",
      " [-0.11309344]\n",
      " [-0.28324604]\n",
      " [-0.22349906]\n",
      " [-0.1912441 ]\n",
      " [-0.162561  ]\n",
      " [-0.07351471]\n",
      " [-0.17534024]\n",
      " [-0.08738615]\n",
      " [-0.05753236]\n",
      " [-0.13514623]\n",
      " [-0.16395205]\n",
      " [-0.12719452]\n",
      " [-0.15025517]\n",
      " [-0.09605046]\n",
      " [-0.55704135]\n",
      " [-0.09686419]\n",
      " [-0.27148405]\n",
      " [-0.16790494]\n",
      " [-0.33943996]\n",
      " [-0.23190102]\n",
      " [-0.02429245]\n",
      " [-0.18878837]\n",
      " [-0.28324285]\n",
      " [-0.4773322 ]\n",
      " [-0.31087863]\n",
      " [-0.33123887]\n",
      " [-0.23668003]\n",
      " [-0.24493587]\n",
      " [-0.3976489 ]\n",
      " [-0.09992094]\n",
      " [-0.27511644]\n",
      " [-0.18204975]\n",
      " [-0.28749165]\n",
      " [-0.63562626]\n",
      " [-0.22485852]\n",
      " [-0.36615434]\n",
      " [-0.21375562]\n",
      " [-0.24907646]\n",
      " [-0.32116082]\n",
      " [-0.3209895 ]\n",
      " [-0.04293998]\n",
      " [-0.11744167]\n",
      " [-0.12668866]\n",
      " [-0.2456366 ]\n",
      " [-0.32010618]\n",
      " [-0.30637896]\n",
      " [-0.20834127]\n",
      " [-0.5118068 ]\n",
      " [-0.08386773]\n",
      " [-0.2368449 ]\n",
      " [-0.26149505]\n",
      " [-0.21901396]\n",
      " [-0.06142695]\n",
      " [-0.31880364]\n",
      " [-0.20064643]\n",
      " [-0.11692727]\n",
      " [-0.17793323]\n",
      " [-0.10974582]\n",
      " [-0.211574  ]\n",
      " [-0.03182001]\n",
      " [-0.05748276]\n",
      " [-0.09180836]\n",
      " [-0.17753696]\n",
      " [-0.17084464]\n",
      " [-0.35811844]\n",
      " [-0.18103606]\n",
      " [-0.1674468 ]\n",
      " [-0.04680133]\n",
      " [-0.12878565]\n",
      " [-0.0792477 ]\n",
      " [-0.17605075]\n",
      " [-0.18744259]\n",
      " [-0.26297206]\n",
      " [-0.2926116 ]\n",
      " [-0.1035701 ]\n",
      " [-0.19568832]\n",
      " [-0.46821815]\n",
      " [-0.12148665]\n",
      " [-0.17584397]\n",
      " [-0.14026318]\n",
      " [-0.18101455]\n",
      " [-0.07013716]\n",
      " [-0.0781624 ]\n",
      " [-0.09086003]\n",
      " [-0.35151854]\n",
      " [-0.20743479]\n",
      " [-0.29888776]\n",
      " [-0.22223713]\n",
      " [-0.09664398]\n",
      " [-0.17689887]\n",
      " [-0.2429643 ]\n",
      " [-0.28175318]\n",
      " [-0.22705038]\n",
      " [-0.38149258]\n",
      " [-0.18402895]\n",
      " [-0.3586012 ]\n",
      " [-0.37899807]\n",
      " [-0.20853019]\n",
      " [-0.14888895]\n",
      " [-0.18252695]\n",
      " [-0.11240763]\n",
      " [-0.43520927]\n",
      " [-0.20429716]\n",
      " [-0.19049475]\n",
      " [-0.14423744]\n",
      " [-0.45061308]\n",
      " [-0.15326726]\n",
      " [-0.15828942]\n",
      " [-0.31931007]\n",
      " [-0.23482446]\n",
      " [-0.22687203]\n",
      " [-0.1607152 ]\n",
      " [-0.5604669 ]\n",
      " [-0.06639464]\n",
      " [-0.3034838 ]\n",
      " [-0.4669882 ]\n",
      " [-0.11552474]\n",
      " [-0.39692363]\n",
      " [-0.17163122]\n",
      " [-0.10906837]\n",
      " [-0.33925498]\n",
      " [-0.28250712]\n",
      " [-0.6231829 ]\n",
      " [-0.24857107]\n",
      " [-0.4209444 ]\n",
      " [-0.22737713]\n",
      " [-0.23924097]\n",
      " [-0.44450846]\n",
      " [-0.10109878]\n",
      " [-0.17413789]\n",
      " [-0.2816239 ]\n",
      " [-0.16930953]\n",
      " [-0.1551091 ]\n",
      " [-0.05046729]\n",
      " [-0.07899603]\n",
      " [-0.22521345]\n",
      " [-0.08193112]\n",
      " [-0.35369295]\n",
      " [-0.24892522]\n",
      " [-0.25891235]\n",
      " [-0.17211103]\n",
      " [-0.0825863 ]\n",
      " [-0.2009219 ]\n",
      " [-0.20347796]\n",
      " [-0.33154213]\n",
      " [-0.1639435 ]\n",
      " [-0.12298229]\n",
      " [-0.34121442]\n",
      " [-0.22152813]\n",
      " [-0.18195581]\n",
      " [-0.12376338]\n",
      " [-0.32572648]\n",
      " [-0.15070516]\n",
      " [-0.07505837]\n",
      " [-0.16299173]\n",
      " [-0.34121522]\n",
      " [-0.35789382]\n",
      " [-0.30760115]\n",
      " [-0.27663082]\n",
      " [-0.27086428]\n",
      " [-0.17356871]\n",
      " [-0.19979352]\n",
      " [-0.4163891 ]\n",
      " [-0.05994168]\n",
      " [-0.1665382 ]\n",
      " [-0.3521317 ]\n",
      " [-0.37987542]\n",
      " [-0.187004  ]\n",
      " [-0.10108767]\n",
      " [-0.16190317]\n",
      " [-0.37272292]\n",
      " [-0.39410135]\n",
      " [-0.3827231 ]\n",
      " [-0.0389552 ]\n",
      " [-0.23043928]\n",
      " [-0.43558115]\n",
      " [-0.26852426]\n",
      " [-0.06785401]\n",
      " [-0.2453634 ]\n",
      " [-0.20194566]\n",
      " [-0.11768013]\n",
      " [-0.0712596 ]\n",
      " [-0.24230741]\n",
      " [-0.18421991]\n",
      " [-0.2201908 ]\n",
      " [-0.14078067]\n",
      " [-0.16328244]\n",
      " [-0.38450173]\n",
      " [-0.08551128]\n",
      " [-0.07346488]\n",
      " [-0.5014145 ]\n",
      " [-0.10896389]\n",
      " [-0.36321378]\n",
      " [-0.3616226 ]\n",
      " [-0.31166887]\n",
      " [-0.11266901]\n",
      " [-0.13558511]\n",
      " [-0.2881206 ]\n",
      " [-0.10173196]\n",
      " [-0.2190509 ]\n",
      " [-0.41591674]\n",
      " [-0.2257063 ]\n",
      " [-0.31011683]\n",
      " [-0.47909585]\n",
      " [-0.09362537]\n",
      " [-0.31396684]\n",
      " [-0.15244593]\n",
      " [-0.39532733]\n",
      " [-0.2505742 ]\n",
      " [-0.18657678]\n",
      " [-0.5242508 ]\n",
      " [-0.25780112]\n",
      " [-0.12921922]\n",
      " [-0.22759332]\n",
      " [-0.1928093 ]\n",
      " [-0.23318422]\n",
      " [-0.11903939]\n",
      " [-0.1863627 ]\n",
      " [-0.11671758]\n",
      " [-0.16006356]\n",
      " [-0.09098816]\n",
      " [-0.43753332]\n",
      " [-0.40078768]\n",
      " [-0.11815488]\n",
      " [-0.42625955]\n",
      " [-0.19079119]\n",
      " [-0.2936315 ]\n",
      " [-0.17019741]\n",
      " [-0.13146427]\n",
      " [-0.22489165]\n",
      " [-0.22785895]\n",
      " [-0.04913407]\n",
      " [-0.2761654 ]\n",
      " [-0.23741801]\n",
      " [-0.1448063 ]\n",
      " [-0.24351808]\n",
      " [-0.23838119]\n",
      " [-0.42441076]\n",
      " [-0.1912611 ]\n",
      " [-0.17544468]\n",
      " [-0.18639357]\n",
      " [-0.32191613]\n",
      " [-0.5448693 ]\n",
      " [-0.05858466]\n",
      " [-0.14207439]\n",
      " [-0.18403749]\n",
      " [-0.29842484]\n",
      " [-0.1247959 ]\n",
      " [-0.151178  ]\n",
      " [-0.26799113]\n",
      " [-0.23153034]\n",
      " [-0.22258408]\n",
      " [-0.11455231]\n",
      " [-0.3154349 ]\n",
      " [-0.2709346 ]\n",
      " [-0.07949953]\n",
      " [-0.05729795]\n",
      " [-0.14931186]\n",
      " [-0.24079649]\n",
      " [-0.17640129]\n",
      " [-0.10953019]\n",
      " [-0.31854364]\n",
      " [-0.14301945]\n",
      " [-0.20992659]\n",
      " [-0.23809053]\n",
      " [-0.12542444]\n",
      " [-0.32508743]\n",
      " [-0.20600249]\n",
      " [-0.36395633]\n",
      " [-0.25966728]\n",
      " [-0.18972424]\n",
      " [-0.21430928]\n",
      " [-0.2399585 ]\n",
      " [-0.3112314 ]\n",
      " [-0.09922836]\n",
      " [-0.23207787]\n",
      " [-0.19898959]\n",
      " [-0.17325954]\n",
      " [-0.15573592]\n",
      " [-0.3637741 ]\n",
      " [-0.13725506]\n",
      " [-0.38183475]\n",
      " [-0.4224736 ]\n",
      " [-0.18020292]\n",
      " [-0.12295058]\n",
      " [-0.43070233]\n",
      " [-0.26392594]\n",
      " [-0.2199392 ]\n",
      " [-0.16067414]\n",
      " [-0.16604432]\n",
      " [-0.21322295]\n",
      " [-0.16337551]\n",
      " [-0.08311638]\n",
      " [-0.36664468]\n",
      " [-0.21690634]]\n",
      "tensor([[-1.9078, -0.8604, -0.4136],\n",
      "        [ 1.8877,  0.5566, -1.3355],\n",
      "        [ 0.4860, -1.5473,  1.0827],\n",
      "        ...,\n",
      "        [-0.9576, -0.5893, -0.4586],\n",
      "        [ 0.8567,  1.6774, -0.8052],\n",
      "        [ 1.0768, -2.1697, -1.1942]])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# Use Teacher CNN to generate a new dataset\n",
    "X_generated = torch.tensor(np.random.randn(TEST_SET_SIZE, 3), dtype=torch.float32)\n",
    "print(teacher_model(X_generated).detach().numpy())\n",
    "y_generated = (teacher_model(X_generated).detach().numpy() > 0).astype(int)  # Use trained model to create labels\n",
    "y_generated = torch.tensor(y_generated, dtype=torch.float32)\n",
    "print(X_generated)\n",
    "print(y_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define fully-connected student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with early stopping\n",
    "def train_model(model, X_train, y_train, optimizer, loss_fn, l1_lambda = 0):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(2000):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = torch.tanh(model(X_train))  # Apply sigmoid for binary classification\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "        # for the case where we have L1 norm\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss += l1_norm * l1_lambda #is zero if l1_lambda is 0\n",
    "\n",
    "        # end of that case\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Early stopping condition\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch}, best loss: {best_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    # Evaluate model\n",
    "    X_test = torch.tensor(np.random.randn(10, 3), dtype=torch.float32)\n",
    "    y_test_pred = (torch.tanh(model(X_test)).detach().numpy() > 0).astype(int)  # Convert to binary output\n",
    "    print(\"Binary Output Sequence:\", y_test_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0000\n",
      "Early stopping at epoch 10, best loss: 1.0000\n",
      "Binary Output Sequence: [0 0 0 0 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# \"Student\" Model (fully connected with same number of neurons as teacher)\n",
    "class DenseNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 4, bias=False)  # Fully connected layers without bias\n",
    "        self.fc2 = nn.Linear(4, 2, bias=False)\n",
    "        self.fc3 = nn.Linear(2, 1, bias=False)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))  # Sigmoid for binary classification\n",
    "        return x\n",
    "\n",
    "# Train the Student Model on the Generated Dataset\n",
    "student_model = DenseNN()\n",
    "sgd_optim = optim.SGD(student_model.parameters(), lr=0.01) #, weight_decay=1e-4) #L2 regularization\n",
    "loss_fn = nn.HingeEmbeddingLoss()  # Binary cross-entropy loss\n",
    "train_model(student_model, X_train=X_generated, y_train=y_generated, optimizer=sgd_optim, loss_fn=loss_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Model Weights:\n",
      "conv1.weight: tensor([[[ 0.6242, -0.6786]],\n",
      "\n",
      "        [[ 0.5425, -0.0254]],\n",
      "\n",
      "        [[-0.0823,  0.4446]],\n",
      "\n",
      "        [[-0.0646,  0.4403]]])\n",
      "conv2.weight: tensor([[[ 0.2979, -0.3002],\n",
      "         [ 0.1592,  0.0759],\n",
      "         [ 0.0830,  0.0675],\n",
      "         [ 0.1038, -0.1544]],\n",
      "\n",
      "        [[-0.0393,  0.3409],\n",
      "         [ 0.2412, -0.0113],\n",
      "         [-0.3310,  0.0182],\n",
      "         [-0.2393,  0.2901]]])\n",
      "fc.weight: tensor([[-0.4594, -0.0844]])\n",
      "\n",
      "Student Model Weights:\n",
      "fc1.weight: tensor([[-0.1281,  0.1027,  0.5433],\n",
      "        [ 0.0549,  0.3344,  0.4481],\n",
      "        [ 0.4661, -0.1994, -0.1291],\n",
      "        [ 0.2782, -0.1575,  0.2703]])\n",
      "fc2.weight: tensor([[-0.1092, -0.3391,  0.2035,  0.0767],\n",
      "        [ 0.2229,  0.4967,  0.3414,  0.4740]])\n",
      "fc3.weight: tensor([[ 0.0378, -0.6083]])\n"
     ]
    }
   ],
   "source": [
    "# print the weights\n",
    "print(\"Teacher Model Weights:\")\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data}\")\n",
    "\n",
    "print(\"\\nStudent Model Weights:\")\n",
    "for name, param in student_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Sparse Function, then train CNN and Dense NN on this and compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f(x) = sin(x1) + log(1 + abs(x2)) + x3^2\n",
    "\n",
    "Models learn: y = 1 if f(x) > 0, else 0 \n",
    "\n",
    "\n",
    "F(x) = Conv1(x) + ReLU(Conv2(x)) + x^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "X_train = torch.tensor(np.random.randn(TEST_SET_SIZE, 3), dtype=torch.float32)\n",
    "y_train = torch.sin(X_train[:, 0]) + torch.log(1 + torch.abs(X_train[:, 1])) + X_train[:, 2] ** 2\n",
    "\n",
    "y_train = (y_train > 0).float().unsqueeze(1)  # Label is 1 if positive, 0 if negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sparse function using a CNN\n",
    "class SparseCNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SparseCNN_2, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=4, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv1d(in_channels=4, out_channels=2, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=2, stride=1)\n",
    "        self.fc = nn.Linear(2, 1)  # Fully connected layer to output a single value\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension for CNN\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.tanh(self.fc(x))  # Apply tanh activation for positive/negative output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model with early stopping\n",
    "def train_model(model):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01) # Could vary here\n",
    "    loss_fn = nn.BCELoss()  # Binary cross-entropy loss\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = torch.sigmoid(model(X_train))  # Apply sigmoid for binary classification\n",
    "        loss = loss_fn(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Early stopping condition\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= PATIENCE:\n",
    "                print(f\"Early stopping at epoch {epoch}, best loss: {best_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    #Evaluate model\n",
    "    # Generate and print binary output sequence\n",
    "    X_test = torch.tensor(np.random.randn(10, 3), dtype=torch.float32)\n",
    "    y_test_pred = (torch.sigmoid(model(X_test)).detach().numpy() > 0).astype(int)  # Convert to binary output\n",
    "    print(\"Binary Output Sequence:\", y_test_pred.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN model with early stopping\n",
    "model = SparseCNN_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the weights\n",
    "print(\"Teacher Model Weights:\")\n",
    "for name, param in teacher_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data}\")\n",
    "\n",
    "print(\"\\nStudent Model Weights:\")\n",
    "for name, param in student_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
