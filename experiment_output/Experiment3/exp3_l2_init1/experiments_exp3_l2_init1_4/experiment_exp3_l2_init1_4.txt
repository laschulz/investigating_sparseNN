Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.3301512  -0.3301196   0.19602978]]]
layers.1.weight: [[[-0.2781349  1.0295225]]]
layers.2.weight: [[[ 0.20006615 -0.35124314]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.00362293 -0.00331839  0.00104498 ... -0.01994259 -0.02063347
   0.01283863]
 [ 0.00065826 -0.000336    0.         ...  0.0023874   0.0022793
  -0.00045345]
 [ 0.00012091  0.00012733  0.00016756 ...  0.00452176  0.00427612
  -0.00194433]
 ...
 [-0.00308693 -0.00202838  0.00169603 ... -0.01744229 -0.01667359
   0.01073618]
 [ 0.00191863  0.00192614 -0.00103615 ...  0.01602948  0.0163125
  -0.01009193]
 [ 0.00043359  0.001017   -0.00078935 ...  0.00297167  0.00370497
  -0.00193279]]
layers.1.weight: [[ 0.00518473 -0.00108257 -0.00153983 ...  0.00420019 -0.00416988
  -0.00080334]
 [-0.0067095   0.00039554  0.00132654 ... -0.00581251  0.00522087
   0.00141205]
 [ 0.00971796 -0.00152317 -0.00199849 ...  0.00826881 -0.00768254
  -0.00154001]
 ...
 [-0.00546924  0.          0.00073193 ... -0.0041475   0.0039735
   0.00047379]
 [-0.00956713  0.00113723  0.00216944 ... -0.00726902  0.00633753
   0.00145683]
 [-0.0076587   0.00015907  0.00226393 ... -0.00727083  0.0065907
   0.00156579]]
layers.2.weight: [[-0.04138577  0.04151708 -0.06724162 -0.04575932 -0.03943796 -0.0242347
  -0.01232051  0.05121543 -0.02755866  0.05603597 -0.06717514  0.0202026
  -0.05299141 -0.08610763  0.00687256  0.00295562 -0.01335532 -0.08395883
   0.00705342  0.05177365  0.00139678  0.02992094 -0.03549565  0.04593628
   0.00960014 -0.00226632 -0.09443358  0.04075824  0.03456644  0.03282557
   0.06035542  0.05578407]]

Final Loss: 0.0000
Distance Metric: 2.5614
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 571

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[ 0.00014339  0.          0.         ...  0.          0.
   0.        ]
 [-0.00012622  0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ... -0.00011177  0.
   0.        ]
 ...
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.         -0.00010473 ...  0.          0.
  -0.00012559]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[ 0.00026309  0.         -0.00028735  0.          0.          0.00012577
   0.00079764 -0.00011293  0.         -0.00037975  0.         -0.00022305
   0.         -0.00024829 -0.00038941  0.         -0.00059417 -0.00053999
  -0.00018292  0.          0.00028922 -0.00044219  0.          0.00032701
   0.          0.          0.0003363  -0.00021122 -0.00029129  0.00044455
  -0.00015807  0.        ]]

Final Loss: 0.0853
Distance Metric: 2.6873
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 900

================================================================================

baselineCNN_relu -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00138665 -0.00381868  0.00200011 ...  0.02224874  0.04089458
  -0.0584    ]
 [-0.01199507 -0.01036389  0.0244127  ... -0.04946213 -0.09180457
   0.1299375 ]
 [-0.04054114 -0.0824352   0.12321472 ... -0.00291954 -0.00722453
   0.01252572]
 ...
 [-0.0232169  -0.04788184  0.07119629 ...  0.00112639  0.00099975
  -0.00013375]
 [-0.03213603 -0.0680519   0.10058082 ...  0.00549644  0.00836651
  -0.01021727]
 [-0.02190343 -0.0500998   0.07248238 ...  0.01430953  0.02485865
  -0.034208  ]]
layers.1.weight: [[ 0.01185314 -0.02256144  0.02356351 ...  0.0147727   0.02359171
   0.02271156]
 [ 0.01259965 -0.02400712  0.02446798 ...  0.01537795  0.02459499
   0.02378381]
 [-0.01886411  0.03638977 -0.03069636 ... -0.01967862 -0.03187983
  -0.03194819]
 ...
 [-0.00867952  0.01631344 -0.01910726 ... -0.01187543 -0.01880206
  -0.01774544]
 [-0.01424823  0.02709118 -0.02643724 ... -0.01671675 -0.02676932
  -0.02610669]
 [-0.0166798   0.03195137 -0.02882922 ... -0.01836406 -0.02958208
  -0.02927202]]
layers.2.weight: [[-0.7362495  -0.77121     1.0480669  -0.8362741  -3.918585    0.70184493
  -0.7531929  -0.093342   -0.8879188   0.06892751 -0.83793926 -0.6910109
   0.92281353 -0.7386209  -0.9149917   1.0468574   1.0134726  -0.54391307
  -0.87061155  1.0132176   1.0258425   0.8982107  -0.66799164  0.06467626
   0.4846965   1.0438898  -0.33959237 -0.82572573  0.9060247   0.5843731
   0.8580981   0.96134657]]

Final Loss: 0.0038
Distance Metric: 21.4860
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 1568

================================================================================

