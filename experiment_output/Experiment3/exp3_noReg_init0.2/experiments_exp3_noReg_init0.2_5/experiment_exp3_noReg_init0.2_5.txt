Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.18215744 -0.05971599  0.04684201]]]
layers.1.weight: [[[0.06803157 0.1369415 ]]]
layers.2.weight: [[[ 0.22338672 -0.12639771]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.02435827 -0.01830606 -0.00443117 ...  0.01671107  0.00676773
   0.02640894]
 [ 0.02799744  0.023093    0.00598211 ... -0.02634297  0.0021474
  -0.02038253]
 [-0.01232263  0.01144745 -0.0001959  ... -0.02741918  0.02107635
  -0.00587324]
 ...
 [-0.01951717 -0.02699462  0.00086859 ...  0.00586893 -0.00572327
  -0.01809064]
 [ 0.0090235   0.00404515  0.02048045 ... -0.0167354   0.02375745
   0.01185713]
 [ 0.01294862  0.01226423  0.00975007 ...  0.01121948  0.0209232
  -0.00182509]]
layers.1.weight: [[ 0.02364209  0.00169374  0.00343608 ...  0.02057169  0.01562547
   0.00714783]
 [ 0.00922854 -0.0114022   0.00130413 ...  0.00696583  0.01785989
   0.01882229]
 [-0.01328535  0.00981998 -0.01042812 ... -0.02697673 -0.00502687
   0.0179491 ]
 ...
 [-0.00201586 -0.02270319  0.02383612 ...  0.01013056 -0.0107715
  -0.01699915]
 [ 0.00542927  0.02315571 -0.0050949  ...  0.02686174  0.00172154
   0.00935721]
 [ 0.02049949 -0.01764151 -0.02354238 ...  0.0110664  -0.02842245
   0.00824679]]
layers.2.weight: [[ 0.07514219  0.06229355 -0.03609089 -0.03209797 -0.03699606 -0.08838557
  -0.0640898  -0.02729502  0.06769985 -0.06074524 -0.07579761  0.05332128
   0.06905095  0.00837813  0.02832727  0.07069856  0.03923984  0.0042446
  -0.08258072  0.04174069  0.02889626  0.0089322  -0.01570268  0.05571082
  -0.03637397  0.02758839 -0.0451238   0.00199596  0.02841018  0.08285674
   0.04083496 -0.0739348 ]]

Final Loss: 0.0000
Distance Metric: 3.1354
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[ 0.12418226  0.2084678  -0.21845424 ... -0.12205333 -0.09957963
   0.05963666]
 [ 0.05451247 -0.31012708  0.21965984 ...  0.12800269  0.08193462
  -0.23494053]
 [-0.00552056 -0.32122695 -0.12910822 ... -0.15500112 -0.18089063
  -0.05823217]
 ...
 [ 0.10669833  0.27759075  0.15396869 ... -0.01443045  0.18915355
  -0.08165669]
 [ 0.00194446 -0.1845507   0.07503343 ...  0.21678375  0.24719152
   0.09583938]
 [ 0.06607648  0.03923511 -0.19742872 ... -0.34299594 -0.09820261
   0.10798504]]
layers.1.weight: [[ 0.30701607  0.17199813  0.2576698  ...  0.24336651  0.31053704
  -0.26734576]
 [-0.35099208  0.3104355  -0.30769497 ...  0.43013266  0.04122031
   0.03958302]
 [ 0.089817    0.18575272 -0.1605231  ...  0.22228949  0.16438176
  -0.2566174 ]
 ...
 [-0.10244426 -0.1116541   0.05508364 ...  0.07784258  0.05020256
  -0.01041812]
 [-0.08308874  0.1030754  -0.26268026 ... -0.10617802  0.15147935
   0.04852662]
 [ 0.10635098  0.04997608  0.13207388 ...  0.05495708  0.02343587
  -0.11984712]]
layers.2.weight: [[-0.48971507 -0.12888706 -0.1076417   0.097396    0.10440903 -0.45444056
  -0.2525862   0.09434669 -0.09587     0.12086581 -0.1147534   0.1039125
   0.09448179  0.10560458 -0.24316974 -0.5784262   0.09764198  0.09528835
  -0.3143254   0.09785628 -0.2344707  -0.23539448  0.08881294  0.08756685
  -0.415194    0.09793787 -0.25205308 -0.36827153 -0.37076488  0.08912632
   0.09460942  0.09916755]]

Final Loss: 0.0085
Distance Metric: 28.6192
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[ 0.02758144 -0.01121967 -0.00958649 ...  0.03807775  0.00691078
  -0.03633851]
 [ 0.00283673  0.02245662 -0.02620496 ...  0.00611961 -0.03570824
   0.01005833]
 [ 0.00314698  0.01110325 -0.00029032 ...  0.00596569 -0.01658862
  -0.00438061]
 ...
 [ 0.02494864 -0.00195998 -0.02223919 ...  0.0178916  -0.01474825
  -0.00250082]
 [ 0.02757372 -0.02642301 -0.03045595 ... -0.00301419  0.04009512
  -0.01109101]
 [-0.02622559  0.03670409 -0.02698322 ...  0.02165861  0.00868736
  -0.03148962]]
layers.1.weight: [[-0.03155249 -0.00694898  0.0125953  ... -0.00503203 -0.04222399
   0.004884  ]
 [-0.0395149   0.03826531  0.02279387 ... -0.02384995  0.00950968
   0.05600247]
 [-0.04612644  0.01690843  0.03806585 ... -0.04456801 -0.02070279
   0.01368865]
 ...
 [ 0.00539374  0.00968976  0.01243862 ... -0.03543716  0.01351751
   0.03957089]
 [-0.02089493 -0.01035004  0.02493556 ... -0.00014003 -0.04715279
   0.0101504 ]
 [-0.00318804  0.00069608  0.03169276 ...  0.02902981  0.03141193
  -0.00379617]]
layers.2.weight: [[ 0.001012   -0.00211598 -0.00540735 -0.00868907  0.0027194   0.00810915
   0.00447114 -0.00645669  0.00365855 -0.01154071  0.00299816 -0.01077629
  -0.00667707 -0.0063995   0.00307496  0.00957436  0.00429022 -0.0005306
   0.00053045  0.01704808 -0.00112385  0.00307399 -0.01172548 -0.01977253
  -0.00011688 -0.00802441  0.00169667  0.00638242  0.00146541  0.00499274
  -0.00940446 -0.01403498]]

Final Loss: 0.2491
Distance Metric: 4.2689
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 125

================================================================================

