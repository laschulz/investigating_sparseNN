Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.15554956 -0.0532649   0.12202001]]]
layers.1.weight: [[[-0.14661655 -0.02391262]]]
layers.2.weight: [[[-0.09776441 -0.23804806]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.00805063 -0.02544513 -0.01129393 ...  0.003671   -0.02497688
   0.01633505]
 [-0.010164   -0.02866564  0.02679813 ... -0.01668755  0.01085314
  -0.02208887]
 [-0.01625956 -0.0104606   0.01781858 ...  0.00735272  0.00511943
   0.02743906]
 ...
 [-0.01865171 -0.02338171 -0.00532205 ...  0.01894238 -0.00971382
   0.00024468]
 [ 0.01540724  0.0046011   0.01748723 ... -0.00155426 -0.00793447
   0.00077319]
 [ 0.00947537 -0.01801466  0.00250456 ...  0.02564754  0.02515913
   0.01925307]]
layers.1.weight: [[ 0.01711961  0.01379164  0.02815913 ...  0.02007621 -0.01549455
   0.0122396 ]
 [-0.00902235  0.01370719  0.00632654 ...  0.00983388  0.02529021
   0.00989527]
 [-0.02762943  0.00423094 -0.00956266 ... -0.01469493  0.0002226
  -0.02499972]
 ...
 [ 0.00389872 -0.01963521 -0.00448269 ... -0.01296635  0.00868269
   0.02619067]
 [ 0.01196059  0.01711657  0.0155517  ... -0.01598111 -0.01833838
  -0.01730854]
 [-0.00031221 -0.02206165  0.02019796 ... -0.02399156  0.00156348
  -0.02438225]]
layers.2.weight: [[ 0.0088354  -0.10233036  0.03977501  0.03912428  0.04248197 -0.04040643
   0.00664308  0.05359286  0.05419229  0.0065136   0.04364547 -0.07614248
   0.0135151  -0.05048145 -0.0403076  -0.01350135 -0.03342415  0.03861028
  -0.04402324 -0.06952574 -0.04042376 -0.08447087  0.01894267  0.0288289
   0.03837126 -0.07965239  0.01641284  0.01506623  0.01261189 -0.1033807
   0.02426563 -0.03076948]]

Final Loss: 0.0000
Distance Metric: 3.1107
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[ 0.09285973  0.09250324  0.08803333 ... -0.2688903   0.04195454
   0.01110341]
 [-0.13402852  0.03415578 -0.41245022 ...  0.04441605  0.05807864
   0.05187759]
 [-0.2266405  -0.11284323 -0.14219311 ... -0.21805023 -0.28368747
   0.08357839]
 ...
 [ 0.0065829  -0.3506454   0.1839176  ... -0.17141747 -0.23263375
  -0.21353894]
 [-0.3518501   0.28715065  0.05497272 ... -0.0009148   0.24799864
   0.08884858]
 [ 0.18324251 -0.01666888 -0.14969945 ...  0.24554065 -0.19413921
  -0.03076301]]
layers.1.weight: [[-0.11425757 -0.03368789 -0.20319025 ... -0.1055789   0.02520461
  -0.11073379]
 [-0.0999173  -0.06060237  0.02004115 ...  0.11455056 -0.04759608
   0.1395301 ]
 [-0.2059904  -0.20439804  0.1093957  ... -0.12101457  0.11654251
   0.01492614]
 ...
 [-0.06820471 -0.08179584 -0.25947842 ... -0.5463472   0.3053302
   0.02845485]
 [-0.22556637 -0.00996443  0.05327607 ... -0.07164769 -0.03929362
   0.04338   ]
 [ 0.03767536  0.5878895   0.08901907 ... -0.2836647  -0.2870623
  -0.27978942]]
layers.2.weight: [[ 0.10907649  0.1067951  -0.11768539 -0.11221989 -0.20009117 -0.32904345
   0.0903952  -0.45080966 -0.14984961 -0.50991935  0.10890399  0.09434386
  -0.17218892  0.0882525  -0.13329998 -0.49884495 -0.13823283 -0.20104358
   0.11927442  0.09444588  0.11150306 -0.13817573 -0.10010508 -0.18072785
  -0.24768308  0.10259215  0.09815274 -0.13109733  0.11405666 -0.19327408
   0.11092158 -0.16941981]]

Final Loss: 0.0083
Distance Metric: 28.2289
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[ 0.01340922 -0.02342035 -0.01394375 ...  0.00894176  0.00123949
   0.00495413]
 [-0.00977545  0.02257721  0.00306815 ...  0.01572628 -0.0039176
  -0.01780962]
 [ 0.00963381  0.01214583  0.01318019 ... -0.01935492 -0.01186491
   0.00291951]
 ...
 [-0.03305742  0.0084389  -0.0175518  ... -0.01685518  0.00949059
  -0.02221528]
 [ 0.01212302 -0.00754556  0.00039166 ... -0.00833255 -0.02259806
   0.02120378]
 [-0.0492535  -0.02805947 -0.00097389 ... -0.01668645  0.01083988
   0.03081831]]
layers.1.weight: [[-0.02590126  0.0017701   0.03720459 ... -0.02920645  0.01315266
  -0.02157875]
 [-0.00475057  0.02146635 -0.01643552 ... -0.02137968 -0.02976163
  -0.02676636]
 [-0.0541904   0.02855261 -0.03011782 ... -0.0469832  -0.03957999
  -0.03272272]
 ...
 [-0.00205024  0.04185862  0.02094322 ...  0.02729258  0.00390237
   0.03014388]
 [-0.03209102 -0.01619058  0.03324306 ... -0.00481703  0.02010565
   0.03390007]
 [-0.01414347  0.01477672  0.0322208  ... -0.02485699  0.00733414
   0.0323893 ]]
layers.2.weight: [[ 0.0029046   0.00379972 -0.01255484 -0.00723568  0.00209047 -0.0091457
   0.00156037  0.00235314  0.01110279  0.01412887 -0.01084674 -0.0099458
   0.00306705 -0.0037843   0.01473144  0.01269524  0.00365785 -0.00910295
   0.02163535 -0.01618592 -0.00486723  0.01144045 -0.00887325 -0.00617052
  -0.00106893  0.00537637 -0.00866237  0.00177252 -0.00772669  0.00765949
  -0.00602748  0.00313161]]

Final Loss: 0.2437
Distance Metric: 3.7563
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 410

================================================================================

