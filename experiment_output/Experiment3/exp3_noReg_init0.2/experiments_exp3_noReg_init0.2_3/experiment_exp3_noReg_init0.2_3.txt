Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.0940784   0.19681509  0.08915999]]]
layers.1.weight: [[[-0.03762013  0.04123275]]]
layers.2.weight: [[[-0.1918266  -0.23791492]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.00014891  0.0083266  -0.0282703  ... -0.02961185 -0.02942412
   0.01840571]
 [ 0.01386832 -0.02749925  0.0263436  ...  0.01641821 -0.00183741
  -0.01367117]
 [ 0.02738456 -0.00971848  0.02469387 ... -0.00861547  0.02642604
   0.01599442]
 ...
 [ 0.00528194 -0.01294157 -0.00336624 ... -0.00542603 -0.00327953
   0.00189696]
 [ 0.0128298   0.01596864  0.01289697 ...  0.00179841  0.01325928
  -0.01305186]
 [ 0.01425878 -0.01340975 -0.01197021 ...  0.01293208 -0.02747915
   0.01918925]]
layers.1.weight: [[-0.02625632  0.02577486  0.02695403 ... -0.01470688  0.02811676
   0.00059032]
 [ 0.0065038  -0.00325847 -0.02668433 ...  0.02714073  0.01181082
   0.02813848]
 [-0.00343534 -0.01254474  0.01363521 ... -0.00347361  0.00382333
  -0.0192861 ]
 ...
 [ 0.01528104 -0.00468608 -0.00986132 ... -0.00538004  0.0057004
   0.02822474]
 [ 0.00434926  0.02336976  0.01979829 ...  0.00517953  0.00839451
   0.01003493]
 [ 0.0044803  -0.02244262  0.00556156 ...  0.00569275  0.02343899
   0.01534566]]
layers.2.weight: [[-0.04747596 -0.04985728  0.00617335 -0.09228079 -0.0373926   0.0310032
   0.04213745 -0.00535509 -0.03323555 -0.05835499  0.05498717  0.03716154
  -0.00041054  0.0493028   0.04325279  0.00692989  0.06288382 -0.05819744
   0.05659276 -0.04975465 -0.07914463  0.01677785 -0.07310553 -0.00897773
  -0.03265678 -0.10178752  0.015389   -0.04266323  0.03429146 -0.02370722
  -0.00124666 -0.0757008 ]]

Final Loss: 0.0000
Distance Metric: 3.0651
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[ 0.13453169  0.21847877  0.09815219 ... -0.09489687 -0.02651029
  -0.0718753 ]
 [-0.07245679 -0.1809827  -0.03705394 ...  0.06147218  0.05629018
   0.09901827]
 [ 0.27413985  0.02118781  0.13345124 ... -0.08070642  0.12632531
   0.17192478]
 ...
 [-0.2899097   0.01371899 -0.03666748 ...  0.33351082  0.19427525
  -0.05481363]
 [ 0.1350283  -0.11246651  0.09707899 ... -0.20326197  0.03678582
   0.16609178]
 [-0.09307034 -0.19062506 -0.11840566 ... -0.01476805  0.21670547
   0.09713425]]
layers.1.weight: [[ 0.15677424  0.02020547  0.09379162 ... -0.25206113 -0.09207425
   0.08265455]
 [-0.0236586  -0.08333534  0.13032033 ...  0.15666807 -0.16668153
  -0.35028404]
 [-0.17303808  0.3721157   0.10741602 ...  0.06390245  0.04201006
   0.06404089]
 ...
 [-0.0371806  -0.04475495  0.34426388 ... -0.32767916  0.19434106
  -0.09720739]
 [ 0.00972703 -0.18460348 -0.23371221 ... -0.26328266 -0.05728377
  -0.10544694]
 [-0.2764228  -0.14723927 -0.11198638 ... -0.07317428  0.1434576
  -0.1165918 ]]
layers.2.weight: [[ 0.11075888  0.10182536 -0.1354797   0.09820761 -0.10014291 -0.1155136
   0.10881943  0.09216735  0.10017692  0.1007784   0.10643615  0.09626548
   0.09752088  0.1494288  -0.14963506 -0.10058871 -0.35725188  0.12095802
   0.10687437 -0.25342968 -0.10463207  0.10557597 -0.11753412  0.10954841
  -0.15814945  0.09901157 -0.23932447  0.10411931 -0.14509907 -0.14139524
  -0.11091628  0.15527868]]

Final Loss: 0.0096
Distance Metric: 28.3618
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[-0.00763272 -0.01713737  0.00397622 ...  0.01232707  0.01254483
  -0.01431696]
 [ 0.0042624   0.0147551   0.01446544 ... -0.00280636 -0.02229965
  -0.001421  ]
 [ 0.03272835  0.03731762 -0.01470279 ...  0.02243555 -0.00196885
   0.01170282]
 ...
 [ 0.00839785  0.02150218  0.00561516 ...  0.01406972  0.02594973
  -0.02070547]
 [ 0.00339117  0.02299186 -0.01039192 ...  0.0290493  -0.01545196
   0.01066183]
 [-0.0242436  -0.01876964  0.00123194 ... -0.00775597 -0.02310943
  -0.01349253]]
layers.1.weight: [[ 0.04377098 -0.0162454  -0.00430077 ...  0.02604005  0.0181972
   0.04332858]
 [-0.02811991 -0.00076538  0.01511274 ... -0.02366811  0.03264832
   0.03010561]
 [-0.00198922  0.00264596  0.02025197 ...  0.01947576 -0.03982184
   0.03611337]
 ...
 [-0.00484492 -0.03984147 -0.04007887 ...  0.02204794 -0.01304151
   0.00024364]
 [-0.01248067 -0.01174952  0.01616349 ...  0.01030276  0.02945421
   0.00503231]
 [ 0.04750826  0.02724689  0.00749387 ... -0.00308485 -0.01079135
  -0.0152156 ]]
layers.2.weight: [[ 0.00662782  0.03551638  0.01694112  0.00105699  0.02802744 -0.01950669
   0.02092639 -0.01297381 -0.03517831 -0.01922585 -0.00051518 -0.02339483
  -0.02411387 -0.00500891  0.03204104 -0.00025756  0.00466441  0.01275547
   0.00258321  0.          0.00612618  0.00229654 -0.02878477  0.01651511
   0.00311148 -0.01968074  0.003758   -0.0024254   0.02268764 -0.00958407
   0.00778681 -0.01472973]]

Final Loss: 0.2782
Distance Metric: 3.8664
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 204

================================================================================

baselineCNN_relu -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.023469    0.01227685 -0.01490486 ...  0.02155822 -0.01788385
  -0.00917429]
 [-0.01867211  0.01852702 -0.00945238 ...  0.01265239  0.01390454
   0.02624809]
 [ 0.00962031 -0.00254237 -0.02870422 ...  0.01616377 -0.01942069
  -0.01423724]
 ...
 [-0.01019837  0.02875046 -0.02423935 ...  0.00345919 -0.01243381
   0.01186192]
 [-0.02528555 -0.00642244 -0.02252096 ...  0.01443008 -0.01293299
  -0.02744918]
 [-0.01780825 -0.00850571 -0.02800282 ...  0.00860989 -0.01937149
   0.02461583]]
layers.1.weight: [[-0.02099187 -0.02136252  0.01821655 ...  0.03024954  0.02077155
   0.0078125 ]
 [ 0.03700084  0.03422589  0.00094694 ... -0.0107559  -0.00219848
  -0.01685092]
 [-0.00945366  0.02439092 -0.02056925 ... -0.02115831  0.02960528
   0.01344122]
 ...
 [ 0.02176647  0.02068126  0.0360962  ...  0.0331956  -0.00986727
  -0.01300289]
 [-0.00035855  0.02475559  0.03171857 ... -0.01741501 -0.01463072
   0.01909193]
 [ 0.02053945 -0.0206533   0.03235049 ...  0.01772818 -0.01422998
   0.0078125 ]]
layers.2.weight: [[-0.2377884  -0.40175018 -0.25       -0.24562614 -0.40428945 -0.41701344
  -0.30651206 -0.29524755 -0.3563855  -0.39718854 -0.35193083 -0.28436258
  -0.30220276 -0.37449428 -0.26699877 -0.34733886 -0.26236844 -0.25
  -0.24984473 -0.2459979  -0.43013555 -0.4316108  -0.35194325 -0.28785866
  -0.3706872  -0.30000764 -0.39975443 -0.4291758  -0.33961362 -0.30049482
  -0.32885554 -0.27038324]]

Final Loss: 0.0000
Distance Metric: 5.3703
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[-0.00367083 -0.01795969 -0.00848023 ... -0.01571723 -0.0058508
   0.00841181]
 [ 0.02496666  0.00143333 -0.0045662  ...  0.01779659  0.00424954
   0.0273765 ]
 [-0.01804425 -0.00742498 -0.01130071 ...  0.00347159  0.00963002
  -0.00103282]
 ...
 [-0.01103349 -0.01002531  0.01223619 ...  0.00784862 -0.00881918
   0.00518528]
 [-0.01384426  0.00539262 -0.00801277 ... -0.01970401 -0.00089209
  -0.00347839]
 [-0.01274237  0.0081756   0.00895607 ... -0.00292556 -0.00113846
  -0.02906427]]
layers.1.weight: [[-0.020204    0.02388796  0.04686818 ...  0.06442742  0.11174065
  -0.03320663]
 [ 0.04082271  0.06756186 -0.01613144 ...  0.01984951 -0.0831794
   0.04501745]
 [-0.0392417  -0.174972   -0.00132663 ... -0.00865052 -0.07816328
  -0.01972   ]
 ...
 [-0.01116185  0.0997738   0.02341463 ... -0.03716736 -0.06215689
   0.04020265]
 [ 0.05336137  0.05061766 -0.10170711 ... -0.07868682 -0.03195784
  -0.12097052]
 [ 0.05623415  0.08607222  0.0392186  ... -0.02672668  0.00537502
   0.02170602]]
layers.2.weight: [[ 1.2985232e-01 -1.6786718e-01  3.2696206e-02 -1.3418926e-01
  -1.5683293e-01  7.1275961e-01  3.6738056e-01 -4.7094738e-01
  -6.6004053e-02  7.7352389e-03  1.8498781e-01  4.5916844e-02
  -1.5379906e-01  7.3990084e-02 -2.5100464e-01 -2.3865357e-02
   2.6731769e-02 -3.1879064e-01  1.6507764e-01 -3.6763504e-01
  -6.4493239e-02  1.4754637e-01  1.9691920e-02 -1.3413630e-01
   2.4874827e-02  3.3135855e-01 -1.8684807e-01 -2.8451236e-02
  -5.0218350e-01  3.3190918e-01 -1.4664470e-03  5.8809330e-04]]

Final Loss: 0.0000
Distance Metric: 6.9503
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[ 0.03039332  0.02605307 -0.04374632 ...  0.02391537  0.00687672
  -0.03208207]
 [-0.0150807  -0.04628199  0.00877868 ... -0.0372178  -0.02951328
   0.04700091]
 [ 0.05098777 -0.00766075  0.03746276 ... -0.05371221  0.01377911
  -0.01981028]
 ...
 [ 0.02306459  0.03944132 -0.01636229 ... -0.03719044  0.01539438
  -0.0407603 ]
 [ 0.04266129 -0.0379007   0.02610582 ...  0.03903984 -0.04365069
  -0.00724434]
 [-0.00955362 -0.03758488 -0.03709559 ... -0.05329907 -0.04932256
   0.01747562]]
layers.1.weight: [[-0.01567273 -0.01531089  0.00849042 ... -0.03299791 -0.01201704
   0.01598896]
 [-0.02199076  0.0143078  -0.03475396 ... -0.00303907  0.03474205
  -0.01292014]
 [-0.0215562   0.04724392  0.03958787 ...  0.03527347  0.03721496
   0.01141986]
 ...
 [-0.0333556   0.04132527 -0.02486323 ...  0.01538148  0.02777863
  -0.01664067]
 [ 0.013807    0.00599612  0.0412121  ...  0.02258958  0.00922807
   0.02317703]
 [-0.00142993  0.03203778 -0.01828343 ...  0.03999311 -0.045506
  -0.04259023]]
layers.2.weight: [[-0.04804884 -0.04157513 -0.04672442 -0.02982616 -0.01641863  0.10034606
   0.06940665  0.07066973  0.08799812 -0.01205218 -0.05729477 -0.02697025
   0.00501247 -0.09413837  0.08741388  0.0013122  -0.09212986  0.06131803
  -0.02067221  0.0200596   0.09635302  0.04026328 -0.10270739 -0.09717095
  -0.06629944 -0.04215362  0.08308171  0.007786    0.05525482 -0.0538453
   0.0817621   0.01438918]]

Final Loss: 0.0000
Distance Metric: 4.8600
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

