Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.9141675  0.6294384 -0.6204891]]]
layers.1.weight: [[[ 1.1834604  -0.79461116]]]
layers.2.weight: [[[-1.047713   -0.25362793]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.11596186  0.00677078 -0.08091039 ... -0.05176969  0.00616657
  -0.10863377]
 [ 0.00922803  0.03607019 -0.08825684 ... -0.14987677  0.01111806
   0.10594703]
 [ 0.19296613  0.19927053 -0.17685951 ...  0.14763372 -0.03509612
   0.04536061]
 ...
 [ 0.06292178 -0.12381174 -0.0950136  ... -0.130212   -0.04501738
   0.10529177]
 [-0.07743075 -0.01626467 -0.06760845 ...  0.16145594  0.14734384
  -0.11584323]
 [ 0.09763394 -0.02126503 -0.09160304 ...  0.08360394  0.126172
   0.01929472]]
layers.1.weight: [[ 0.03409196 -0.0453313  -0.00865284 ...  0.11624423  0.02508534
   0.02123021]
 [-0.12145557  0.02744355 -0.1359557  ... -0.05208448  0.06775038
  -0.01172222]
 [-0.06441421  0.1351323  -0.01767982 ...  0.00177891 -0.02447727
   0.05625   ]
 ...
 [-0.11178102  0.01271593  0.11260631 ... -0.06174042 -0.13239111
   0.04158948]
 [ 0.03867204  0.02603188  0.1114182  ...  0.08969495 -0.14055245
  -0.01123075]
 [-0.06695063  0.04990947  0.06655028 ...  0.03193391  0.01013912
   0.13669677]]
layers.2.weight: [[-0.4340927   0.33287913  0.37302172  0.40117955  0.2080161   0.10444993
  -0.20070389  0.22619326 -0.02435437  0.47425976 -0.13654926 -0.19570936
   0.02635212 -0.309842   -0.23290946  0.03101085  0.27943176  0.08254314
  -0.08537438  0.10606964  0.25275367  0.19248322 -0.41434658 -0.00390553
   0.17641155 -0.63091755 -0.11006472  0.08531731 -0.4288768  -0.30198264
   0.16680971 -0.53056353]]

Final Loss: 0.0000
Distance Metric: 15.7135
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[ 0.01529323  0.06309035 -0.0557071  ... -0.02612596  0.01263146
  -0.02913882]
 [ 0.00089144 -0.15462449  0.13254905 ...  0.0785742  -0.02236757
  -0.15308587]
 [ 0.09936672 -0.09574034 -0.08210564 ...  0.06378649  0.01196794
  -0.05163131]
 ...
 [-0.07017232 -0.00397302  0.04844194 ...  0.04862936  0.06150832
  -0.05531839]
 [-0.05109058  0.09134673  0.08240531 ...  0.07015604  0.24830571
  -0.03982739]
 [-0.02669867 -0.00516244  0.07061228 ... -0.22317706 -0.00801126
  -0.0010344 ]]
layers.1.weight: [[-0.16710475  0.03253261  0.2487432  ...  0.0973589   0.4644668
  -0.11007323]
 [ 0.05705134  0.46677    -0.29829398 ...  0.2346913  -0.2344821
  -0.00380676]
 [ 0.06745856  0.2824145  -0.08551038 ...  0.04121376  0.17182338
  -0.24981907]
 ...
 [-0.11746721  0.02655424 -0.13690715 ...  0.61796254 -0.15901807
  -0.0788308 ]
 [-0.06468211 -0.19217981 -0.40363464 ...  0.07224295  0.02599264
  -0.19028027]
 [ 0.27945444 -0.25552136 -0.12535296 ... -0.31071657 -0.04966981
   0.12073562]]
layers.2.weight: [[-2.4654489  -0.16234294 -0.16676249  0.3382878   1.2182137  -2.2098408
  -1.3516219   0.28780267 -0.5001479   0.29791868 -1.1891211  -0.00702777
   1.9567498   1.3850672  -0.9347626  -1.2760897  -0.31566992  0.7740497
  -1.6189605   1.8641411  -1.5160847  -1.8021636   0.83038795  0.46688232
  -2.661329    1.193076   -0.9359573  -1.9279666  -2.8013146   0.68025684
   0.66537094  0.579346  ]]

Final Loss: 0.2464
Distance Metric: 37.0501
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 295

================================================================================

baselineCNN_sigmoid -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[-0.10660227  0.15714718 -0.04333931 ... -0.41055566 -0.23703204
  -0.15541358]
 [ 0.33092228 -0.12451076  0.11815614 ...  1.5215392   0.74204934
  -1.9384434 ]
 [ 1.2546551   0.6845121   0.66392124 ... -1.0029562   2.522046
   2.0958612 ]
 ...
 [ 0.23944691 -0.30211258 -1.4300388  ...  0.32045275  0.36088786
   0.08359316]
 [ 0.592604    0.94258106  3.064885   ...  1.1018007  -3.2544606
  -0.6827403 ]
 [-1.6627139  -4.5112762  -0.60925627 ...  2.896254    0.47282773
   0.0984816 ]]
layers.1.weight: [[ 0.01268119 -0.01093616  0.00770479 ...  0.01178472 -0.00869635
   0.01281265]
 [ 0.23982336 -0.27369583 -1.1927067  ... -0.42082003 -1.0330675
   0.04051451]
 [-0.1267169   0.54345065 -0.24332877 ... -0.41642517  0.58434075
   0.05385153]
 ...
 [ 0.02902534  0.05799063 -0.00607184 ... -0.01731205  0.00632786
   0.00656699]
 [-0.02072519  0.02384454  0.03415791 ... -0.01662439  0.01451937
  -0.02573333]
 [ 0.04089848 -0.0014617  -0.00831947 ...  0.02645037  0.01976637
  -0.01195397]]
layers.2.weight: [[-1.4691538e-02  1.5263800e-01 -9.1730423e-02  1.8287823e-02
  -1.1525415e-02 -7.2449900e-02 -1.3824619e-02  1.4368600e-01
   1.9200506e-02  2.6902847e-02 -1.1880821e-01  5.6037707e-03
   2.9944886e-02  1.1792723e-02 -1.5562454e-02 -3.6236190e-03
  -7.5733028e-03 -6.1783649e-04 -1.2957901e-02 -1.8237005e-01
  -2.8680488e-03 -2.5840037e-04  1.6127972e-02 -7.7142091e-03
  -2.6687225e-03  1.1692518e-04  5.2920529e-03 -1.4395573e-02
  -4.4926558e-02 -1.9449797e-02  1.5441922e-03  1.4466761e-02]]

Final Loss: 0.1869
Distance Metric: 102.0802
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[ 8.80171955e-02 -6.52744398e-02  4.01646160e-02 ... -4.39549088e-02
  -7.81724527e-02 -2.52070487e-01]
 [ 1.85977399e-01  4.96754125e-02 -2.41365418e-01 ... -5.49465884e-04
  -2.45380774e-02  3.02740365e-01]
 [-1.05700325e-02 -2.48611905e-03 -1.94949090e-01 ... -3.91418301e-02
   1.18979625e-01 -1.82085246e-01]
 ...
 [ 4.52923588e-02 -1.91167537e-02 -7.73399100e-02 ... -2.59034485e-01
   4.67708446e-02  3.93568099e-01]
 [-9.41143334e-02 -1.12403654e-01  4.99072075e-01 ... -3.07739854e-01
   1.23566546e-01  4.32131708e-01]
 [ 3.41669381e-01  7.43278712e-02 -6.67131126e-01 ... -1.56286463e-01
   4.24867049e-02  1.27406284e-01]]
layers.1.weight: [[-0.0414596   0.00798462 -0.03973885 ...  0.10305052 -0.16287614
  -0.07989667]
 [-0.02387298  0.01266963  0.0372909  ... -0.13966683 -0.00482174
  -0.04127716]
 [ 0.15710399  0.01491129  0.02168086 ... -0.11231725  0.04723262
  -0.2653683 ]
 ...
 [ 0.30206993 -0.12717257 -0.05466345 ... -0.2446681   0.27224454
  -0.0504485 ]
 [-0.05858584  0.10135043  0.07183662 ... -0.06200441  0.06904573
  -0.05369147]
 [-0.12942098 -0.06377102 -0.03417509 ... -0.11256348 -0.08502173
   0.01493706]]
layers.2.weight: [[ 1.1322725   0.81871676 -1.8439167  -0.13580145 -4.887437    2.8700578
   4.816459    1.6179457   1.4883716  -1.3300248  -1.901783    1.0938993
   0.890135   -2.8733618   3.1387436   1.786839    4.5905323  -1.6078544
  -0.767083   -4.591832    0.5340581  -3.7567134   1.8981663   2.8928242
   0.49270338  2.3250067   1.1235662   2.5401123   1.3903655  -3.2344246
   0.0267148  -0.7335571 ]]

Final Loss: 0.0809
Distance Metric: 54.7402
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[-0.1416987  -0.0244568   0.17109177 ... -0.17626444 -0.12808523
  -0.08677261]
 [-0.03534873 -0.09538242 -0.17272156 ...  0.00637641  0.21205562
   0.07107012]
 [ 0.01715142  0.11662506  0.05224743 ...  0.02533272  0.07563979
  -0.02006932]
 ...
 [ 0.06756952 -0.09242526  0.04240333 ... -0.02521751  0.04347907
  -0.10512105]
 [ 0.12322183 -0.12287386 -0.15668496 ... -0.12500829 -0.01035804
  -0.045545  ]
 [ 0.10252687 -0.09915287 -0.03656742 ... -0.03055003  0.04240263
   0.14830483]]
layers.1.weight: [[ 0.10480793 -0.06318408  0.1871816  ... -0.24254426  0.25924793
   0.07724411]
 [-0.39018086  0.30492568 -0.261392   ... -0.28039783  0.28843638
  -0.16667666]
 [ 0.13551392  0.72967577 -0.1376943  ... -0.04614971  0.02218585
   0.21572408]
 ...
 [-0.20054552 -0.09791882 -0.13939315 ...  0.01452012  0.34134632
  -0.60441035]
 [ 0.694811    0.03647611 -0.01776537 ...  0.13204081  0.07175877
  -0.00636655]
 [-0.04233806 -0.16548197 -0.30415753 ... -0.1928798  -0.25721407
  -0.22844455]]
layers.2.weight: [[-0.01747632 -1.4510803  -0.72622985 -0.7281459  -2.6366765   0.97543854
  -1.9681056  -0.16094255  0.07111185  2.1905782  -0.36281136  1.7675626
   1.344682    0.31318125  1.4406773  -0.33141676  2.6495535   0.39346236
  -1.3910435  -0.5955833  -2.0412736  -0.55848294  2.5547404   1.9910921
   0.45467713  0.14037734  0.52515    -2.4037545   1.4010075   1.6009557
  -0.5264816  -0.05023153]]

Final Loss: 0.0000
Distance Metric: 40.0773
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 101

================================================================================

baselineCNN_relu -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[ 0.13201682  0.2793828   0.77541906 ...  0.2571985  -0.18340522
  -0.02957124]
 [ 0.27376726  0.23123918 -0.22639182 ...  0.4931986   0.49478838
  -0.04998551]
 [ 0.6291755  -0.65672827 -0.06777834 ... -0.32189617 -0.32234213
  -0.8696717 ]
 ...
 [ 0.03940127  0.03117183  0.00915911 ...  0.5210542   0.43224138
  -0.36301196]
 [ 0.33608598  0.1972084   1.0398827  ...  0.36982864  0.46392182
   0.4745626 ]
 [ 0.1091091   0.735499    0.03941881 ... -0.34643474 -0.11772114
   0.24673739]]
layers.1.weight: [[ 0.14918837  0.29387033 -0.08328186 ... -0.15589216  0.33908635
  -0.3692441 ]
 [ 0.13748565  0.1800945   0.1464313  ...  0.09433999  0.01863915
  -0.03611456]
 [-0.03859228 -0.13532215  0.02939079 ... -0.01127844 -0.03637191
   0.01271091]
 ...
 [-0.17729807 -0.03355791  0.10761639 ... -0.19083382  0.0299786
  -0.1076611 ]
 [ 0.13777868  0.11631098  0.04351866 ...  0.03590628  0.24150053
  -0.3861543 ]
 [ 0.05232373  0.10620905 -0.0644715  ...  0.046292   -0.09381168
  -0.06763794]]
layers.2.weight: [[ 0.11086416  0.01740754  0.00172558  0.05282519  0.13559276 -0.00634934
   0.0014642   0.01986712  0.26756716  0.19221205  0.21265952  0.14267191
   0.00229241  0.02684352 -0.00665918 -0.00149219 -0.00311505 -0.00514875
  -0.34650207 -0.00413735 -0.00592512 -0.00892811 -0.3545452   0.03055856
   0.0110741   0.00321013 -0.00526534  0.01532548  0.259759    0.00044927
  -0.10956932  0.00389064]]

Final Loss: 0.0044
Distance Metric: 42.6178
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.09289229 -0.35878357 -0.265984   ...  0.07039969 -0.0808661
  -0.03452478]
 [ 0.13548566  0.01744256 -0.01006985 ... -0.13335812 -0.0326197
  -0.10643167]
 [ 0.12762702 -0.67181534 -0.31587058 ...  0.1104447   0.01068018
  -0.05720561]
 ...
 [-0.10176622 -0.5383634  -0.2814019  ... -0.00268135 -0.07662172
   0.08340278]
 [ 0.06693336 -0.07060724  0.00603553 ...  0.0993055   0.09657505
   0.08873665]
 [-0.09316161  0.0933968  -0.12357783 ... -0.1380496  -0.00545094
   0.05552035]]
layers.1.weight: [[-0.02420141  0.09524205  0.00728198 ...  0.1731098  -0.02314959
  -0.0025845 ]
 [ 0.04421993  0.04052536 -0.02659283 ... -0.13446595  0.09914947
  -0.05991288]
 [ 0.09339821  0.13540168  0.09480742 ... -0.0927253  -0.06155266
  -0.14507438]
 ...
 [ 0.23329811 -0.08412783  0.21005282 ... -0.02014988 -0.09805708
  -0.13188143]
 [ 0.05521268 -0.07313342 -0.13446222 ... -0.02721639 -0.04198825
  -0.14332049]
 [ 0.21706244  0.05519512 -0.17075807 ...  0.0047163   0.09681014
  -0.09384862]]
layers.2.weight: [[-1.4092873   0.04365925  0.14084353 -0.5921237  -2.930319   -2.2634766
  -1.0826169  -0.13134888  0.19968027 -0.56393397 -1.8375399  -0.8048928
  -2.5857098   0.10298787  0.06278601 -0.1004552   0.07173254  0.17774054
   0.45284906 -0.8562629  -0.08500049 -1.5293461  -1.4077249   0.28316668
  -0.5620947  -0.16039313 -0.9643854  -0.1764674  -1.121399   -1.1179203
   0.14924455 -1.1738492 ]]

Final Loss: 0.0224
Distance Metric: 28.9454
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[ 0.3550201   0.03322922  0.02779135 ...  0.06572413  0.04087507
  -0.03960266]
 [-0.11482436  0.28872916  0.01851203 ...  0.0870377   0.1659343
  -0.1519187 ]
 [ 0.12412687 -0.11751854 -0.19947895 ... -0.01370833 -0.06488054
  -0.07321002]
 ...
 [ 0.06180163 -0.00699065  0.17035972 ... -0.0876305  -0.17738222
  -0.02432549]
 [-0.07680804 -0.1695597  -0.16288057 ... -0.04558689 -0.0336183
  -0.02645377]
 [ 0.20260593 -0.06750299  0.04489307 ... -0.01303555 -0.07475546
  -0.08618149]]
layers.1.weight: [[ 0.16307563  0.04351623  0.2313271  ... -0.19975653 -0.32127762
   0.06235313]
 [ 0.36254078 -0.32779106  0.01882016 ...  0.01588959  0.32695472
  -0.11530888]
 [ 0.16407233 -0.01366389 -0.09543105 ... -0.05386456  0.33017486
  -0.2207334 ]
 ...
 [ 0.09594685  0.20015612 -0.31188315 ... -0.15437962  0.12652768
   0.10397821]
 [ 0.01920925  0.32358813  0.08117502 ...  0.0324984  -0.04426963
  -0.17360367]
 [ 0.23903506 -0.61622316  0.20299925 ... -0.27646676  0.01008549
   0.03272829]]
layers.2.weight: [[-1.6762855  -0.7151339  -0.47508124  0.3956764   2.4385161   0.43819347
   0.48839512 -1.1449757   0.31864113  2.9325595  -2.3728154  -1.3775061
   0.2876752  -2.150779    0.16072072 -0.61105984 -0.03526725 -0.36441487
   1.6393688  -0.37337878  2.1159403  -0.20581786  0.24868591  4.3860784
   0.35179737  0.11808409 -0.45584363 -2.2736123   0.21268813 -0.80115515
  -0.19571596 -1.1146392 ]]

Final Loss: 0.2915
Distance Metric: 39.2931
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 153

================================================================================

baselineCNN_tanh -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[ 0.09391775  0.17925532 -0.00989745 ... -0.22268856  0.17751293
  -0.09711207]
 [ 0.03143059  0.12402213 -0.03696657 ...  0.00283111 -0.13358131
  -0.16558778]
 [ 0.12280339 -0.195601    0.06880657 ...  0.07703458  0.08157197
   0.07647634]
 ...
 [ 0.11628816 -0.18649703 -0.15753819 ... -0.03870036  0.0992004
  -0.12485207]
 [-0.02142959 -0.0723922  -0.07444552 ... -0.0135222   0.07754251
   0.0971963 ]
 [-0.13876237  0.04876721 -0.10545442 ... -0.11692845 -0.13212387
  -0.15760599]]
layers.1.weight: [[-0.14229324 -0.00633775  0.07526583 ... -0.2116138   0.02750839
   0.13696553]
 [ 0.05286036  0.21526694  0.07986432 ... -0.1896019  -0.12252048
  -0.01994877]
 [ 0.13565858  0.2419044   0.16021195 ... -0.06781343  0.15262957
   0.11389676]
 ...
 [ 0.00276811 -0.17626126  0.04514998 ... -0.00076817 -0.22004448
   0.15871784]
 [-0.02731295 -0.10358141 -0.16957808 ...  0.05260958  0.11435544
   0.19831195]
 [-0.23496062  0.09262483  0.08563048 ...  0.04023386  0.0994299
   0.17615959]]
layers.2.weight: [[-4.7275415e-04  9.7476849e-03 -4.9936227e-03 -5.6302507e-04
  -1.0811234e-02  1.6280970e-02  8.1927399e-04 -5.7221862e-04
   8.4231189e-03  4.8287920e-04  3.1501258e-04 -1.6006610e-03
   4.5765680e-03 -4.3046515e-02 -2.0559115e-02 -7.3761269e-03
  -2.5284132e-01 -1.4706111e-03  1.4708852e-03  5.0721422e-04
   1.5601973e-03  1.5797388e-02  2.4930696e-01 -7.7557244e-04
   1.8095061e-03 -6.4948667e-03 -2.6158986e-04  4.2717063e-01
   1.5233270e-03 -3.0136423e-04 -4.1737258e-02  9.0808602e-04]]

Final Loss: 0.0000
Distance Metric: 21.4892
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

