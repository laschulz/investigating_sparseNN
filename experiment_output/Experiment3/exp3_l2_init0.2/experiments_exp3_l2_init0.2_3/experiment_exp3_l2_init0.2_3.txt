Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.06099727 -0.12747054  0.15070868]]]
layers.1.weight: [[[-0.19082111  0.19470395]]]
layers.2.weight: [[[-0.06340884 -0.0414081 ]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[-0.00327844 -0.00327867 -0.00327824 -0.00327781 -0.00327809 -0.00327713
  -0.00327726 -0.00327843 -0.00327786 -0.00327717 -0.00327804 -0.00327765
  -0.00327796 -0.00327775 -0.00327728 -0.00327714 -0.00327805 -0.00327724
  -0.00327853 -0.00327721 -0.00327748 -0.00327776 -0.00327814 -0.00327772
  -0.00327819 -0.00327831 -0.00327786 -0.00327803 -0.00327814 -0.00327724
  -0.00327815 -0.00327752]]

Final Loss: 0.0000
Distance Metric: 0.5684
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1234

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[-0.01020774  0.02628615  0.07257956 ...  0.01449299 -0.03899844
   0.00333937]
 [-0.06961529  0.08218882 -0.02994667 ... -0.04978021  0.02693474
  -0.03313733]
 [ 0.03682991 -0.02913744 -0.01470452 ...  0.02863908  0.07279217
   0.06980892]
 ...
 [ 0.00833065 -0.00274507  0.00942222 ...  0.02310249 -0.03014521
   0.04083183]
 [ 0.00494836  0.08946623 -0.01769692 ... -0.12736294  0.01276594
   0.04018735]
 [ 0.00235378  0.078452   -0.01314529 ... -0.01840171  0.00519821
   0.04169753]]
layers.1.weight: [[-0.0198489  -0.0472416   0.10729339 ...  0.05987734  0.01652574
   0.10930653]
 [-0.02640704 -0.00220283  0.06797358 ... -0.05161712 -0.00463996
  -0.00391037]
 [-0.00833225  0.02245295  0.02890826 ...  0.00867935  0.00922886
   0.01806509]
 ...
 [-0.11883084  0.01267076  0.03155972 ...  0.02652303 -0.05540166
   0.09965085]
 [-0.00712207  0.01919321  0.02471209 ...  0.00741913  0.00788899
   0.01544238]
 [-0.00816551  0.02795954  0.02241347 ... -0.01640479 -0.08439516
  -0.01982638]]
layers.2.weight: [[-0.37694672 -0.43939742  0.13965678 -0.46151093  0.25309795  0.26863825
   0.18630801 -0.507823    0.2501771  -0.41379476  0.23063041  0.25734308
  -0.53619415 -0.48019534  0.20254397  0.19117832 -0.48905024  0.2894
  -0.40220958  0.22712012  0.3368117  -0.41018045  0.31234765 -0.5143463
   0.20553443 -0.4894024  -0.49374497  0.18973267 -0.43125716 -0.5359319
   0.11938186 -0.3364025 ]]

Final Loss: 0.0094
Distance Metric: 9.1960
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1373

================================================================================

