Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.19008093  0.02028959 -0.05225823]]]
layers.1.weight: [[[-0.19594738  0.11096226]]]
layers.2.weight: [[[-0.00805952  0.00197606]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[-0.00018597 -0.00018578 -0.00018582 -0.00018569 -0.00018571 -0.00018598
  -0.00018593 -0.00018609 -0.00018626 -0.00018623 -0.00018567 -0.00018627
  -0.0001862  -0.00018573 -0.00018576 -0.00018597 -0.00018615 -0.00018606
  -0.00018603 -0.000186   -0.00018631 -0.00018579 -0.00018572 -0.00018575
  -0.00018621 -0.00018599 -0.0001862  -0.00018613 -0.00018621 -0.00018609
  -0.00018582 -0.00018624]]

Final Loss: 0.0000
Distance Metric: 0.4325
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1327

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[-0.09823859 -0.04351534 -0.03055374 ... -0.06829267 -0.00866843
  -0.01870185]
 [ 0.00990559  0.0966211   0.10396028 ...  0.08442517 -0.0164793
  -0.10743453]
 [ 0.04818346 -0.0355962   0.05517925 ... -0.06850993 -0.01989563
  -0.04139065]
 ...
 [-0.02687899  0.05837251 -0.07548215 ... -0.03295825  0.14283246
   0.04335384]
 [ 0.08041222  0.00319378 -0.05180747 ... -0.06933702 -0.02117176
  -0.08516033]
 [ 0.04380102  0.05391414  0.00125892 ... -0.09911869  0.0717192
  -0.02928081]]
layers.1.weight: [[ 0.00711859  0.01185508  0.00569593 ...  0.01971766  0.02783156
  -0.00687943]
 [ 0.02028389  0.04721474 -0.01374375 ...  0.00214396 -0.03442755
  -0.00174743]
 [ 0.00531337 -0.0121797  -0.0424621  ...  0.04251105 -0.00318371
  -0.00633535]
 ...
 [ 0.02656845  0.06248507  0.02550951 ... -0.01065304  0.00672348
  -0.06963088]
 [ 0.09902842 -0.0581185   0.00629767 ...  0.01040563  0.0358767
   0.10452475]
 [-0.02085665  0.02441738  0.01791332 ...  0.01732423  0.03383765
  -0.00268911]]
layers.2.weight: [[ 0.140652   -0.44502053  0.18756644  0.2559184  -0.54061574  0.1872677
  -0.2815087  -0.44828317  0.16622503 -0.4646383   0.23523903 -0.47776723
  -0.49247545 -0.52786595 -0.42176837 -0.41929036 -0.29785296 -0.3942075
   0.2208905   0.14745681  0.24905396  0.38867545 -0.3926455  -0.31825995
  -0.3824075  -0.53114146 -0.36149618 -0.50764984  0.21477787  0.21438222
  -0.4887675   0.13822256]]

Final Loss: 0.0089
Distance Metric: 9.0352
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1590

================================================================================

