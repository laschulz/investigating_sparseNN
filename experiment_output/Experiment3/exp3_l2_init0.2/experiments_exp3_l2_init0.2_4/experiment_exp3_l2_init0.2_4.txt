Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.11520638 -0.10269232  0.07115693]]]
layers.1.weight: [[[ 0.10674288 -0.06032185]]]
layers.2.weight: [[[0.08478504 0.11119439]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[0.00619369 0.00619239 0.00619393 0.00619297 0.00619352 0.00619383
  0.00619402 0.0061931  0.00619469 0.0061938  0.00619436 0.00619173
  0.00619253 0.00619156 0.00619473 0.00619514 0.00619328 0.00619285
  0.00619355 0.00619177 0.00619363 0.00619191 0.00619198 0.00619246
  0.00619174 0.00619222 0.00619183 0.00619135 0.00619192 0.00619434
  0.00619184 0.00619377]]

Final Loss: 0.0000
Distance Metric: 0.4576
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1157

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[ 0.06852572  0.00915607  0.04194861 ... -0.03979696  0.00782273
  -0.01019155]
 [ 0.04127625 -0.07609903  0.05454163 ... -0.01079866  0.01057546
   0.03308459]
 [-0.00425799  0.03485315  0.08968753 ...  0.02435679 -0.03642875
   0.0576171 ]
 ...
 [ 0.00091224 -0.04651694 -0.0384034  ...  0.03048404 -0.00118416
  -0.03418829]
 [ 0.07678425 -0.01141945  0.03807022 ...  0.00387763 -0.01022533
  -0.02181868]
 [-0.02245579 -0.02786252 -0.05904451 ... -0.02632117  0.05175613
  -0.05785301]]
layers.1.weight: [[ 0.05752053 -0.05401116  0.05105536 ... -0.08398159 -0.04211929
   0.06637571]
 [ 0.05572227  0.0194831  -0.0460708  ...  0.03787525 -0.04479744
  -0.00369351]
 [ 0.01134042  0.02594114 -0.04333265 ...  0.03859314  0.02504432
   0.01522521]
 ...
 [-0.0298965   0.03518106 -0.03709655 ... -0.00674567 -0.05941069
  -0.12994118]
 [-0.03591207 -0.02637345  0.03842207 ... -0.01699403  0.01027173
   0.02130061]
 [ 0.06168421  0.00912486 -0.01171563 ... -0.00100936  0.02124568
   0.031734  ]]
layers.2.weight: [[-0.46963233  0.30009234  0.17844397  0.22646655 -0.34401312 -0.48729756
   0.21021679 -0.4205281  -0.4631099   0.17248608  0.23197122  0.4179637
  -0.54215115  0.18346584 -0.50102687  0.19187295 -0.47528255  0.2413173
  -0.29636016  0.12934957  0.23201224 -0.4282013   0.13645254  0.21689896
  -0.3466324  -0.3410167  -0.4292817  -0.33934125  0.09487723 -0.44006982
  -0.4593291   0.16700894]]

Final Loss: 0.0087
Distance Metric: 8.5227
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1839

================================================================================

