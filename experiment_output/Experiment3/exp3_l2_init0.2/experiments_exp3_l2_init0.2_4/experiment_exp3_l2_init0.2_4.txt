Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.11520638 -0.10269232  0.07115693]]]
layers.1.weight: [[[ 0.10674288 -0.06032185]]]
layers.2.weight: [[[0.08478504 0.11119439]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[0.00619369 0.00619239 0.00619393 0.00619297 0.00619352 0.00619383
  0.00619402 0.0061931  0.00619469 0.0061938  0.00619436 0.00619173
  0.00619253 0.00619156 0.00619473 0.00619514 0.00619328 0.00619285
  0.00619355 0.00619177 0.00619363 0.00619191 0.00619198 0.00619246
  0.00619174 0.00619222 0.00619183 0.00619135 0.00619192 0.00619434
  0.00619184 0.00619377]]

Final Loss: 0.0000
Distance Metric: 0.4576
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1157

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[ 0.06852572  0.00915607  0.04194861 ... -0.03979696  0.00782273
  -0.01019155]
 [ 0.04127625 -0.07609903  0.05454163 ... -0.01079866  0.01057546
   0.03308459]
 [-0.00425799  0.03485315  0.08968753 ...  0.02435679 -0.03642875
   0.0576171 ]
 ...
 [ 0.00091224 -0.04651694 -0.0384034  ...  0.03048404 -0.00118416
  -0.03418829]
 [ 0.07678425 -0.01141945  0.03807022 ...  0.00387763 -0.01022533
  -0.02181868]
 [-0.02245579 -0.02786252 -0.05904451 ... -0.02632117  0.05175613
  -0.05785301]]
layers.1.weight: [[ 0.05752053 -0.05401116  0.05105536 ... -0.08398159 -0.04211929
   0.06637571]
 [ 0.05572227  0.0194831  -0.0460708  ...  0.03787525 -0.04479744
  -0.00369351]
 [ 0.01134042  0.02594114 -0.04333265 ...  0.03859314  0.02504432
   0.01522521]
 ...
 [-0.0298965   0.03518106 -0.03709655 ... -0.00674567 -0.05941069
  -0.12994118]
 [-0.03591207 -0.02637345  0.03842207 ... -0.01699403  0.01027173
   0.02130061]
 [ 0.06168421  0.00912486 -0.01171563 ... -0.00100936  0.02124568
   0.031734  ]]
layers.2.weight: [[-0.46963233  0.30009234  0.17844397  0.22646655 -0.34401312 -0.48729756
   0.21021679 -0.4205281  -0.4631099   0.17248608  0.23197122  0.4179637
  -0.54215115  0.18346584 -0.50102687  0.19187295 -0.47528255  0.2413173
  -0.29636016  0.12934957  0.23201224 -0.4282013   0.13645254  0.21689896
  -0.3466324  -0.3410167  -0.4292817  -0.33934125  0.09487723 -0.44006982
  -0.4593291   0.16700894]]

Final Loss: 0.0087
Distance Metric: 8.5227
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1839

================================================================================

baselineCNN_sigmoid -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[ 0.00969462  0.0027933  -0.00199155 ...  0.00029002 -0.00176658
  -0.00135744]
 [ 0.00145171  0.00043642 -0.00029916 ...  0.         -0.00040638
  -0.00015579]
 [ 0.01074973  0.00312329 -0.0021384  ...  0.00058644 -0.00214248
  -0.00171528]
 ...
 [ 0.00805574  0.00245623 -0.00155437 ...  0.00044112 -0.00165413
  -0.00119272]
 [-0.00852014 -0.00251782  0.00156557 ... -0.00024374  0.00172819
   0.00133671]
 [ 0.00231636  0.00059541 -0.00040576 ...  0.00017621 -0.00043831
  -0.00028909]]
layers.1.weight: [[-0.00125252  0.00036949 -0.00132529 ...  0.00015833  0.00136938
   0.00064143]
 [-0.00057599  0.00069473  0.00011127 ... -0.00088663  0.0008051
  -0.00077188]
 [ 0.00138862  0.00082734  0.00141216 ...  0.00125497 -0.00216948
   0.00073009]
 ...
 [ 0.0003589  -0.00079593  0.00047596 ...  0.00103331 -0.00026274
   0.00043884]
 [ 0.          0.00042823  0.00056497 ...  0.0007595  -0.00101419
   0.        ]
 [ 0.00070692  0.00051625  0.0018269  ...  0.         -0.00022596
  -0.00027937]]
layers.2.weight: [[-0.01175856 -0.01249163  0.02906435  0.02562338  0.01119251 -0.00705773
   0.02487667 -0.01494312  0.01578817  0.01533623 -0.01181452 -0.03176663
   0.01078341 -0.02151763 -0.05139731  0.00528825  0.00148835  0.00353846
  -0.01455956  0.02661704 -0.011674    0.02447886 -0.019696    0.00343097
   0.0025408  -0.02106898  0.01136304 -0.04103524 -0.0128372   0.01330743
   0.00978664  0.02467507]]

Final Loss: 0.2313
Distance Metric: 1.0098
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 410

================================================================================

baselineCNN_relu -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0.00844424 0.00782619 0.00841971 ... 0.00844424 0.00782567 0.00844418]
 [0.00782315 0.00843905 0.0078216  ... 0.00843911 0.00789693 0.00789695]
 [0.00839462 0.00834197 0.00822756 ... 0.0084418  0.00784163 0.00835623]
 ...
 [0.0079192  0.00843941 0.00782373 ... 0.00843947 0.00782432 0.00791952]
 [0.00846571 0.00846555 0.00788334 ... 0.00785096 0.00797419 0.00785097]
 [0.00831655 0.00820132 0.00880615 ... 0.00820126 0.00820133 0.00821138]]
layers.2.weight: [[-0.24650158 -0.24675308 -0.2461728  -0.26524934 -0.26505193 -0.26303717
  -0.24818802 -0.24669525 -0.26533163 -0.2491832  -0.2654098  -0.2655212
  -0.24639621 -0.24663135 -0.2652274  -0.26530644 -0.24669847 -0.24623278
  -0.24680004 -0.2654823  -0.2490504  -0.26507467 -0.24673764 -0.2651468
  -0.26544487 -0.26530954 -0.25608787 -0.24768803 -0.2653082  -0.24671331
  -0.24831668 -0.2651279 ]]

Final Loss: 0.0000
Distance Metric: 2.6714
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 930

================================================================================

baselineCNN_relu -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.01573546 0.0213696  0.01177747]
 ...
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.02705422 0.03673065 0.02021022 ... 0.00010076 0.00012562 0.        ]
 [0.01114442 0.01512041 0.00831966 ... 0.         0.         0.        ]]
layers.1.weight: [[0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.00071956 0.00694178 ... 0.0006523  0.01192919 0.00491183]
 [0.         0.         0.         ... 0.         0.         0.        ]
 ...
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]]
layers.2.weight: [[0.         0.0666026  0.         0.01474473 0.         0.
  0.02192406 0.         0.         0.         0.         0.02877213
  0.         0.         0.         0.00204208 0.         0.18692966
  0.01888454 0.         0.10827201 0.         0.         0.
  0.         0.         0.13152452 0.         0.08554444 0.
  0.         0.        ]]

Final Loss: 0.0000
Distance Metric: 1.3005
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 3219

================================================================================

baselineCNN_relu -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0.]]

Final Loss: 0.0000
Distance Metric: 1.2825
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 5148

================================================================================

baselineCNN_tanh -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.00047184 -0.00047275  0.00117743 ...  0.00033893  0.00037469
  -0.00082995]
 [-0.00045675 -0.00048173  0.00119382 ...  0.00035325  0.00037844
  -0.00083698]
 [-0.00046071 -0.00047159  0.00117793 ...  0.00034505  0.00036353
  -0.0008362 ]
 ...
 [-0.00046483 -0.00047272  0.00118392 ...  0.00035257  0.00036567
  -0.00082364]
 [-0.0004619  -0.00048328  0.00119594 ...  0.00034982  0.00037079
  -0.00083267]
 [-0.00047812 -0.00047852  0.00119308 ...  0.00033498  0.00037541
  -0.0008195 ]]
layers.1.weight: [[0.00835222 0.00835358 0.00836853 ... 0.0083687  0.0083686  0.00836428]
 [0.00833623 0.00835302 0.00834961 ... 0.00833774 0.00835005 0.00834394]
 [0.00834647 0.00834847 0.00834215 ... 0.00834052 0.00835093 0.00834186]
 ...
 [0.00835384 0.00835965 0.00835064 ... 0.00836091 0.00835125 0.00835653]
 [0.00834452 0.00835007 0.00836399 ... 0.00835069 0.00836059 0.00835991]
 [0.0083365  0.00835126 0.00833213 ... 0.00833419 0.00833478 0.00833965]]
layers.2.weight: [[-0.2557612  -0.2553171  -0.25526968 -0.25490263 -0.2559336  -0.25551373
  -0.2555977  -0.25499535 -0.25499797 -0.2556803  -0.25510114 -0.25496906
  -0.2559201  -0.25594926 -0.25565752 -0.25599304 -0.25567514 -0.25579816
  -0.25538373 -0.25596714 -0.25589332 -0.25585425 -0.25483584 -0.2548365
  -0.25597653 -0.25486353 -0.2559125  -0.2553436  -0.25540832 -0.25555658
  -0.25556037 -0.25518382]]

Final Loss: 0.0001
Distance Metric: 2.9941
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 838

================================================================================

baselineCNN_tanh -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[-0.00278604 -0.023985    0.00169901 ... -0.00168748 -0.01449762
   0.00600256]
 [ 0.13664879 -0.11324673 -0.00106922 ... -0.05570726  0.14260429
  -0.03051712]
 [ 0.08038069 -0.04030029  0.04088116 ...  0.05215391 -0.10578519
   0.06282746]
 ...
 [ 0.1275233  -0.01141775  0.02462024 ...  0.00658055  0.03184309
  -0.04236228]
 [-0.00363623 -0.13249342 -0.01307972 ... -0.0503372  -0.00239523
   0.09240015]
 [-0.06655155  0.06802256 -0.02562451 ... -0.0075633  -0.04193404
  -0.03898142]]
layers.1.weight: [[ 0.02821928 -0.02364922 -0.01662604 ... -0.0239683   0.0545546
  -0.01932205]
 [-0.19571298  0.04550412  0.0065245  ... -0.03960007 -0.03835207
   0.04417953]
 [ 0.02827237  0.03933246 -0.00134811 ...  0.01061943 -0.03986711
  -0.00099544]
 ...
 [ 0.03327564 -0.01599153  0.0052857  ...  0.00524678  0.03196522
   0.02431431]
 [-0.05540841 -0.11227249  0.08922228 ... -0.01315103  0.05692245
   0.01264072]
 [ 0.00908371  0.01192378 -0.00369467 ...  0.0171016   0.07226851
  -0.06343956]]
layers.2.weight: [[-0.2804762   0.29016894 -0.6625086   0.14994322  0.31174025 -0.37054875
   0.4808531  -0.62159395 -0.73525345  0.20514886  0.41583893 -0.267028
  -0.28023294  0.35463712  0.4166712  -1.2005659   0.19752385 -0.6779385
   0.27379078  0.4740395   0.22477208  0.42628562  0.40191862 -0.61796874
  -0.23002869 -0.3284838   0.17736833 -1.1158099  -0.43649632 -0.24349222
  -0.44682747 -0.5726968 ]]

Final Loss: 0.1913
Distance Metric: 17.9406
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 979

================================================================================

baselineCNN_tanh -> fcn_256_32_tanh

Student Model Parameters:
layers.0.weight: [[ 0.0033438  -0.00309314  0.00144404 ...  0.0100464  -0.01123616
   0.00307571]
 [-0.00636673  0.00728923 -0.00220981 ... -0.00192042  0.00222188
  -0.00013546]
 [ 0.00438512 -0.00474212  0.00161909 ...  0.00503732 -0.00562937
   0.00154464]
 ...
 [-0.00730726  0.00822236 -0.00220062 ...  0.00103465 -0.0007749
   0.00046579]
 [ 0.00025888 -0.00020379  0.         ... -0.00125324  0.00122431
  -0.00044466]
 [-0.01524707  0.01663738 -0.0045804  ... -0.00186824  0.00300915
  -0.00065483]]
layers.1.weight: [[-0.00052104  0.00077297 -0.0005529  ...  0.00033499  0.00019016
   0.00048002]
 [-0.00025718  0.00036503 -0.00026573 ...  0.00014964  0.
   0.00021168]
 [-0.00019279  0.00027194 -0.00019867 ...  0.00011089  0.
   0.00015619]
 ...
 [-0.00523374  0.00964519 -0.00660123 ...  0.00435982  0.00251679
   0.00580097]
 [-0.00363714  0.0096259  -0.0059307  ... -0.00106099  0.00497509
  -0.00922971]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[-0.11301925 -0.05465242 -0.04086206  0.01789407  0.69538563 -0.01543546
   0.35965663  0.77819943 -0.01653752  0.06595425  0.02969936 -0.0105552
  -0.06603391  0.871931   -0.04830334  0.04008177  0.00813472 -0.07478166
  -0.07567182 -0.0585818   0.01897409  0.07724939  0.03459819  0.01595272
   0.0425804   0.06261428 -0.7717265   0.00923506  0.04160426 -0.8747695
   0.70159614 -0.01474354]]

Final Loss: 0.0006
Distance Metric: 11.6437
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1286

================================================================================

