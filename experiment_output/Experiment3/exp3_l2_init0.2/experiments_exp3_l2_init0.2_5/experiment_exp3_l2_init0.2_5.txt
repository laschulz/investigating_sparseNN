Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.13686724  0.01444254 -0.16963851]]]
layers.1.weight: [[[ 0.06392922 -0.08888977]]]
layers.2.weight: [[[-0.2104857   0.00641397]]]

================================================================================

baselineCNN_sigmoid -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[-0.00633194 -0.00633384 -0.00633392 -0.00633467 -0.00633569 -0.00633401
  -0.00633152 -0.00633449 -0.00633459 -0.00633436 -0.00633557 -0.00633161
  -0.00633356 -0.00633193 -0.00633349 -0.00633602 -0.0063313  -0.00633612
  -0.00633352 -0.00633505 -0.00633212 -0.00633508 -0.00633288 -0.0063358
  -0.00633458 -0.00633127 -0.00633471 -0.00633565 -0.00633292 -0.00633545
  -0.00633608 -0.00633273]]

Final Loss: 0.0000
Distance Metric: 0.5672
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1112

================================================================================

baselineCNN_sigmoid -> fcn_256_32_relu

Student Model Parameters:
layers.0.weight: [[-0.06148637 -0.01587317 -0.08021557 ...  0.02445159  0.00375585
   0.01145833]
 [ 0.08013421 -0.05703451  0.07216338 ... -0.06929658  0.04674952
  -0.00560239]
 [ 0.03950664  0.05814687  0.02145772 ... -0.00541925 -0.00391466
   0.00244758]
 ...
 [-0.00900117 -0.05473616  0.04943376 ...  0.05457684  0.05438627
  -0.04147791]
 [-0.07351907  0.07414588 -0.00409516 ... -0.03660582 -0.03528111
  -0.01813885]
 [ 0.04848823  0.01348038 -0.0840343  ...  0.05414614 -0.01296347
  -0.06172571]]
layers.1.weight: [[-0.05843181  0.06749834 -0.02930977 ... -0.04592205  0.01279495
  -0.00872953]
 [ 0.01317088 -0.00694606 -0.0177222  ... -0.00320966  0.00744263
  -0.09721715]
 [-0.02004823  0.05640897  0.03697829 ...  0.04598196 -0.01514577
  -0.05552699]
 ...
 [ 0.01686994 -0.03449751 -0.02629709 ... -0.01712295  0.
   0.06278466]
 [ 0.01422745  0.00407048 -0.00537393 ... -0.00117502  0.01613755
   0.00987943]
 [ 0.0127502   0.00134338 -0.00760946 ...  0.02103854  0.01615666
   0.00112495]]
layers.2.weight: [[-0.49628386 -0.50181365  0.36182293  0.14667033  0.10355103 -0.50478536
   0.38581607  0.2209683   0.32745445  0.32645753 -0.3164413  -0.47501835
   0.25569022  0.2519519   0.21427263 -0.36609966  0.38687593  0.2074577
  -0.46393383 -0.5595119  -0.47271645  0.2490835   0.1625512  -0.49417478
  -0.5275146   0.26178765 -0.49985766  0.20007706  0.29048312  0.23967873
   0.07775091  0.2102858 ]]

Final Loss: 0.0082
Distance Metric: 8.5961
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1164

================================================================================

baselineCNN_relu -> fcn_256_32_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.0001442  -0.00015641 -0.0001199  ...  0.          0.
   0.        ]
 [ 0.0001444  -0.00015655 -0.00011988 ...  0.          0.
   0.        ]
 [ 0.00014438 -0.00015567 -0.0001194  ...  0.          0.
   0.        ]
 ...
 [ 0.00014397 -0.00015554 -0.00011891 ...  0.          0.
   0.        ]
 [ 0.00014444 -0.00015641 -0.00011917 ...  0.          0.
   0.        ]
 [ 0.00014411 -0.0001559  -0.00011884 ...  0.          0.
   0.        ]]
layers.1.weight: [[0.00826767 0.00826499 0.00826842 ... 0.00826634 0.00826837 0.00826773]
 [0.0082661  0.0082637  0.00826432 ... 0.00826362 0.00825831 0.00826094]
 [0.00825292 0.00824688 0.00824891 ... 0.00824788 0.00825287 0.00824774]
 ...
 [0.00826361 0.00827099 0.00826892 ... 0.00826977 0.00826781 0.00826825]
 [0.00828108 0.00827683 0.00828178 ... 0.00828041 0.00827787 0.00828121]
 [0.00825265 0.00825259 0.00825945 ... 0.00825956 0.00825394 0.00825183]]
layers.2.weight: [[-0.253208   -0.25314507 -0.25280303 -0.25316513 -0.25344023 -0.25314933
  -0.2529344  -0.2528498  -0.253505   -0.25288427 -0.25288132 -0.2527914
  -0.25281507 -0.25267947 -0.2534205  -0.25345746 -0.25268412 -0.25350025
  -0.25280035 -0.2534065  -0.25336313 -0.25327852 -0.25357547 -0.25279322
  -0.25286794 -0.25283217 -0.2533909  -0.2528504  -0.25280014 -0.25327316
  -0.2535872  -0.25297055]]

Final Loss: 0.0000
Distance Metric: 3.1832
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1147

================================================================================

