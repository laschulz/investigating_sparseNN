Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.33105296  1.1262296  -0.01593335]]]
layers.1.weight: [[[-0.6960452  -0.39051613]]]
layers.2.weight: [[[0.15918233 0.02116659]]]

================================================================================

nonoverlappingCNN_relu -> fcnn_decreasing_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0.]]

Final Loss: 0.0000
Distance Metric: 2.1327
L1 norm: 1e-05
L2 norm: 0
Batch size: 32
Clipping: 0
Learning rate: 0.005
data size: 5120

================================================================================

nonoverlappingCNN_relu -> fcnn_decreasing_tanh

Student Model Parameters:
layers.0.weight: [[ 6.6607237e-01  3.3694363e-01 -1.6308823e-01 ...  6.0625352e-02
   7.2512358e-01  1.8637635e-01]
 [ 2.9674415e-02 -4.2264724e-01  1.9312768e-01 ...  9.4044290e-02
   2.7762185e-04 -7.6144241e-02]
 [ 7.4266464e-01 -2.4591011e-01 -8.6687344e-01 ...  2.8488743e-01
  -1.7301874e-01 -3.9600515e-01]
 ...
 [ 1.7971267e-01 -3.9180912e-02 -1.7266833e-03 ... -1.1297131e-01
   5.1220506e-01  1.1423694e-01]
 [ 6.8736160e-01  1.6889285e-01  2.6837742e-01 ...  7.4802595e-01
  -3.6204126e-01 -5.3263944e-02]
 [-1.7613240e-01 -5.0917294e-02 -9.5140494e-02 ... -9.3306624e-04
  -3.2281855e-01  1.1532524e-03]]
layers.1.weight: [[-4.96330082e-01 -1.18674025e-01  1.78789899e-01 ...  5.91040240e-04
  -1.82311743e-01 -1.94846511e-01]
 [-1.09380037e-01 -9.34678828e-04 -9.28959693e-04 ... -2.54848652e-04
   4.60297946e-04  6.36217534e-04]
 [-1.62926078e-01  8.42118487e-02  1.86684149e-04 ...  1.23700229e-04
   6.39754056e-04  5.91816567e-02]
 ...
 [-2.67921537e-01  4.21964884e-04  4.54147868e-02 ...  1.27718854e-03
  -2.05173506e-03  5.30210286e-02]
 [-1.01156808e-01 -4.43483964e-02  7.40498155e-02 ... -4.60591400e-04
  -9.45778415e-02  6.61793165e-04]
 [-1.70364529e-01 -2.53491133e-01 -7.65084401e-02 ...  3.47573659e-04
  -3.87467619e-04  4.28428029e-04]]
layers.2.weight: [[ 0.71068144  0.62602425  0.5663186   1.4241211   0.51623607  0.6278326
   0.6851329   0.48803952  0.56498015  0.5914489  -0.75214046  0.51384497
  -0.16039327  0.4566143   0.7068298  -0.38111633 -0.4557062  -1.6848291
  -0.6756017   0.49780843 -0.895326   -0.69720864 -0.4336397   0.7388302
   0.54182553  0.8320651   0.5558445  -0.5562507   0.59795576 -0.42487928
   0.6181354  -0.5256019 ]]

Final Loss: 0.0279
Distance Metric: 36.0037
L1 norm: 1e-05
L2 norm: 0
Batch size: 32
Clipping: 0
Learning rate: 0.005
data size: 5120

================================================================================

nonoverlappingCNN_relu -> fcnn_decreasing_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[-0.11761239 -0.053547   -0.48061803 -0.5965023  -0.04255517 -0.2335395
  -0.68437195 -0.21183945 -0.7258994  -0.55772305 -0.07080945 -0.5
  -0.5606285  -0.2971043  -0.06679863 -0.08960316 -0.61433154 -0.04024129
  -0.49978912 -0.45886979 -1.251179   -0.47424176 -0.06380306 -0.49175522
  -0.33710846 -0.6763109  -0.3848722  -0.06646937 -0.72729665 -0.05496225
  -0.03125    -0.03919338]]

Final Loss: 0.0001
Distance Metric: 4.1261
L1 norm: 1e-05
L2 norm: 0
Batch size: 32
Clipping: 0
Learning rate: 0.005
data size: 5120

================================================================================

nonoverlappingCNN_tanh -> fcnn_decreasing_relu

Student Model Parameters:
layers.0.weight: [[-0.01142631 -0.03319104  0.00144438 ...  0.00175383  0.00351765
   0.10821433]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 ...
 [-0.01060865 -0.09687927  0.0399797  ... -0.00626919  0.
   0.00076433]
 [-0.00439584 -0.00498003  0.00030275 ... -0.00479069  0.00044918
  -0.00025683]
 [-0.01311415  0.02467205  0.11601447 ... -0.1352152   0.00338759
  -0.00150512]]
layers.1.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [-0.00836273  0.          0.         ... -0.03053351  0.00010865
   0.05193925]
 ...
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.00027282  0.          0.         ...  0.          0.
   0.00032528]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[ 0.          0.         -2.0661132   1.9479661   0.         -2.3364482
  -2.1618636   2.8739932   0.          1.7253675  -1.7922077   1.7410395
   0.         -1.8237652   0.          1.6359106   2.6294107   2.6180925
  -1.6243328   0.         -1.9623507   0.          0.         -0.18043332
   0.         -1.9373826   0.         -2.0700502   0.          0.
  -2.1204367   0.        ]]

Final Loss: 0.2950
Distance Metric: 16.9229
L1 norm: 1e-05
L2 norm: 0
Batch size: 32
Clipping: 0
Learning rate: 0.005
data size: 5120

================================================================================

nonoverlappingCNN_tanh -> fcnn_decreasing_tanh

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.14776939 0.         0.         0.
  0.         1.336037  ]]

Final Loss: 0.0001
Distance Metric: 3.5406
L1 norm: 1e-05
L2 norm: 0
Batch size: 32
Clipping: 0
Learning rate: 0.005
data size: 5120

================================================================================

