Experiment Summary:
================================================================================

Teacher Model Parameters:
layer1_weights.0: [[ 2.59 -2.83  0.87]]
layer1_weights.1: [[-1.22  0.45  0.88]]
layer2_weights.0: [[-1.38  1.29]]
layer2_weights.1: [[ 0.35 -0.73]]
layer3_weights.0: [[ 0.86 -0.84]]

================================================================================

multiWeightCNN_relu -> fcnn_decreasing_relu

Student Model Parameters:
layers.0.weight: [[-0.00555367  0.00707024 -0.00638025 ... -0.22723398  0.08770779
   0.16393389]
 [-0.14909975  0.16368629 -0.05057033 ...  0.00244243  0.
   0.0020241 ]
 [-0.10909023  0.1265915  -0.04317054 ... -0.0025821   0.
   0.00089282]
 ...
 [-0.0020553  -0.00769124  0.00847286 ... -0.08933505  0.04790194
   0.07072604]
 [ 0.00707045 -0.00106629  0.00580311 ... -0.0179466   0.01381335
   0.02290855]
 [-0.06794676  0.10993133 -0.03214388 ... -0.05931113  0.02281615
   0.04877317]]
layers.1.weight: [[ 0.00631096  0.00823283  0.04048093 ... -0.02052063 -0.05316979
  -0.00020858]
 [ 0.08947252 -0.07822125 -0.03301225 ...  0.00823432  0.03659259
   0.07784487]
 [ 0.06858851  0.04919484  0.03364376 ...  0.05484423 -0.06435024
  -0.04391861]
 ...
 [ 0.05584353  0.02385404 -0.03328666 ...  0.0280568  -0.03584629
  -0.0663534 ]
 [-0.02434157  0.12297274 -0.00061525 ... -0.05415642 -0.01919352
  -0.02020114]
 [ 0.04323181  0.02440533 -0.05257179 ...  0.03042105 -0.03252596
   0.01161978]]
layers.2.weight: [[-0.04506932  0.30088055  0.15211543  0.19935861 -0.3602364   0.32081258
   0.6755271  -0.6344706   0.8110385  -0.17413956 -0.04992435 -0.3479598
   0.02242849  0.05240704  0.34062073 -0.07679149  0.84948283  0.5211173
  -0.01239451 -0.1739221  -0.31788749 -0.43343097 -0.4194056  -0.7318438
   0.76793605 -0.9395509  -0.14537735  0.49411577 -0.15751672 -0.12870738
  -0.05671975 -0.282831  ]]

Final Loss: 0.0047
Distance Metric: 15.0869
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 10240
multiWeightCNN_tanh -> fcnn_decreasing_tanh

Student Model Parameters:
layers.0.weight: [[ 0.0293237  -0.02208489  0.03031236 ...  0.06526395 -0.04042711
  -0.04462555]
 [-0.08022487  0.09331459 -0.02212919 ... -0.05598938 -0.01265624
  -0.00341257]
 [ 0.02040819  0.05081131  0.0206068  ... -0.33649275  0.13097812
   0.23542687]
 ...
 [-0.05714315  0.06978985 -0.05872815 ...  0.02634172  0.02094267
  -0.04870167]
 [ 0.03285629 -0.04757895  0.01313805 ... -0.08257423  0.04618277
  -0.02320964]
 [ 0.10360941 -0.07477698  0.06389787 ...  0.05062494 -0.01841914
  -0.03733003]]
layers.1.weight: [[ 0.03621536 -0.02277845 -0.01223572 ...  0.02704525  0.01913381
   0.05371641]
 [ 0.00763714  0.02770599 -0.00951404 ...  0.04709826 -0.01931728
   0.01173256]
 [-0.02850867 -0.05322239  0.01722502 ... -0.01238744  0.03320966
   0.07029913]
 ...
 [ 0.0404693  -0.04770668  0.05994657 ...  0.01967055  0.01880433
  -0.03465773]
 [-0.02276491 -0.05516563 -0.02480874 ... -0.08672135 -0.04025242
   0.11958902]
 [-0.03109278  0.0044617   0.02275062 ... -0.01027287 -0.1417049
   0.1250255 ]]
layers.2.weight: [[ 0.03668324 -0.11072961 -0.00809962 -0.0284925  -0.07256119  0.46724257
  -0.0316419  -0.01841917 -0.02899702  0.01098049  0.01168542  0.22540613
   0.00081874 -0.01324668  0.04416847  0.00904127 -0.02093654  0.01254921
  -0.4784417   0.04725571  0.00169902 -0.05540953  0.00890424 -0.05905677
  -0.02297319  0.4517471  -0.00170587  0.04111174  0.00405404  0.05214002
  -0.46973336 -0.42326623]]

Final Loss: 0.0047
Distance Metric: 13.6840
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 10240
multiWeightCNN_sigmoid -> fcnn_decreasing_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.00678995 -0.00924511  0.02150622 ... -0.00193378  0.01038942
  -0.00941231]
 [-0.01133887  0.05910335 -0.02168405 ... -0.01840929  0.00795495
   0.02157573]
 [-0.05898421  0.07919611 -0.0126024  ... -0.04301792  0.00170806
   0.04304848]
 ...
 [ 0.02184061 -0.05932375  0.04172026 ...  0.03348181 -0.00762128
  -0.03626036]
 [ 0.03562439 -0.00338293 -0.01958156 ...  0.03124321 -0.00714224
  -0.01131457]
 [ 0.04165832 -0.04489299  0.01444988 ...  0.01146232 -0.03143413
  -0.01417193]]
layers.1.weight: [[-0.02467084 -0.01564248 -0.01091784 ... -0.00803395 -0.00213531
   0.01695693]
 [ 0.01638309  0.03612059  0.0383299  ... -0.02869452 -0.0054685
  -0.037357  ]
 [ 0.01645041 -0.02316969 -0.01069989 ...  0.03282958  0.01180835
  -0.0112672 ]
 ...
 [-0.0150888  -0.02580425 -0.05105654 ...  0.01504765 -0.00859977
   0.03372879]
 [ 0.02509286 -0.02377852  0.03302416 ... -0.00246783  0.01742832
  -0.0021531 ]
 [-0.01579195  0.00021641 -0.02293998 ...  0.01101149 -0.01722514
   0.02727811]]
layers.2.weight: [[ 0.02251849  0.25927603 -0.22020443 -0.2161509   0.3125542   0.08680446
   0.42805    -0.03585674  0.2331224  -0.19574031 -0.02317773 -0.13453335
  -0.2730425   0.19340882  0.42243165 -0.08768596 -0.12892517  0.21790113
  -0.3230124  -0.2941579  -0.14507528 -0.14516449 -0.17575395 -0.10704783
   0.29295933  0.07877149  0.0347943   0.21396051  0.22751927 -0.45381096
   0.09228987 -0.10881697]]

Final Loss: 0.0023
Distance Metric: 11.0736
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 10240
