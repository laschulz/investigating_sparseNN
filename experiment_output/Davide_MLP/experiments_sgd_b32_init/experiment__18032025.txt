Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

nonoverlappingCNN_relu -> fcnn_decreasing_relu

Student Model Parameters:
layers.0.weight: [[ 0.6014456   0.5346444   0.04362437 ... -0.09853671 -0.45450565
   0.5017538 ]
 [-0.24469754 -0.06242399  0.24948467 ...  0.86049145 -0.33260006
   0.34897813]
 [ 0.6052939  -0.08425592  0.04424099 ... -0.01163985 -0.416696
  -0.05940325]
 ...
 [ 0.4958323   0.05358533 -0.69808656 ...  0.13181536 -0.29303634
   0.444953  ]
 [-0.05400887 -0.05007656  0.48838246 ... -0.04123053  0.2337611
   0.06427461]
 [ 0.19162619 -0.46948886  0.6461302  ...  0.8960418  -0.44998056
   0.07711323]]
layers.1.weight: [[ 0.08880997  0.039415    0.02533909 ... -0.01028136  0.00850767
  -0.07933999]
 [-0.10721626 -0.08643303 -0.0119646  ... -0.06128301 -0.19650884
   0.01418008]
 [ 0.08326767  0.0059743   0.18143117 ... -0.07287113 -0.04623736
   0.11990643]
 ...
 [ 0.0636119   0.07034184  0.20284203 ...  0.1041105   0.00456228
  -0.01982005]
 [-0.04254931  0.04969541  0.06480447 ... -0.04112053 -0.04499945
  -0.02458704]
 [-0.03659801  0.00200647 -0.1238269  ...  0.04383705 -0.00055856
   0.03742761]]
layers.2.weight: [[-0.0200532  -0.27530378 -0.2022254  -1.2930288  -0.21339275 -0.25821412
  -0.09755322  0.05333987 -0.4934584  -0.874907   -0.34762    -0.1321628
  -0.2494475  -0.38420868 -0.0611253   0.0572989  -0.5117984  -0.08483188
   0.35149556 -0.07343233 -0.21627526 -0.01682851 -0.1458049   0.3593707
  -0.49827528 -0.25368157 -0.27661026 -0.2413531  -0.5914772  -0.9207265
  -0.08694007 -0.0893298 ]]

Final Loss: 4.3162
Distance Metric: 45.4173
L1 norm: 0
L2 norm: 0
Batch size: 32
Clipping: 0

================================================================================

nonoverlappingCNN_tanh -> fcnn_decreasing_tanh

Student Model Parameters:
layers.0.weight: [[-0.65257096  0.0126782   0.56604224 ... -0.3342533   0.10235889
   0.14974505]
 [ 0.00795748 -0.5172242   0.67960674 ... -0.14159271  0.49277255
   0.12730998]
 [-0.6235807  -0.11896431 -0.34426734 ...  0.71887016 -1.186297
  -0.16186589]
 ...
 [-0.16747318 -0.19696508 -0.63125676 ...  0.49232408 -0.5330578
  -0.13748592]
 [ 0.1712384  -0.281459   -0.7906296  ...  0.66132045 -0.37688524
  -0.44782406]
 [ 0.24688734  0.14488564 -0.09729391 ... -0.5008013  -0.3451435
   0.41737485]]
layers.1.weight: [[ 0.05098597 -0.08344266  0.037437   ... -0.01892971 -0.00736205
   0.05057184]
 [-0.0340367   0.01224635 -0.0225105  ... -0.04264043 -0.04606035
  -0.05747569]
 [ 0.00535788  0.02290362 -0.0373215  ... -0.07819036 -0.00931955
  -0.02500917]
 ...
 [ 0.02484404 -0.05540617 -0.08070008 ... -0.02669883 -0.02694141
   0.04037159]
 [ 0.01045665 -0.0392764  -0.04616844 ... -0.00079137 -0.04726391
  -0.0368348 ]
 [-0.02127561  0.02788291  0.11659773 ... -0.07767121 -0.06006757
   0.05525656]]
layers.2.weight: [[-0.01507841  0.002978    0.0127789  -0.00448748  0.00719871  0.00375335
  -0.00286083  0.00631299 -0.01257528  0.00202199 -0.00628176 -0.00269362
  -0.00126137  0.0146216   0.0015589   0.00409533  0.00558591  0.00471365
   0.01276099 -0.01685349 -0.00390821  0.00484032 -0.00437408  0.00841127
  -0.0002609   0.01068091 -0.00615671  0.00165105 -0.00620482  0.01077588
   0.00869096  0.00732927]]

Final Loss: 0.3585
Distance Metric: 44.3277
L1 norm: 0
L2 norm: 0
Batch size: 32
Clipping: 0

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_decreasing_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.15267448  0.13825418 -0.33649105 ... -0.02241182 -0.04979401
   0.3407292 ]
 [-0.11969393  0.8558749   0.33526483 ...  0.09370413  0.24391773
   0.8650024 ]
 [-0.7576085  -0.55897987  0.00972919 ...  0.00273538 -0.3341691
   0.15327209]
 ...
 [ 0.57219464 -0.23818175 -0.32211617 ...  0.0066892   0.15975714
   0.03766822]
 [-0.6478478  -0.26887506  0.2559724  ...  0.6589428   0.6125749
   0.08855761]
 [ 0.41147318 -1.0544662   0.215062   ...  0.30920517 -0.17794369
   0.5747606 ]]
layers.1.weight: [[-0.05515454 -0.00625044  0.10178082 ... -0.06345165 -0.00402043
   0.0094501 ]
 [ 0.05889432  0.13670576 -0.03616146 ... -0.03022032  0.022875
  -0.02794508]
 [ 0.10135143 -0.00486451  0.07578228 ...  0.03162983  0.08185443
   0.04809254]
 ...
 [-0.02868338 -0.01969674 -0.10744358 ... -0.03954624  0.02635458
   0.06609556]
 [-0.03311577  0.0156811  -0.06100214 ... -0.13867989  0.01138943
  -0.0351983 ]
 [ 0.04801322  0.1313255  -0.01255879 ...  0.03093249 -0.08418338
  -0.05931404]]
layers.2.weight: [[-0.15430719  0.1137985  -0.2342154  -0.51268286 -0.07112835  0.08460788
   0.13969874 -0.08633088 -0.07642559  0.7470433   0.0899009   0.
  -0.01477935 -0.04346313 -0.36318812  0.15046479 -0.09077231  0.11322555
  -0.419565   -0.17599191 -0.060877   -0.08558871  0.06782381  0.17121142
  -0.21357717  0.04551912  0.23298092  0.03866342 -0.07501499 -0.37768522
   0.17698722 -0.07748727]]

Final Loss: 0.0024
Distance Metric: 44.4621
L1 norm: 0
L2 norm: 0
Batch size: 32
Clipping: 0

================================================================================

