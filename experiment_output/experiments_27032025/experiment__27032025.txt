Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.78 -0.12  0.7 ]]

 [[-1.16  0.47  0.05]]

 [[-0.73  1.96 -1.01]]

 [[-0.32  0.21  0.63]]]
layers.1.weight: [[[ 0.    0.04]
  [ 0.68  0.34]
  [ 0.54 -0.22]
  [-0.14 -0.33]]

 [[-0.14  1.59]
  [ 1.48 -0.52]
  [-1.26  0.3 ]
  [-0.4  -1.09]]

 [[-0.71  0.44]
  [-0.02 -0.14]
  [ 0.37 -0.7 ]
  [-0.83 -0.38]]

 [[ 0.89 -0.48]
  [-0.27 -0.81]
  [ 1.76 -0.41]
  [ 0.15  0.49]]]
layers.2.weight: [[[-0.54  0.16]
  [-0.74 -0.46]
  [ 0.08  0.18]
  [-0.22  0.81]]]

================================================================================

overlappingCNN_relu -> fcnn_decreasing_relu

Student Model Parameters:
layers.0.weight: [[ 1.43301717e-04 -1.70075923e-01  1.94638036e-04 ...  1.73828297e-03
  -1.25491187e-01 -3.33429813e-01]
 [-1.24875985e-01 -4.02888935e-03 -3.27211630e-04 ... -1.54918956e-03
   0.00000000e+00  1.19946681e-01]
 [ 0.00000000e+00  4.88922582e-04 -2.12982995e-03 ...  2.25565434e-02
  -9.37235653e-02 -3.40055563e-02]
 ...
 [-1.37537010e-02  5.64146042e-03 -7.64874974e-03 ...  6.58124126e-03
   2.60158144e-02  3.32822558e-03]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
   0.00000000e+00  0.00000000e+00]
 [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00
   0.00000000e+00  0.00000000e+00]]
layers.1.weight: [[-0.00407581  0.06495175  0.         ...  0.          0.
   0.        ]
 [ 0.00034964 -0.00024885  0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 ...
 [ 0.02619006  0.00036845  0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.07627868  0.00244946  0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[ 0.56512666  1.2472919   0.41493452 -0.7375171   0.          1.3126941
   2.3709517   0.90742093 -1.0416322  -1.1554064  -2.3611002  -1.8069383
  -1.558777   -0.6392718   0.         -1.0652703  -0.6151475   0.33267382
   1.0377253   1.2887725  -0.690134   -1.1590277  -1.1337421   0.8180164
   0.38908127 -1.7267988  -1.341488    2.3902621   0.91970783  0.93213767
   0.          1.6049172 ]]

Final Loss: 0.0013
Distance Metric: 18.1873
L1 norm: 5e-06
L2 norm: 0
Batch size: 32
Clipping: 0
Learning rate: 0.005
data size: 5120

================================================================================

