Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

nonoverlappingCNN_relu -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[ 0.46489614 -0.16944574 -0.22846155 ...  0.33755133  0.3874084
   0.17943308]
 [-0.09127592  0.05771792  0.1579187  ...  0.6395371  -0.6830466
   0.14381772]
 [-0.13569446 -0.6899523  -0.02606043 ...  0.39826164  0.32233584
   0.20532674]
 ...
 [-1.005986   -0.04285526  0.5851449  ... -0.10196488  0.16456233
  -0.16595747]
 [ 0.2788078   0.20501374  0.3380513  ... -0.17613307 -0.43920827
  -0.58379835]
 [ 0.1111675   0.42690995  0.         ...  0.1280799   0.12808168
   0.00738999]]
layers.1.weight: [[ 0.10273253 -0.08040771 -0.0937128  ... -0.11103229  0.
  -0.00193079]
 [ 0.15948723  0.15083213  0.01148737 ...  0.12027439  0.0924975
   0.        ]
 [-0.02533859 -0.18367437 -0.06565722 ... -0.23079254  0.06581893
   0.00737791]
 ...
 [-0.20491834 -0.01775283  0.         ... -0.06877591 -0.0769245
  -0.00614745]
 [-0.06851397  0.         -0.13987109 ... -0.20717506  0.11139148
  -0.04634706]
 [ 0.03746922  0.          0.0820402  ... -0.49372375  0.09737357
   0.23112102]]
layers.2.weight: [[ 0.          0.08309951 -0.07327504 -0.39484915 -0.49698126 -0.7779487
   0.2578329   0.27011433 -0.5757589  -0.89810723 -0.27271196 -0.36389875
   0.         -0.23446038 -0.42031768 -0.11145568  0.05275561  0.42814156
   0.07208657 -0.19545725  0.526526   -1.1318382   0.2179783  -0.55890375
   0.13192996 -0.6648878  -0.25570658 -0.15071622 -0.47695985 -0.97152257
  -1.2629615  -0.19247326 -0.0650872  -0.9672448  -0.5106465   0.19685175
  -0.29810402  0.10506593 -0.50268537 -0.20525157 -0.15107276 -0.34019142
  -0.37995765 -0.20323092 -0.03662818 -0.21173163  0.32649434 -0.08268732
  -0.09141724  0.6232046  -0.15961075 -0.10214777 -0.06797353 -0.13155742
   0.2301612  -0.01811953 -0.29392993 -0.0296647   0.33240044 -0.6487808
   0.41475722 -0.28108865 -0.59487283 -0.20956852 -0.38712606 -1.103526
  -0.37104234 -0.56861603 -0.86649275 -0.8366985   0.14833815 -0.00770364
  -0.1606106   0.53591794 -0.42708954 -0.23364606 -0.52639467  0.09717632
  -0.01322524 -0.6391122  -0.2591587  -0.28064352 -0.170556   -0.41155902
  -0.23341553 -0.0731652  -0.13556364 -0.7165746  -0.5827177  -0.22464938
   0.28918654  0.02261797  0.09582446  0.005959   -0.24953333 -0.4012655
  -0.70788854 -0.4350811  -0.2616034  -0.1784291   0.42547324 -0.03818454
  -0.15235293  0.38283822 -0.11559748 -0.43748173  0.17463648 -0.7672624
  -1.1490887  -0.02196964 -0.16461419 -1.2102028   0.10755362 -0.4004077
   0.7453062  -0.39736134 -0.05752262 -0.0315415  -0.56804156 -0.3203423
  -0.2859915  -0.38497177 -0.52846843 -0.6166869   0.6336014  -0.7860693
  -0.25677624 -1.1199499 ]]

Final Loss: 5.0419
Distance Metric: 40.2045
L1 norm: 1e-05
L2 norm: 0
Batch size: 32
Clipping: 0

================================================================================

nonoverlappingCNN_tanh -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.         -0.3888973   0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.4481757   0.
   0.          0.          0.          0.          0.          0.37302232
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.4850335   0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.        ]]

Final Loss: 0.0004
Distance Metric: 11.4946
L1 norm: 1e-05
L2 norm: 0
Batch size: 32
Clipping: 0

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 2.8504938e-04  2.8301304e-04  2.8142173e-04  2.8745338e-04
   2.8241856e-04  2.8081983e-04  2.8746901e-04  2.8581993e-04
   2.8305277e-04  2.8270189e-04  2.8324037e-04  2.8564865e-04
   2.7644369e-04  2.8880368e-04  2.8876681e-04  2.8609845e-04
   2.8017643e-04  2.8138387e-04  2.8426852e-04  1.4755028e-01
   2.8530211e-04  2.7685182e-04  1.4449242e-03  2.8385190e-04
   1.7061636e-01  2.8127956e-04  2.8073616e-04  2.7697074e-04
   2.8535750e-04  2.9002447e-04  2.8734747e-04  2.8999726e-04
   2.8729480e-04  2.8514685e-04  2.8134050e-04  2.8919635e-04
   2.8603457e-04  2.8400958e-04  2.8233352e-04  2.8737029e-04
   2.8217168e-04  2.8048269e-04  2.8616533e-04  2.8495348e-04
   2.8200462e-04  2.4479660e-01  2.8106780e-04  2.8912650e-04
   2.8130540e-04  2.8397259e-04  2.8123730e-04  2.8940770e-04
   2.9294604e-01  5.9651458e-01  2.8090036e-04  2.7153027e-01
   2.7680837e-04  2.8948422e-04  2.8493092e-04  2.8617983e-04
   2.8861884e-04  2.8567051e-04  2.8187322e-04  2.8324031e-04
   2.8813552e-04  2.8101169e-04  2.8087475e-04  2.8952892e-04
   2.8706601e-04  2.8203402e-04  2.8185008e-04  2.8100098e-04
   2.8564758e-04  2.7687321e-04  2.8393776e-04  2.8140243e-04
   2.8235430e-04  2.8535008e-04  2.8215221e-04  2.8343822e-04
   2.8896003e-04  2.8364561e-04  2.8752041e-04  2.8469611e-04
   2.8189100e-04  2.7682277e-04  2.8406599e-04  2.8185986e-04
   2.8894452e-04  2.8121352e-04 -2.0537634e+00  2.8714791e-04
   2.8436328e-04  2.8880901e-04  2.8577787e-04  2.8859350e-04
   2.8899475e-04  2.8252497e-04  2.8557584e-04  2.8429218e-04
   2.8723109e-04  2.8802271e-04  2.8456410e-04  2.8467260e-04
   7.8195585e-03  2.8317297e-04  2.8952095e-04  2.7689993e-04
   2.8466943e-04  2.8997051e-04  2.8916157e-04  2.8183800e-04
   5.9774555e-02  2.7682795e-04  2.8988012e-04  2.8313277e-04
   2.8908896e-04  2.8428758e-04  2.9010413e-04  2.8702355e-04
   2.7673983e-04  2.8445298e-04  2.8339645e-04  2.8453054e-04
   2.8952502e-04  2.8850968e-04  2.8945471e-04  2.8881576e-04]]

Final Loss: 0.0003
Distance Metric: 10.3227
L1 norm: 1e-05
L2 norm: 0
Batch size: 32
Clipping: 0

================================================================================

