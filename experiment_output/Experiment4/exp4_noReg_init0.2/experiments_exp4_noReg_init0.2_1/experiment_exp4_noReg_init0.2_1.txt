Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[0.11674231 0.17623578 0.0038084 ]]]
layers.1.weight: [[[-0.16627426  0.05092204]]]
layers.2.weight: [[[-0.08269805 -0.2032088 ]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00382134  0.03178837  0.01173365 ... -0.01497536  0.03777045
  -0.01763328]
 [-0.03637891  0.03769454 -0.02504522 ... -0.0138485   0.01961791
  -0.02410329]
 [-0.04055517 -0.00032048  0.02099562 ... -0.03283655  0.00566937
   0.00408686]
 ...
 [ 0.01405275  0.03576941 -0.02664145 ...  0.01843898  0.03398881
  -0.00997138]
 [ 0.02928396  0.03668912 -0.0085125  ... -0.02353793 -0.0106537
   0.02629833]
 [ 0.02350328 -0.02158246  0.02767985 ... -0.02955792 -0.00761486
   0.03271601]]
layers.1.weight: [[-0.00790296  0.01837855 -0.02997553 ...  0.02169916  0.02207133
   0.00391344]
 [-0.02447446  0.02698938  0.00115839 ... -0.02916897 -0.02471775
   0.01286853]
 [-0.00599422 -0.00279189 -0.0266501  ... -0.00586491 -0.02637307
  -0.02978758]
 ...
 [ 0.02892988  0.02793919  0.00047063 ...  0.02815093 -0.0051955
   0.02329232]
 [-0.00081045  0.01898189  0.02539816 ...  0.00776839  0.02641092
  -0.00137787]
 [ 0.00277853 -0.00654428  0.02174464 ...  0.01076011  0.02503804
  -0.02864433]]
layers.2.weight: [[ 0.02329493 -0.02873066 -0.00639824  0.0058523   0.00680007  0.01483588
   0.00442657  0.01279114 -0.04615727 -0.0081719  -0.01804177 -0.04223756
  -0.02584915  0.03304052 -0.03372431 -0.03822978  0.03278366  0.02144502
   0.0361523  -0.01695505  0.02133247 -0.01209854 -0.02162017 -0.048107
   0.00182571  0.03066746 -0.01697478 -0.03324207 -0.04427569 -0.00362528
   0.03233039  0.03101045  0.01902156 -0.01857939  0.01701073 -0.01180544
   0.01996846 -0.03646648  0.03857343 -0.0238725   0.0294637  -0.02410758
   0.00646203 -0.00296142  0.00117919 -0.00973506 -0.04892946 -0.01297214
  -0.02979861 -0.03726898 -0.03360484  0.00959205 -0.03577783  0.01128684
   0.03290166 -0.02737046 -0.01470198  0.00823166 -0.0025643   0.02697668
   0.03437868 -0.00760049  0.00930105 -0.04171034  0.0180043   0.02851092
   0.01043563  0.02134817 -0.01445097 -0.03315375  0.01533483 -0.02748318
   0.01706908 -0.0021357   0.00734103 -0.00582785  0.03343782  0.00194074
  -0.0216753   0.01751207  0.01748294  0.0183032  -0.00480399 -0.03394491
   0.04679256  0.013642    0.00247763  0.03153839  0.01828452  0.00517424
   0.03359904 -0.02422379 -0.00471501 -0.02189696 -0.02942859  0.00809448
   0.0353324  -0.03584835 -0.00653074 -0.02650125  0.03014753 -0.01715372
  -0.04958643 -0.02336177 -0.00971305 -0.02798281 -0.02830516  0.00336915
  -0.01116025  0.03465249  0.01695534 -0.04647974 -0.02326205  0.00215692
  -0.04202048  0.01668299  0.02907891  0.01674378  0.03845419 -0.03181056
   0.01363071 -0.02728015  0.0046814  -0.01737126  0.02703858  0.02730061
  -0.02872269  0.02337344]]

Final Loss: 0.0000
Distance Metric: 3.9140
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.20716038 -0.04945659 -0.00314888 ...  0.0603885   0.00636537
  -0.08537931]
 [ 0.11802125 -0.09322481 -0.04134776 ...  0.0335736  -0.02230057
   0.00977288]
 [ 0.22600174  0.04501373  0.1085332  ... -0.00954002  0.11993775
   0.05156646]
 ...
 [ 0.23093088  0.27221218  0.21653003 ... -0.16032736 -0.07597061
   0.07381656]
 [-0.24394271 -0.00635309 -0.11307878 ... -0.06383522  0.23708408
   0.05939673]
 [ 0.21152541 -0.01902003  0.1965637  ... -0.02837575  0.0790766
   0.10033585]]
layers.1.weight: [[-0.15377168 -0.10538258 -0.01917417 ...  0.04774167 -0.20425594
   0.04396033]
 [ 0.2758172  -0.15132996 -0.01377772 ...  0.03166046 -0.04611491
   0.2416529 ]
 [-0.11523752 -0.01728354  0.02396605 ... -0.164371    0.04748709
  -0.00606722]
 ...
 [-0.04684529  0.00320793  0.03923051 ... -0.10916822  0.03818358
   0.13940488]
 [ 0.1554081  -0.00472192  0.19239898 ...  0.08753708 -0.07669741
  -0.33132413]
 [ 0.30069008  0.01389228 -0.03510093 ...  0.12319064 -0.19914588
  -0.06593323]]
layers.2.weight: [[-0.35399923  0.0972105   0.09770394  0.09792273  0.11510899 -0.11472916
  -0.14105953 -0.3149182  -0.11705352  0.12822331 -0.2454039   0.08245412
   0.10271137  0.1048425  -0.1266904   0.09915045 -0.25758952 -0.3617885
   0.10127319 -0.14572307  0.10317244  0.12751694  0.10162471 -0.14715236
  -0.17400894  0.17701107  0.11716902  0.09916452  0.10868099  0.12828442
  -0.31387442  0.10260668 -0.20782533 -0.26437023  0.10580297  0.12250442
   0.10921812  0.11087897  0.11842807  0.11717989  0.1011263  -0.15546824
   0.08977224 -0.15855578 -0.11752959  0.09290452 -0.16760309 -0.17759821
   0.12113772  0.12184749 -0.15626478  0.1034509  -0.11496086 -0.11614267
  -0.29624733  0.08517177 -0.12105977 -0.1160996  -0.21357982 -0.26902506
   0.14253253 -0.16119255 -0.14401479 -0.32596    -0.33319962 -0.18019027
   0.1461696  -0.12146758  0.09480237  0.09957276 -0.3990584  -0.11292163
   0.09365578  0.10983984 -0.14994046  0.1232747  -0.10895608 -0.12870966
  -0.29125875 -0.12512647  0.11876435 -0.12916525 -0.21413404 -0.11953377
  -0.14932777  0.15020147 -0.1290902  -0.11905898 -0.13993503  0.10000001
   0.09876101 -0.12693132 -0.16085716  0.11597338  0.09107406 -0.14907274
  -0.22973108 -0.23433605  0.13921706  0.11578239 -0.12571234  0.12388249
   0.09321285 -0.14754829 -0.24414468 -0.3196455   0.22679277 -0.11800689
  -0.1996297   0.09776866  0.13683164  0.16014181  0.11490493  0.10052948
   0.09577398  0.12134558  0.09922223  0.09375329  0.12380433 -0.20305042
   0.10343482  0.10659425  0.09934128  0.21679047  0.08623655 -0.21506365
   0.10968937  0.09016282]]

Final Loss: 0.0062
Distance Metric: 26.5411
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.03369328  0.01003333 -0.00943603 ...  0.0091174  -0.02748878
   0.01966098]
 [ 0.02774321  0.00501612 -0.03999481 ... -0.02349541 -0.00443404
  -0.03571777]
 [-0.03449322 -0.01531938 -0.00163916 ... -0.02338868  0.01426995
  -0.04128832]
 ...
 [-0.03918589 -0.03232911 -0.01427323 ... -0.00463598 -0.01010106
  -0.03701681]
 [ 0.02952083 -0.0177938   0.02619671 ... -0.00984131  0.04215688
  -0.0411042 ]
 [-0.02091067  0.00560915 -0.03837502 ...  0.00364331  0.03852953
   0.03018281]]
layers.1.weight: [[-0.02431793 -0.03820398 -0.00548535 ... -0.04223442  0.01328608
  -0.0060494 ]
 [ 0.00049963 -0.04094489  0.03523011 ...  0.01629854 -0.00837276
  -0.00543661]
 [ 0.04111391  0.         -0.04810227 ... -0.0240035   0.03829543
   0.03553296]
 ...
 [ 0.02601688 -0.03657505  0.04550216 ...  0.02773477 -0.03676585
  -0.00931988]
 [-0.02742537 -0.04819051 -0.02224732 ... -0.03877585 -0.03244772
  -0.04302255]
 [-0.01887405 -0.0235431  -0.02603661 ... -0.00653302  0.01412763
  -0.02692464]]
layers.2.weight: [[-0.0027415  -0.00225964 -0.00124254  0.00408207 -0.00581188 -0.00082853
  -0.00569321  0.0013608   0.00536413 -0.00460201 -0.0002066   0.00081182
  -0.00049238 -0.00196012 -0.00268103 -0.00086197  0.00231343  0.00195868
  -0.00532569  0.00209204  0.00411226  0.00322015 -0.00444176 -0.0002588
  -0.00013675 -0.0012609  -0.00197306  0.00054139 -0.00235226 -0.00265195
  -0.00120065 -0.00064218 -0.00133363  0.00658807 -0.00032151  0.00266921
   0.00189611  0.00114628 -0.0017859  -0.006432   -0.00028539 -0.00032647
   0.          0.00084806 -0.00176368 -0.00053327 -0.00091618 -0.00031809
   0.00306607 -0.00256816 -0.0047386   0.0017168  -0.00562331 -0.0022182
   0.00149601  0.0002261  -0.00461283  0.00150622  0.00376794 -0.00014025
   0.00187578 -0.00092432  0.         -0.0021979   0.00083735 -0.00204331
   0.00014417 -0.00027262 -0.00105958 -0.00383731 -0.00223703 -0.00195319
   0.00140391 -0.00161003  0.00385165 -0.00385076  0.00164434 -0.0020304
  -0.00169281  0.0023629   0.00256792  0.00203915 -0.00090956 -0.00313378
   0.00117357 -0.00273423  0.00159815 -0.00294254 -0.00215493 -0.00230584
   0.0021434   0.0030494  -0.00024372 -0.00325587 -0.00465986 -0.00338627
  -0.00078793 -0.0023111   0.00168154  0.0016883  -0.00135669 -0.00122164
   0.00283931 -0.00462824  0.00216407  0.00024076  0.00205342  0.0024761
   0.00243094  0.00209824 -0.0012209  -0.00179731  0.00225995  0.00182388
  -0.00042615  0.00361503  0.00294073 -0.00236906  0.00013066  0.00463781
  -0.00512266  0.00359441 -0.00533303 -0.00492402 -0.00024213  0.0001262
   0.00289563  0.00490329]]

Final Loss: 0.2007
Distance Metric: 5.1147
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 211

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.02168357 -0.02972143 -0.00628887 ... -0.00365983 -0.01991182
   0.00189115]
 [ 0.01990723  0.00799619  0.02861956 ... -0.02173582 -0.02713606
  -0.03222016]
 [-0.03187007 -0.00628365 -0.04000759 ... -0.02672039  0.00415771
  -0.00676581]
 ...
 [-0.00698215 -0.00908678 -0.01088687 ...  0.03763104  0.02540607
   0.00075456]
 [-0.03274162  0.04028537 -0.01315223 ... -0.0356286   0.00692479
  -0.01405453]
 [-0.03456388 -0.02258419  0.01932739 ...  0.01517297 -0.04125759
  -0.04054207]]
layers.1.weight: [[-0.0126311   0.03052704 -0.02367276 ...  0.01420252  0.0234491
  -0.00951005]
 [ 0.00924631 -0.02177411 -0.02868284 ... -0.00599355  0.01023566
  -0.00852801]
 [-0.00977449  0.02933889 -0.02822276 ... -0.01150502 -0.00528488
   0.02890168]
 ...
 [-0.0207336  -0.01782827 -0.01025903 ... -0.0249774   0.01238207
  -0.02430289]
 [ 0.02688383 -0.02192635  0.0232755  ... -0.01388179 -0.01092295
   0.01337066]
 [-0.00991217 -0.02444709  0.01311337 ...  0.01052577  0.00890392
  -0.00539226]]
layers.2.weight: [[-0.125      -0.09588739 -0.09461463 -0.11196114 -0.13316773 -0.08676966
  -0.08146833 -0.11488322 -0.09685519 -0.14887767 -0.125      -0.08721727
  -0.14788139 -0.07895409 -0.09912042 -0.16689846 -0.12014564 -0.125
  -0.13619563 -0.13402325 -0.13197672 -0.12980658 -0.13224603 -0.11185431
  -0.13301092 -0.125      -0.1545115  -0.11137284 -0.14422822 -0.13009532
  -0.125      -0.125      -0.14729027 -0.11021855 -0.16032247 -0.12641409
  -0.10184705 -0.15490688 -0.1330662  -0.11594913 -0.15037966 -0.14658579
  -0.125      -0.14594384 -0.125      -0.09228792 -0.09436058 -0.16293238
  -0.13341726 -0.09306946 -0.15179507 -0.11818196 -0.1481547  -0.11184908
  -0.09021698 -0.08548203 -0.13347425 -0.14590926 -0.09682066 -0.11340499
  -0.14979185 -0.13292608 -0.11717859 -0.150418   -0.11001422 -0.09020606
  -0.13702978 -0.16092415 -0.09689221 -0.125      -0.10844042 -0.14526948
  -0.10639542 -0.14443839 -0.125      -0.11919086 -0.10455368 -0.16197136
  -0.15538926 -0.14016777 -0.15501733 -0.10783011 -0.15870157 -0.14349055
  -0.13738571 -0.125      -0.1432993  -0.13368163 -0.10925561 -0.08089204
  -0.08621462 -0.14111514 -0.12665856 -0.10637397 -0.12137092 -0.1019102
  -0.125      -0.14092636 -0.10043953 -0.15449356 -0.11418661 -0.14219548
  -0.08560702 -0.08135432 -0.13316779 -0.125      -0.09507924 -0.1250015
  -0.1097389  -0.1156398  -0.1192586  -0.08339006 -0.08881106 -0.14706098
  -0.14547384 -0.1377348  -0.10252596 -0.11071142 -0.13205266 -0.08957064
  -0.15618905 -0.07619768 -0.1364948  -0.09779768 -0.13744599 -0.09619103
  -0.13237663 -0.125     ]]

Final Loss: 0.0000
Distance Metric: 4.9276
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.00314752 -0.00728406 -0.01977064 ...  0.         -0.02359373
   0.00086235]
 [ 0.00452441  0.00283676 -0.00022485 ... -0.02604878  0.03000077
   0.01488831]
 [ 0.00427565  0.00443043  0.01233747 ... -0.0050821   0.02276208
   0.00829232]
 ...
 [ 0.00551146 -0.00786984  0.00176937 ... -0.00507861  0.03765545
   0.0264842 ]
 [-0.00790537 -0.00997826 -0.00406966 ... -0.04570568 -0.01153514
  -0.00486033]
 [-0.00049132 -0.00721053  0.0138179  ...  0.00825869  0.02233765
   0.00793794]]
layers.1.weight: [[ 0.00876931  0.02005783  0.01076997 ...  0.03324467  0.02200623
   0.02769066]
 [ 0.03157663  0.00169259  0.04888976 ...  0.00472702 -0.00383414
  -0.01991314]
 [-0.02436885  0.00773755 -0.01002496 ... -0.04416354  0.01131923
   0.02101811]
 ...
 [-0.00648884 -0.01056805 -0.00930928 ...  0.01521439  0.01698928
   0.01323972]
 [-0.01507185 -0.00388849 -0.02377235 ... -0.01115503 -0.00423213
   0.0233575 ]
 [ 0.00463386  0.02146882 -0.04066033 ...  0.02748158 -0.01576399
  -0.00301435]]
layers.2.weight: [[ 8.65826383e-02  3.97109181e-01  8.83491561e-02 -8.99425596e-02
   1.21329427e-01 -3.66689324e-01 -1.45154983e-01 -3.36852133e-01
  -3.89269902e-03 -5.16178608e-01  6.06356412e-02 -9.35614556e-02
  -1.85345978e-01 -1.88059639e-02  2.06422806e-01 -2.54441172e-01
   2.20537767e-01  2.60140784e-02 -2.54479259e-01  2.02169880e-01
  -9.04731080e-02 -5.17370462e-01 -2.05827102e-01 -8.34991261e-02
  -1.38221458e-01 -1.81305804e-03  4.72219437e-02  1.91719793e-02
  -1.30112126e-01  1.03221923e-01  3.82818095e-02  1.24215178e-01
  -2.58934796e-01 -1.20932199e-01  2.22388163e-01 -3.53893816e-01
  -1.63360611e-02 -6.18484914e-01  5.24175763e-01  8.34981352e-02
  -2.47382790e-01 -1.13177046e-01 -6.52377069e-01  3.30353409e-01
   1.58528268e-01  2.85970718e-02 -6.02364913e-02 -3.29240449e-02
   3.29834789e-01 -1.43081024e-01  4.20726746e-01  1.19350530e-01
   3.41696620e-01  3.43550265e-01 -1.38181016e-01 -3.26920748e-01
   6.24746561e-01  3.67238432e-01  2.68713862e-01  2.23219603e-01
   3.17408353e-01 -8.36718455e-02  4.74652529e-01  1.35868803e-01
  -1.31778806e-01 -3.04255962e-01 -1.92713346e-02  4.66676414e-01
   1.65282056e-01 -6.81704981e-03  1.89069495e-01  5.11212230e-01
  -2.06856817e-01 -4.86802995e-01  2.10363656e-01  7.41038203e-01
  -4.87978935e-01  3.80776763e-01  3.71084064e-01 -9.77947861e-02
   4.86894511e-03  1.51672065e-01  3.27716768e-01 -2.27732986e-01
  -5.73930368e-02 -2.13528082e-01  3.54325920e-01 -3.61030012e-01
  -2.21165389e-01 -1.62449658e-01  5.74271418e-02  3.49024713e-01
   4.45063636e-02 -7.74381012e-02  1.37209535e-01 -1.42467856e-01
   1.89054787e-01  7.16203973e-02 -2.13991001e-01  3.66502494e-01
  -1.48934141e-01  8.43036950e-01  6.03316212e-03  5.20877838e-02
  -3.87649089e-01 -1.36637300e-01 -4.18390989e-01 -1.54854685e-01
   7.29593337e-02  4.86023426e-02 -2.24648967e-01  1.81131616e-01
   5.28274439e-02 -7.13567883e-02  9.66082513e-03 -1.95899475e-02
   3.08448195e-01  9.87332985e-02  1.39795735e-01  1.39984384e-01
   3.25261764e-02 -5.90575160e-04 -2.89967041e-02 -1.31482601e-01
  -4.09032822e-01  1.59432888e-01  2.53838092e-01 -5.38166277e-02]]

Final Loss: 0.0000
Distance Metric: 7.2608
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.00175359 -0.05655989  0.04624311 ... -0.00015387  0.02360835
   0.06964441]
 [-0.06476958 -0.04260977 -0.05170498 ...  0.03218773  0.05789924
   0.03223239]
 [-0.01570394  0.01396428 -0.02949513 ... -0.05993307 -0.0014749
  -0.04410912]
 ...
 [-0.01890925  0.01136587 -0.06475871 ... -0.02495768 -0.00998943
   0.05811088]
 [ 0.01069682  0.0520946  -0.03265265 ...  0.06278396 -0.00277785
  -0.05476151]
 [ 0.01784924  0.0369967   0.05110788 ... -0.03626469 -0.05787456
  -0.05952629]]
layers.1.weight: [[-0.02374768 -0.01450674  0.02675088 ... -0.02787099 -0.03585171
  -0.02699374]
 [ 0.00149295 -0.03200629  0.03420068 ... -0.00487107  0.03903455
   0.00953247]
 [ 0.01372979 -0.01816378 -0.03133462 ...  0.02194211 -0.04636706
  -0.00622108]
 ...
 [-0.04917713  0.00340077 -0.03354817 ... -0.02651718 -0.03552843
  -0.03574426]
 [ 0.01756501 -0.0486711   0.00770456 ...  0.04041945  0.041484
  -0.03337856]
 [-0.03858241  0.00959623  0.04192642 ... -0.04015543  0.03540892
   0.00100113]]
layers.2.weight: [[-0.03965182  0.04121231 -0.05280077 -0.03452895 -0.02695834  0.01520703
  -0.01206913  0.01980003  0.0388332   0.03917522 -0.03433178  0.00582784
   0.03494732  0.0157776  -0.01699992  0.01420431 -0.00944136 -0.03583491
  -0.01039724 -0.01483744 -0.02112046  0.01311305 -0.02038562 -0.02766663
   0.02734349  0.04617963  0.002225   -0.03762961  0.01796245 -0.01984065
  -0.00356198  0.         -0.00193213  0.          0.04736429  0.02784224
  -0.03204099  0.03193806  0.03295116 -0.04010175  0.01889519  0.01136135
   0.02518169 -0.00047259 -0.01695117 -0.01021984  0.05767824  0.04444551
   0.0331911   0.01935268  0.02858989 -0.00862399  0.00995317  0.00682526
   0.02906229  0.00540584 -0.02144104 -0.0285089  -0.03188922 -0.02459745
  -0.03810341  0.04139578 -0.02975366 -0.01688143 -0.01104686  0.02996546
  -0.01922962  0.04689282  0.03350311  0.0086343  -0.01216877  0.01120768
   0.00969216  0.0193067  -0.02426525 -0.02507816  0.00428924 -0.03525449
  -0.03741782  0.04090093 -0.02106774 -0.02698633  0.01429987  0.00433704
  -0.02692043 -0.02453831  0.01250405  0.01414895  0.02206121  0.03670059
  -0.02691577 -0.01389024 -0.00820872  0.01888529 -0.04143511  0.00606037
   0.01443946 -0.03416467  0.01760915  0.00854758 -0.03878119  0.02794124
  -0.01225977 -0.02234776  0.01964655  0.04089268  0.00280145  0.02448134
   0.0056926  -0.04020727  0.00924868 -0.02384496 -0.03326824 -0.05740013
  -0.03232262 -0.04176239  0.0207173  -0.0217416  -0.00731153 -0.02884164
  -0.02872914  0.00494538  0.03001621  0.00925658  0.02565423 -0.03064124
  -0.03544514  0.03252564]]

Final Loss: 0.0000
Distance Metric: 6.0475
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.03673248 -0.00375829  0.00179237 ... -0.00371815  0.00151712
   0.03850131]
 [ 0.04051438  0.00685555 -0.04028215 ... -0.03801421 -0.00747979
  -0.02709579]
 [-0.0177296  -0.01436876 -0.00618745 ... -0.00988647  0.00607386
  -0.02649567]
 ...
 [-0.02731261 -0.02292512 -0.01017834 ... -0.01219262  0.00634873
  -0.04118396]
 [ 0.0250203  -0.0095135   0.02767583 ...  0.0055064   0.03940272
  -0.01472067]
 [-0.02049069 -0.0185612  -0.0342904  ...  0.01022976  0.03712849
   0.03410504]]
layers.1.weight: [[-0.01047746 -0.02601452 -0.00356915 ... -0.02864986  0.00948744
  -0.00038476]
 [ 0.00409057 -0.02480589  0.01819233 ...  0.0078417  -0.00116785
  -0.00293918]
 [ 0.02544626 -0.0068612  -0.02992201 ... -0.01190113  0.02111063
   0.02406888]
 ...
 [ 0.01680123 -0.01678633  0.02890436 ...  0.01688051 -0.01160893
  -0.0038615 ]
 [-0.01598226 -0.02545263 -0.01662745 ... -0.01809367 -0.01553266
  -0.02104266]
 [-0.01200617 -0.01931506 -0.01610589 ... -0.00305223  0.01631871
  -0.01625274]]
layers.2.weight: [[-0.08056158 -0.10926557 -0.06040425 -0.10057599 -0.15960298 -0.14166166
  -0.14234877 -0.07771317 -0.12006418 -0.15180063 -0.05047564 -0.15585127
  -0.15079203 -0.10995901 -0.10109257 -0.11675858 -0.10917259 -0.05541164
  -0.09103876 -0.14854337 -0.07948641 -0.08316508 -0.11309227 -0.1353315
  -0.1054114  -0.10366371 -0.16085237 -0.13352402 -0.13839018 -0.14032225
  -0.08479687 -0.12468579 -0.14123158 -0.06744926 -0.06150582 -0.11580958
  -0.16671039 -0.09773792 -0.12147436 -0.11120716 -0.12814839 -0.10829579
  -0.12846509 -0.07037682 -0.07883895 -0.12117156 -0.07415776 -0.07834129
  -0.17221916 -0.11242419 -0.11017876 -0.07310123 -0.11244227 -0.13904737
  -0.11454234 -0.05881319 -0.07299174 -0.0570216  -0.09583028 -0.10044727
  -0.11069719 -0.08980608 -0.10764098 -0.07071533 -0.10504967 -0.1289882
  -0.13101485 -0.16178699 -0.1339407  -0.14710723 -0.06064881 -0.12133311
  -0.10945977 -0.13700284 -0.06938738 -0.13142723 -0.06331078 -0.07239947
  -0.10500536 -0.09327473 -0.08225052 -0.09642112 -0.08129227 -0.1343816
  -0.13411938 -0.08459204 -0.14760023 -0.13156201 -0.06298307 -0.1356615
  -0.11092584 -0.10462003 -0.1091353  -0.15036164 -0.13627984 -0.08186068
  -0.10070376 -0.08710669 -0.087434   -0.16786946 -0.12869771 -0.1213965
  -0.09430196 -0.14354905 -0.10216612 -0.08329232 -0.15121195 -0.08572441
  -0.09000305 -0.13630381 -0.06300848 -0.12441617 -0.13836226 -0.05219876
  -0.08415608 -0.08706164 -0.0721851  -0.11069367 -0.16415724 -0.06953743
  -0.13637726 -0.15595062 -0.14725581 -0.16696979 -0.15390529 -0.08377805
  -0.13047864 -0.10238899]]

Final Loss: 0.0009
Distance Metric: 5.2632
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.01745113 -0.00810258  0.01137738 ... -0.00211943  0.02496706
  -0.00048095]
 [ 0.00262281  0.00533552 -0.00079218 ...  0.00504589 -0.01569885
  -0.00489546]
 [ 0.0078739  -0.00961842  0.00028467 ... -0.00861225  0.01586761
   0.00543665]
 ...
 [ 0.00542808  0.02100143  0.02156702 ... -0.00162688 -0.01178814
  -0.00082422]
 [-0.01676441  0.00079601  0.00237179 ... -0.007881    0.01718838
  -0.01560434]
 [ 0.0016225  -0.00535381  0.00096001 ... -0.00396473 -0.01447209
  -0.00567874]]
layers.1.weight: [[ 0.02271261  0.00139787 -0.01623163 ...  0.00138872 -0.05619829
   0.00759084]
 [ 0.00099713 -0.01083585  0.02029632 ... -0.0231712   0.01553633
   0.02846697]
 [-0.0403952  -0.01656281 -0.00104379 ...  0.01512685 -0.04143656
  -0.02963566]
 ...
 [ 0.01669779  0.01498574 -0.0679681  ... -0.03148418  0.04080752
   0.00204847]
 [ 0.00960767 -0.00320175  0.01498357 ...  0.01036322  0.03941679
   0.00507663]
 [ 0.01267761  0.01070413 -0.01165053 ...  0.01334195 -0.05191161
   0.0278923 ]]
layers.2.weight: [[-0.45544463  0.24239437  0.7867847   0.26428255  0.04596562 -0.08978955
  -0.10825038 -0.26224113 -0.02275232  0.08283193 -0.3256988   0.22158001
   0.0318469   0.13101116 -0.05783996  0.46369502 -0.34504288 -0.5821165
   0.5346332  -0.068866    0.00730571  0.3069385   0.13878368 -0.05463973
  -0.2554878   0.49120775  0.02467439  0.08308397  0.39620897  0.4535991
  -0.29006246  0.24260858 -0.08838592 -0.55210185  0.05621682  0.2788022
   0.01134529  0.20468123  0.26812175  0.16102216  0.11655916 -0.22610722
   0.7976966  -0.0912514  -0.03957915  0.33581293 -0.13711138 -0.25224862
   0.07331502  0.3619567  -0.17282328  0.12968983 -0.13019627 -0.00174585
  -0.284956    0.25459364 -0.03502127 -0.09298144 -0.33261833 -0.48287338
   0.23103254 -0.21922135 -0.09609296 -0.72758836 -0.33241522 -0.10269753
   0.17934752 -0.03054217  0.17211954  0.04571891 -0.5728697  -0.11162876
   0.14362986  0.11741189 -0.15132482  0.109051   -0.0066741  -0.131301
  -0.40632367 -0.19257978  0.48257938 -0.09386362 -0.11542133 -0.2046637
  -0.07653362  0.4622667  -0.03825694 -0.10049243 -0.10907612  0.36905634
   0.3886389  -0.03804617 -0.1374792   0.19874828  0.19190623 -0.11912413
  -0.33759892 -0.15606575  0.23072597  0.00735357 -0.11502584  0.09872258
   0.34719595 -0.03421368 -0.15686712 -0.43420804  0.5320255  -0.05925644
  -0.19146124  0.24648705 -0.01760567  0.07620414  0.05681697  0.5116927
   0.5016617   0.41772178  0.5068145   0.12658808  0.01118011 -0.40911275
   0.16542538  0.08444253  0.09688513  0.02200413  0.40227625 -0.26094687
   0.03501799  0.21389666]]

Final Loss: 0.0004
Distance Metric: 7.2303
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.06087292  0.01251577  0.0155246  ... -0.00035501 -0.05081848
   0.03974212]
 [ 0.06537977  0.02407313 -0.06411947 ... -0.05769823 -0.03454627
  -0.04969783]
 [-0.01946666 -0.01949164 -0.00896475 ... -0.01325626  0.01280366
  -0.04035387]
 ...
 [-0.04362582 -0.03366672 -0.01392987 ... -0.02217471  0.00977009
  -0.06698281]
 [ 0.04789444 -0.00785263  0.04668448 ...  0.01077125  0.04878632
  -0.03361779]
 [-0.03274365 -0.03079333 -0.05674129 ...  0.01855215  0.03182315
   0.0557946 ]]
layers.1.weight: [[-0.02111058 -0.04242466 -0.00378134 ... -0.04974232  0.01159058
  -0.00485473]
 [ 0.00246923 -0.04272708  0.03054832 ...  0.01087357 -0.00718143
  -0.01147449]
 [ 0.03966778 -0.00712056 -0.04714159 ... -0.02165025  0.03267392
   0.03742416]
 ...
 [ 0.02444998 -0.02670301  0.04963333 ...  0.02601328 -0.02411879
  -0.01046268]
 [-0.03152143 -0.04461052 -0.02774147 ... -0.03276418 -0.03184601
  -0.04240755]
 [-0.02393975 -0.03345128 -0.02655909 ... -0.00718267  0.02215058
  -0.03256299]]
layers.2.weight: [[ 0.03583201 -0.02426159  0.06713494  0.027573   -0.06778742 -0.05725149
  -0.05531467  0.04821306  0.0043277  -0.06975346  0.05314116 -0.05863691
  -0.06256506 -0.02201239  0.01770305 -0.02006095  0.02522048  0.06846198
   0.04292272 -0.05869542  0.03663778  0.05070999 -0.00521379 -0.01969332
   0.01371741 -0.01478094 -0.06804886 -0.02001097 -0.02982307 -0.03305624
   0.05146043 -0.02703029 -0.03387564  0.05519648  0.07783338 -0.00361965
  -0.07903522  0.03099997  0.0036789  -0.00825012 -0.01671286 -0.01161287
  -0.02415714  0.06664603  0.01918485 -0.03480474  0.0545524   0.04415512
  -0.04919931  0.00266996  0.01201855  0.05381542 -0.02253946 -0.02126979
   0.01081093  0.07114524  0.06156677  0.07912624  0.01878937  0.03279541
  -0.01547238  0.03557056  0.01486637  0.04797101  0.00838115 -0.05362454
  -0.03547538 -0.05942253 -0.04544014 -0.04431187  0.06277846 -0.02073001
  -0.0015882  -0.0228491   0.05270926 -0.02948216  0.06912502  0.04634865
   0.00510389  0.02575626  0.04396102  0.02751212  0.00540647 -0.01910294
  -0.05761496  0.03898004 -0.03099405 -0.0366214   0.06637085 -0.03758747
  -0.01380346  0.01738453  0.01181465 -0.05671088 -0.00710215  0.03028458
   0.01354069  0.01909229  0.04681787 -0.07954985 -0.00487539 -0.00333478
  -0.00031147 -0.05923179  0.004229    0.03281943 -0.03947252  0.03924781
   0.03520834 -0.0349934   0.06558479 -0.02139961 -0.03777336  0.06365974
   0.02558636  0.03151609  0.0669829  -0.0089768  -0.06433812  0.04187028
  -0.0470567  -0.07029499 -0.07838725 -0.05905703 -0.03333331  0.04600906
  -0.03282905 -0.01793197]]

Final Loss: 0.0000
Distance Metric: 6.5099
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1999

================================================================================

