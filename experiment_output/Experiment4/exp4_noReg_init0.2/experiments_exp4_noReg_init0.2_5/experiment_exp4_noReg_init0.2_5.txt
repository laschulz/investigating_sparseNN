Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.15312181 -0.11383855  0.17919353]]]
layers.1.weight: [[[0.0442166  0.11298411]]]
layers.2.weight: [[[0.10773773 0.23714446]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.02897935 -0.00073333  0.02031199 ...  0.01038413 -0.00978239
   0.03436347]
 [-0.03865303  0.00286238 -0.00030298 ... -0.02467597  0.00938961
   0.0344154 ]
 [-0.02458211 -0.02882533  0.0286281  ...  0.00380082  0.03499538
  -0.02474782]
 ...
 [-0.0208822   0.03782136 -0.0173895  ...  0.03347973  0.00378479
  -0.00801464]
 [-0.02494599 -0.0096368  -0.02811213 ... -0.01383906  0.01955401
  -0.02274142]
 [ 0.02939736  0.00773348 -0.01263462 ...  0.0068468  -0.03531838
  -0.03131942]]
layers.1.weight: [[ 0.02746369  0.01063251  0.01067505 ... -0.03036522 -0.01594025
  -0.02461346]
 [-0.02356735  0.02101895 -0.01915951 ... -0.00544161  0.02278211
   0.0054946 ]
 [ 0.00351665 -0.01070226 -0.02095963 ... -0.01149066 -0.00036751
   0.03005207]
 ...
 [-0.02978276  0.02748344  0.00262367 ... -0.02374312  0.00543472
   0.01155971]
 [-0.01282124 -0.02359568 -0.02228159 ... -0.01965432 -0.02282508
   0.02916863]
 [ 0.0159358  -0.01256409  0.01020399 ... -0.02356314 -0.00469991
   0.00814068]]
layers.2.weight: [[ 0.01431049  0.00339358  0.00841143  0.02684519  0.03448388 -0.00698524
   0.00681576 -0.00758853 -0.03656683  0.02947819  0.00401113 -0.04859466
  -0.0304606   0.02920007  0.01644288 -0.03439467  0.03319889  0.02762503
   0.03975754  0.04129155 -0.01472723  0.02573152  0.04477114 -0.03368765
   0.04453215  0.03985825 -0.0195682   0.02782784 -0.01253861 -0.02243191
   0.00950958  0.03367474  0.05119843 -0.01367986  0.04586398 -0.03073796
  -0.03719863 -0.03075541  0.01423081 -0.03108451  0.00048393 -0.02951869
   0.02177695 -0.01519059  0.02960857  0.03367447 -0.03115172 -0.03334618
  -0.00102476  0.00783259  0.05057259 -0.00043496  0.03158733  0.02510466
  -0.01214412 -0.03677431 -0.00614175  0.02779678 -0.00793212  0.00165425
  -0.01090796 -0.04044447  0.00197343 -0.04178093  0.01200728  0.04038946
  -0.0353949   0.01882015 -0.03559679  0.02720355 -0.01245651  0.009616
  -0.02408618  0.00075055  0.02708513 -0.04142421  0.02323497 -0.02892375
   0.03544934 -0.01840089  0.02462736  0.00899979 -0.04032697  0.03942431
  -0.04307247 -0.02325694  0.00949721  0.01908585  0.03371513 -0.02008346
   0.04470908  0.00455869  0.00576282  0.03642219  0.00746015  0.00829591
  -0.02017508 -0.0109735  -0.03654524 -0.01691624 -0.0378847   0.01385785
   0.02128154  0.0403611   0.02483367  0.00311062  0.03540884  0.01997888
   0.04958177  0.01227701 -0.04069474  0.00885962 -0.01019597 -0.01627211
   0.04074252 -0.01968107 -0.00591633 -0.02602445 -0.0269605   0.00694275
   0.00317397 -0.02410645  0.01879194  0.03459268  0.00514828  0.03049687
  -0.00438496 -0.00656863]]

Final Loss: 0.0000
Distance Metric: 3.9444
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.18591805 -0.09430161  0.18091422 ...  0.05669829  0.16231392
  -0.3180227 ]
 [ 0.00649856  0.0071217  -0.04203966 ...  0.09659527  0.2654494
   0.14131755]
 [-0.28746432 -0.0896925   0.04832    ...  0.06511458  0.36061877
  -0.16513073]
 ...
 [-0.03979804  0.15994996  0.21812713 ... -0.196273   -0.21674237
   0.06670177]
 [ 0.26631206 -0.07195626 -0.05259497 ...  0.21390674 -0.07237026
  -0.1410084 ]
 [-0.04576035 -0.15221344  0.16659269 ...  0.01528458 -0.05942679
  -0.10802655]]
layers.1.weight: [[ 0.04836695  0.23040617  0.1634038  ... -0.25064766  0.18386818
   0.01991271]
 [-0.21547993 -0.08185742 -0.08643409 ... -0.06157833 -0.26770413
  -0.0337967 ]
 [-0.02980114 -0.18088251 -0.19907606 ...  0.07664912 -0.08772674
  -0.12752862]
 ...
 [-0.14106083  0.09869757  0.29753408 ... -0.0609434   0.01532543
  -0.00712978]
 [-0.3172773   0.07176537  0.10495468 ... -0.37017947  0.1081091
   0.06502312]
 [-0.10499416 -0.10501059 -0.17572926 ...  0.13104811 -0.23077437
  -0.08414904]]
layers.2.weight: [[-0.11417557  0.08201591 -0.12526536 -0.14948805  0.08138609  0.1034783
  -0.11380071 -0.15038612 -0.12364764 -0.15796705 -0.27431875 -0.41021973
   0.09324691  0.08965496 -0.1269173   0.08034381  0.11059121  0.08895224
  -0.13015044  0.09101123 -0.14449196  0.09487281  0.09946785 -0.16084215
  -0.12423387  0.09381645 -0.18181737 -0.16988051  0.11324903  0.10013074
  -0.4005537   0.09866081 -0.11410798 -0.10745357  0.1283621  -0.3124228
  -0.1371768  -0.123004   -0.30132616  0.08967115  0.07626774  0.1430277
  -0.11561127 -0.19967869  0.07545608  0.07182737  0.09808954 -0.11597889
  -0.15396394  0.09009849 -0.16834779 -0.17753233  0.07662813  0.09811387
  -0.22363377 -0.12445619  0.10210695  0.11080635  0.11578962  0.09659336
   0.08726333 -0.12064745 -0.2970389   0.08271489 -0.22097237  0.09359374
   0.11144172  0.08754358  0.08727097 -0.12206914 -0.17917094  0.07989624
   0.09567398 -0.2776468  -0.13796641  0.09380919  0.09317597 -0.28316286
   0.08430579 -0.12732403  0.08970163  0.08883971 -0.27320224  0.08959689
   0.08939465  0.10334385  0.08203334 -0.12484843 -0.24920718  0.10804106
   0.11348801  0.09950852  0.09687226 -0.11258639  0.10524274 -0.4339872
   0.0917207   0.077763    0.0875954   0.10366853  0.06860144 -0.29711586
  -0.23041901  0.08777485 -0.12218085 -0.12297069 -0.13451906 -0.43656296
   0.08243143  0.11123529 -0.79865843 -0.1841077   0.07847135  0.11129971
  -0.15317011  0.09338959 -0.12611784  0.10335266 -0.5057334   0.09828995
  -0.17392734  0.08604314  0.09113344  0.10577133 -0.578978    0.09828633
   0.10243566  0.0884757 ]]

Final Loss: 0.0078
Distance Metric: 28.7199
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.00641509 -0.00486078 -0.00668448 ... -0.00634677 -0.01955712
   0.00172838]
 [-0.01835443 -0.00613901 -0.00400752 ...  0.00496016  0.00973402
   0.01978189]
 [ 0.0161624  -0.02269023  0.01412016 ...  0.00046589  0.00781149
  -0.00995935]
 ...
 [ 0.00138889 -0.01002681  0.01055484 ...  0.02036052  0.01624006
   0.00083264]
 [ 0.01090465 -0.01242973  0.03208996 ...  0.02544165  0.00877456
   0.01358589]
 [-0.00424145 -0.01657562  0.03363728 ...  0.00778918  0.01818105
   0.01592274]]
layers.1.weight: [[ 0.04769336 -0.0183804   0.02940894 ...  0.0276131  -0.03523707
  -0.04105088]
 [-0.04345271  0.04526706 -0.04866251 ...  0.00482855  0.01635932
  -0.01888151]
 [ 0.02684277 -0.02748303  0.03633075 ...  0.01200719 -0.04956331
   0.03841915]
 ...
 [ 0.04097215 -0.04107306  0.00353694 ... -0.01803813  0.04706362
   0.03094729]
 [-0.01220037 -0.03632058  0.03731901 ... -0.0464308  -0.00657747
  -0.0227202 ]
 [ 0.037119    0.02716758  0.03215969 ...  0.03419732  0.02989857
  -0.02239382]]
layers.2.weight: [[-0.00387443 -0.00031934 -0.00618633  0.00313184  0.00340651 -0.01561787
  -0.01280606  0.01204178  0.0055489   0.00489435  0.00828588 -0.00636434
  -0.00308523 -0.00625346  0.00453453  0.01077384  0.01361576  0.01090731
   0.00912885 -0.00145145  0.00367536  0.0037024   0.00839572  0.0114917
  -0.0039496  -0.0005738   0.00130393  0.00887933 -0.00435354 -0.00657537
   0.00241358 -0.00454348 -0.00421143 -0.00854433 -0.00074115  0.00122631
   0.00043161  0.00879269  0.00599067  0.00261105  0.00340915  0.00469711
   0.00879226  0.00395745 -0.0115821  -0.01184125  0.00670632 -0.00330231
   0.00279114 -0.00088276  0.00276113 -0.01834335 -0.0019855  -0.01282945
   0.0017228  -0.00950545 -0.0117511   0.00018135 -0.00420589 -0.00317999
   0.00648846 -0.0053986   0.00837453  0.00630993 -0.004274   -0.00403822
   0.00039759 -0.00302585  0.00631594  0.01511463 -0.00055234  0.00303541
  -0.01098025 -0.00029356  0.01516062 -0.01482334 -0.00309296 -0.00994321
  -0.01208605  0.00058868 -0.00024531  0.00236095  0.00848667  0.00118946
  -0.00155416  0.01292579 -0.00906292  0.00497928 -0.00708514 -0.00785029
  -0.0237217  -0.01782429 -0.0067782   0.00691528 -0.00325797 -0.00234222
  -0.00713694  0.00228429  0.00952729  0.01258491  0.00635653 -0.00402116
   0.023598    0.00180732  0.00662111  0.00954222  0.01057795  0.01147866
  -0.00590251  0.00400276 -0.00527099 -0.00434634  0.00292265 -0.00515182
   0.00332112 -0.00988143  0.00910435  0.00263324 -0.00101093 -0.00061763
  -0.00332938  0.00225384 -0.00966646  0.00314936 -0.00026785 -0.01234614
   0.00158778  0.00421809]]

Final Loss: 0.2491
Distance Metric: 4.9173
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 311

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.03513793 -0.00187091 -0.04037275 ...  0.00357489 -0.02284205
  -0.00603254]
 [ 0.00871334 -0.03646034  0.01680051 ... -0.01093943  0.00032309
   0.01070653]
 [ 0.01113871 -0.00338729  0.00596282 ... -0.02256768 -0.01121186
  -0.0263616 ]
 ...
 [ 0.00563158 -0.02654465  0.03004719 ... -0.01036671  0.03672152
   0.03217513]
 [ 0.0341575   0.00486736 -0.03422935 ...  0.00632296  0.0409891
   0.01034104]
 [ 0.03661813  0.03898827 -0.01424755 ... -0.02302108 -0.00881988
   0.00383359]]
layers.1.weight: [[-0.00851797  0.02034909  0.01642497 ...  0.0172076  -0.02890194
   0.00659491]
 [ 0.02229475  0.02265818 -0.00901749 ...  0.01820689 -0.02800249
   0.01661285]
 [ 0.00372827  0.00863675 -0.00521924 ... -0.00673045 -0.00418942
  -0.02846262]
 ...
 [ 0.0068709   0.00390625  0.01004277 ...  0.02976455  0.02190338
   0.02335639]
 [ 0.03151057  0.03174975  0.01866397 ...  0.03102751 -0.01256131
   0.02144444]
 [ 0.01010277  0.02040512 -0.02030079 ...  0.01562359 -0.02708217
  -0.00825851]]
layers.2.weight: [[-0.11290672 -0.1241179  -0.11656145 -0.125      -0.15200123 -0.08361814
  -0.11467122 -0.12853913 -0.07815296 -0.1246336  -0.09638765 -0.14034767
  -0.14436129 -0.1248605  -0.125      -0.09055664 -0.10724404 -0.15043876
  -0.125      -0.125      -0.159143   -0.14154574 -0.11839289 -0.11699677
  -0.10134494 -0.14241388 -0.10359971 -0.125      -0.13397369 -0.14916293
  -0.108558   -0.12401976 -0.125      -0.125      -0.16223092 -0.10753599
  -0.13716707 -0.125      -0.11128785 -0.125      -0.125      -0.125
  -0.12186887 -0.15707801 -0.10722633 -0.09794443 -0.125      -0.11024078
  -0.14938235 -0.13667606 -0.11183486 -0.13470376 -0.10052015 -0.09876647
  -0.125      -0.14708965 -0.09288021 -0.125      -0.09539611 -0.12409948
  -0.10092535 -0.09988404 -0.125      -0.14019768 -0.15207511 -0.09018712
  -0.08983723 -0.10922081 -0.14798097 -0.13848852 -0.15445255 -0.10954814
  -0.15750667 -0.09606134 -0.13782802 -0.12471495 -0.10508106 -0.09345603
  -0.15052775 -0.14013337 -0.08761385 -0.10291334 -0.09043785 -0.11061525
  -0.06932201 -0.125      -0.13124384 -0.12103854 -0.10823028 -0.1121899
  -0.15821275 -0.16003588 -0.1659272  -0.13807221 -0.09566285 -0.09964596
  -0.11141052 -0.1264204  -0.10713141 -0.15766877 -0.1360172  -0.10831641
  -0.125      -0.12724878 -0.125      -0.14634636 -0.14418992 -0.12394108
  -0.08797123 -0.13386461 -0.13387331 -0.15112185 -0.08519789 -0.1077649
  -0.08174656 -0.14509256 -0.15341917 -0.14822723 -0.09095093 -0.11824472
  -0.14981556 -0.15615574 -0.125      -0.11327294 -0.12446976 -0.16413093
  -0.15336238 -0.09888794]]

Final Loss: 0.0000
Distance Metric: 5.3103
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.0029765  -0.00176038 -0.0332326  ...  0.00063497 -0.02652684
  -0.00867087]
 [-0.01282117 -0.01888679 -0.02635175 ... -0.02745465  0.02862448
  -0.00832658]
 [ 0.00412533 -0.03597764  0.01357624 ...  0.02894126 -0.03174357
  -0.02167983]
 ...
 [-0.0096438  -0.01334241  0.04594668 ...  0.03318136  0.0071695
  -0.02800032]
 [-0.00980602 -0.04333796  0.01348053 ... -0.01133217 -0.01021171
   0.03428409]
 [ 0.03956898  0.06842031 -0.00409546 ... -0.00236096 -0.03142406
   0.05275505]]
layers.1.weight: [[-0.00874205 -0.01518667  0.00616286 ... -0.01610154 -0.0070322
   0.00027634]
 [ 0.01618943  0.00917998  0.03088787 ...  0.0242951   0.02052448
   0.0041684 ]
 [ 0.01876272 -0.01467699  0.04187583 ... -0.00864976 -0.03544535
   0.00947565]
 ...
 [ 0.03750096 -0.00116654 -0.03670653 ...  0.06136647  0.04694022
   0.01570344]
 [ 0.00806821  0.00263426 -0.03337517 ...  0.00207636 -0.00091153
   0.03841479]
 [ 0.00409905  0.02135895 -0.00098378 ...  0.0235472  -0.02041046
  -0.00208167]]
layers.2.weight: [[-0.10754657 -0.40431276  0.31790832  0.00714269 -0.17146726 -0.23472437
   0.2598144   0.48443848 -0.47817612 -0.07325318  0.30994138  0.09814928
  -0.08696101 -0.36169145  0.05460848 -0.02511668  0.10553288 -0.2536699
   0.5618808   0.12714137 -0.171934   -0.445957    0.316473    0.37127447
   0.08339529  0.37778237 -0.4500474   0.2927236  -0.01738048 -0.16670455
   0.11606348  0.15432896  0.2058915   0.41604012  0.29575732 -0.16092171
   0.20718291 -0.30469796 -0.10186309 -0.09029341 -0.10815934  0.6432664
   0.18791893  0.19150054  0.02858352  0.01700859  0.50035244  0.32750633
   0.01819269 -0.03978952  0.02510328 -0.08855114 -0.34804425  0.09122994
   0.0053302  -0.1959558  -0.05519672  0.24254262 -0.23474535 -0.08566125
  -0.52139205  0.39254576  0.2572532   0.06034394 -0.53263754 -0.24161112
   0.15331498 -0.157839    0.44476944 -0.12234935 -0.6580377   0.1616636
  -0.24145602 -0.62413496  0.01336983  0.13915053  0.09038758 -0.2241633
   0.20628692 -0.1286452  -0.065      -0.30460516 -0.15652534 -0.02976445
  -0.02904031  0.1315238  -0.39906162  0.29738525  0.17745991 -0.10213427
  -0.08642893 -0.21013974 -0.60804987 -0.0074532   0.14774008 -0.05569746
  -0.58628327 -0.59453464  0.32600585  0.14728917 -0.27113196  0.42260575
  -0.6294069   0.28946528  0.14160533  0.3578299   0.3513211  -0.04932363
   0.14651346 -0.13686314 -0.15514584 -0.17856151 -0.12102845  0.20287643
   0.0138964  -0.13133256 -0.16365685 -0.19224063  0.1466634   0.25873938
   0.05892518 -0.18433711  0.18785198 -0.4306625  -0.06912886  0.25686783
  -0.05893593  0.17527786]]

Final Loss: 0.0000
Distance Metric: 7.4397
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.02596656 -0.06906564  0.03343365 ... -0.00408232  0.0137381
   0.02495301]
 [ 0.01118927  0.04880545  0.05051712 ...  0.06486106  0.06134328
  -0.00128782]
 [-0.01332266  0.02300952 -0.04592853 ...  0.06806715  0.00972729
  -0.02084997]
 ...
 [ 0.05238203  0.0532392  -0.03458274 ...  0.01546057 -0.04573543
   0.00916115]
 [ 0.04935287  0.03625619 -0.01402329 ...  0.04277462 -0.01081137
   0.01183597]
 [ 0.03006057  0.06366096  0.01145296 ...  0.07046652  0.03090497
  -0.02653458]]
layers.1.weight: [[-0.00191261 -0.03069437 -0.04179478 ...  0.0259566   0.02053341
  -0.00034823]
 [-0.00395607  0.01934261  0.00196027 ... -0.01089082 -0.01948633
  -0.00129375]
 [-0.01466483  0.00062018  0.0508166  ...  0.03528428  0.01091782
  -0.01478255]
 ...
 [ 0.03096368 -0.04397582  0.02008811 ... -0.01562482 -0.03217664
  -0.04077654]
 [-0.04574858 -0.0442982   0.02885586 ...  0.         -0.00050251
  -0.01554899]
 [-0.00435966 -0.00728752  0.0357125  ... -0.04377647  0.02170466
   0.01035673]]
layers.2.weight: [[ 0.02471792  0.02295125  0.00998937 -0.05218221 -0.03432376  0.03153233
  -0.02479246  0.03438061  0.03146596 -0.00579176 -0.04943703 -0.02564032
   0.00969076  0.02722465 -0.05943255  0.04604338 -0.0543425   0.02525981
   0.07003026 -0.00208395 -0.04182738  0.02318194  0.00787893  0.0120761
  -0.05575585 -0.00990793  0.04557213 -0.04550886  0.00982928  0.0147114
  -0.0626972  -0.05062462 -0.02809361  0.05349691  0.00055161 -0.01448195
  -0.03533149 -0.0211181   0.03887269  0.00201739  0.03917898 -0.03007014
  -0.04258062 -0.01094669 -0.06040192  0.00211227 -0.02661213 -0.033446
  -0.05992666 -0.01889372 -0.01089055 -0.04613109  0.05221744  0.01548074
  -0.0290563  -0.03420359  0.05163451 -0.03799259  0.03452456  0.01728432
   0.01677063 -0.04392391 -0.0297176  -0.01767179  0.04474868  0.0605475
  -0.03586814 -0.02623893  0.03386408 -0.02468142  0.05999349 -0.03095411
   0.01903548  0.03442223 -0.05530351 -0.03150176 -0.05295671  0.00553455
  -0.00011059  0.04452137 -0.04705745  0.014205   -0.0362425  -0.03967117
   0.03632835 -0.05524596  0.02453455 -0.01064439 -0.02705935  0.00279214
   0.00878194  0.02281158  0.01809662  0.0623427  -0.00132605  0.0487232
   0.048572    0.05645529  0.0350181  -0.04795247  0.00403628  0.03149876
   0.04087647  0.00308425 -0.00647283 -0.00741963 -0.03623364  0.05380072
  -0.02071354  0.05993144  0.01698404  0.06079069 -0.04601384 -0.01292844
  -0.03732664  0.02577746 -0.03826053  0.         -0.04383234 -0.00882731
  -0.01440102  0.03808664 -0.04580322  0.02364239 -0.00463687 -0.00630303
   0.04767519 -0.04372023]]

Final Loss: 0.0000
Distance Metric: 6.1913
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.3141703  -0.34802413  0.1278925  ... -0.404435   -0.4648662
   0.18024321]
 [-0.00714814  0.03037974 -0.01657175 ...  0.02865064  0.0616731
   0.01830893]
 [-0.08628517 -0.15956385  0.062916   ... -0.13249193 -0.12771529
   0.02679145]
 ...
 [-0.01575078 -0.04236535  0.00199996 ...  0.01227783  0.01151338
  -0.01334436]
 [ 0.09284535  0.0911962  -0.02170428 ...  0.15673958  0.1701529
  -0.06440993]
 [-0.17008398 -0.19739299  0.09215628 ... -0.14602637 -0.14758837
   0.07354595]]
layers.1.weight: [[ 0.10893391 -0.04496889  0.02705264 ... -0.0042397  -0.09396966
  -0.00242842]
 [-0.00694737  0.02439387 -0.02352131 ...  0.00127413 -0.00251016
  -0.00259616]
 [ 0.05494444 -0.04399576  0.02620125 ... -0.00525319 -0.06385236
   0.03136593]
 ...
 [ 0.39465922 -0.10055201  0.12275379 ... -0.02638482 -0.19092731
   0.18475784]
 [ 0.01801412 -0.02923464  0.02762854 ... -0.02582709 -0.01381663
  -0.00108394]
 [ 0.06456602 -0.00836615  0.02422361 ...  0.01037204 -0.02110155
  -0.00095098]]
layers.2.weight: [[-0.42433098 -0.14389156 -0.2383861   0.00363383  0.05862298 -0.6974243
  -0.77782184 -0.00280194 -0.13031575  0.02251807  0.09303339 -0.12601997
  -0.60434616 -0.20207556 -0.04440314 -0.05302387  0.0720712  -0.05890197
   0.06794404 -0.14665596 -0.2991355  -0.11479057  0.06168648 -0.26114896
   0.02860196 -0.20452735  0.07944399  0.13757677 -0.1286049  -0.08317275
  -0.0959494  -0.12207738  0.08753866 -0.15087359 -0.24416026  0.07525305
  -0.10921267 -0.08811108 -0.11178911 -0.9405725  -0.13101928 -0.6225312
   0.09294523 -0.08862759 -0.15069763 -0.27466565 -0.10058661  0.08817509
  -0.29007638  0.07660656 -0.550298   -0.2696105  -0.12600674 -1.3303517
  -0.04578511 -0.00696384 -0.09251463 -0.30114043 -0.16348673 -0.17677669
  -0.32126793  0.03073938 -0.07477304 -0.12600563 -0.19486919  0.00338172
  -0.39528844 -0.07756285  0.12665194  0.00339597 -0.5408395  -0.35279128
  -0.15976068  0.02666651  0.26902062 -0.94979286 -0.13141549  0.11321182
  -0.20256625  0.01936777 -0.14837444 -0.04433412 -0.086447   -0.37113884
  -0.12082467 -0.63162446  0.01178868 -0.5815639  -0.0210158  -0.40807527
  -0.15793398 -0.22947486 -0.16935572  0.02202087 -0.21633163 -0.08631834
  -0.13563344 -0.0493461  -0.12495825 -0.03083304  0.06405629  0.01114554
   0.05426594 -0.1591896   0.08195527  0.03490285  0.14707665 -0.01622234
  -0.55455315 -0.12574714 -0.00618809 -0.06703851 -0.13450356 -0.06138629
  -0.11974547 -0.30481088 -0.03332053 -0.39259678  0.11953536 -0.05519695
   0.03443494 -0.28526732  0.01812256 -0.2570667  -0.02497248 -1.7661164
  -0.1422965  -0.26843563]]

Final Loss: 0.0014
Distance Metric: 16.0563
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1999

================================================================================

