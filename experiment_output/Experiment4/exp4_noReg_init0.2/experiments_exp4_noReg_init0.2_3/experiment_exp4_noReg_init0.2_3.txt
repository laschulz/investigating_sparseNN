Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.1813058   0.16882463 -0.07542441]]]
layers.1.weight: [[[0.18718158 0.01524857]]]
layers.2.weight: [[[0.12226782 0.17832951]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.01080883 -0.03090492 -0.01109212 ... -0.00637548 -0.03215878
  -0.01509057]
 [ 0.0226345   0.01063755  0.03821049 ...  0.00817865 -0.01076595
   0.01700111]
 [ 0.0333466  -0.01333185 -0.02401242 ...  0.0225124   0.0064572
   0.        ]
 ...
 [-0.02579558 -0.03459605 -0.03872808 ...  0.01200738  0.03384607
  -0.013732  ]
 [-0.03666952  0.02138545  0.00429009 ... -0.01675603 -0.03309838
  -0.0380114 ]
 [-0.00894673 -0.01476068 -0.03790424 ...  0.03597033 -0.02945603
   0.00354934]]
layers.1.weight: [[ 0.01837605  0.0045301   0.0083978  ... -0.02662393  0.01217592
  -0.00998614]
 [-0.01384339 -0.02868595 -0.01023609 ...  0.03025774  0.00277588
   0.02775756]
 [-0.01449807 -0.00786444  0.03043229 ... -0.00256542  0.00918715
  -0.01969814]
 ...
 [-0.02743279  0.02772104 -0.01735516 ... -0.01656968 -0.01128756
  -0.00187379]
 [-0.00414315  0.02313944  0.02296285 ...  0.01607039  0.00220521
  -0.02470669]
 [ 0.02042887 -0.01237371  0.01888531 ... -0.00775233  0.00231017
   0.0274992 ]]
layers.2.weight: [[-0.00301711  0.01082927  0.04142366  0.00276276 -0.01874835 -0.01546895
   0.03059639 -0.01929862  0.02174142  0.04302043 -0.01249827 -0.01056606
  -0.00313365  0.04285919 -0.02286364 -0.02696573 -0.03747765 -0.01132398
  -0.03579131 -0.02745929 -0.03636461 -0.04238908  0.04040557 -0.02142977
   0.02176608  0.0256458   0.04006993 -0.01058594 -0.04256026 -0.01133747
   0.02901272  0.00969753  0.04303066  0.00170914 -0.02331488  0.02968336
   0.01929875 -0.00785293  0.00829894 -0.00179745  0.01757493  0.0333413
   0.0114421  -0.03583467 -0.00542525 -0.02693267  0.04748243  0.02921341
  -0.02331341  0.03810328 -0.02019952 -0.02346141 -0.01006598 -0.03982656
   0.00482203  0.03977827  0.02185624 -0.01120276  0.0110264  -0.03305307
   0.02689618  0.03633111  0.01541479 -0.00331593  0.00263172  0.0166196
   0.01562257  0.0521076  -0.00536349 -0.03925329 -0.03002958  0.03497421
  -0.0232354  -0.02451215 -0.00840624  0.00644176 -0.00040296 -0.01992108
  -0.02759144  0.02477651  0.03092336  0.00301224  0.0159267   0.03853194
  -0.02156773  0.01402904 -0.03933297  0.03985923  0.00025015  0.00090172
   0.02218006  0.00395701 -0.00968519  0.00198278  0.02973979  0.01799204
   0.01430058 -0.00709167 -0.02787567 -0.03387496  0.03015207 -0.03997164
  -0.028092   -0.00661543  0.01464199 -0.01124758 -0.0069176   0.02614298
   0.00728989 -0.0199439  -0.01120162  0.0240101  -0.03117507  0.02346702
  -0.02946027  0.00853265 -0.00021554  0.00562492 -0.00173044 -0.00660653
   0.01220334  0.00951296  0.04350085  0.02031897 -0.00543761  0.02538536
   0.01589205  0.04757083]]

Final Loss: 0.0000
Distance Metric: 3.9625
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.18283994 -0.04680055 -0.2941896  ... -0.02905693  0.1883542
   0.10934853]
 [ 0.20806734 -0.12707321 -0.2374136  ... -0.2097991  -0.21477586
  -0.19345793]
 [ 0.01910291  0.13829997  0.08577831 ...  0.22102228 -0.19814602
  -0.0018652 ]
 ...
 [ 0.07909285  0.0650586  -0.29635453 ...  0.15483111  0.24820477
  -0.15698333]
 [-0.09367476 -0.03300061 -0.04735673 ...  0.11904698 -0.1663515
   0.24748552]
 [ 0.3457035   0.00283914 -0.16024244 ...  0.02666428 -0.02578377
  -0.20626809]]
layers.1.weight: [[ 0.00299216 -0.05046937  0.0447802  ...  0.10453139 -0.08331112
   0.20231184]
 [ 0.05794775  0.23121661  0.00378347 ...  0.1608668   0.19559133
  -0.31363872]
 [-0.02983702 -0.04788011  0.01100428 ... -0.27001762 -0.17481977
   0.04486666]
 ...
 [ 0.06562547  0.2212051  -0.16338304 ... -0.12059075  0.21210134
   0.5199671 ]
 [ 0.02268513 -0.02640959 -0.13314818 ...  0.12155414 -0.1317473
   0.35874423]
 [ 0.04781765 -0.3006889   0.26737046 ... -0.15965265  0.02241858
   0.08000832]]
layers.2.weight: [[ 0.08856511  0.1077124   0.10660511 -0.36213967  0.10169764  0.09840553
   0.09182641 -0.29648775  0.11194828 -0.17829877  0.11439489  0.09890425
  -0.10499275  0.11000993  0.10608894 -0.27945548  0.21555795 -0.2906461
  -0.11716069  0.1067676   0.10746539 -0.20740634 -0.17833354  0.0912111
  -0.20023459  0.08896311  0.11617192 -0.12729926 -0.10990101 -0.12521397
  -0.11137573  0.10248179 -0.95388675 -0.33479506  0.10662564  0.08873905
  -0.13005482 -0.14774239  0.08654024 -0.11525936  0.10512946 -0.18766895
  -0.38762933  0.11001666 -0.11585459  0.11045308  0.18248364  0.08610025
  -0.52813363 -0.11579987 -0.1873011  -0.17176381 -1.115504    0.18402801
  -0.17201078 -0.13938144  0.08662297  0.10420867 -0.12933345 -0.24640942
  -0.45992315  0.18773705  0.10875017  0.10116736  0.09595411  0.11448974
  -0.37859485 -0.3470238  -0.29328868  0.10798121 -0.11818442 -0.11993813
   0.10230431 -0.15743387 -0.13254686 -0.33941078 -0.16466187 -0.11905536
  -0.12127343 -0.10799484 -0.14015396  0.10693111 -0.12335512 -0.12996903
   0.09973592 -0.2736033   0.10055462  0.10662106  0.10061004  0.09654244
   0.09457891  0.09572021  0.11195507 -0.23452163 -0.23034206  0.09665956
   0.10166795  0.09403054  0.1110083  -0.152778   -0.15305106 -0.12195776
  -0.32227263 -0.11529385  0.13017327  0.10044353 -0.13395493  0.1183287
   0.09517054 -0.11011162  0.09361254 -0.21594834 -0.15551923 -0.11578113
   0.11868188 -0.10953225  0.13754584 -0.34837982  0.11528568 -0.16455057
  -0.13832247 -0.15297829  0.1114896  -0.18574692 -0.39911595 -0.4339162
   0.09578409 -0.13243477]]

Final Loss: 0.0087
Distance Metric: 30.4109
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.01064153 -0.02237068  0.00126119 ...  0.01256845  0.01563201
   0.00451851]
 [-0.0238695  -0.01373084  0.01172776 ...  0.00062514  0.03698202
  -0.01450079]
 [ 0.03561968  0.0143427  -0.01356047 ... -0.0420028  -0.0275964
  -0.02474948]
 ...
 [-0.0358976  -0.00701805  0.01071099 ...  0.01428239  0.01223588
   0.01241672]
 [-0.00332438 -0.02383013  0.01642566 ...  0.02560556  0.01493459
   0.02414731]
 [ 0.00774278 -0.01838345  0.01869137 ...  0.02426787  0.00225128
   0.02174413]]
layers.1.weight: [[ 0.02116953  0.0090385   0.03762029 ... -0.03801836  0.04623009
  -0.00963449]
 [ 0.0078829   0.02629715 -0.03478697 ... -0.01098061 -0.04966324
   0.03114583]
 [-0.04670938 -0.05044879  0.02654461 ... -0.03926979  0.01834131
  -0.03008525]
 ...
 [ 0.02215798  0.03363731 -0.02458154 ...  0.01380173 -0.00952091
   0.02184323]
 [ 0.01325521  0.01094021 -0.03979333 ...  0.04825636 -0.02159607
  -0.03288435]
 [-0.02938144 -0.03887514  0.0480969  ... -0.00467637  0.05708801
   0.0053917 ]]
layers.2.weight: [[-0.00527854 -0.00445805 -0.00223135  0.00106293 -0.00397552 -0.00226717
  -0.0091034   0.00179342 -0.00854633  0.01163269  0.         -0.00346801
   0.00335256  0.00278532 -0.00135105  0.00295378 -0.00211031  0.00133675
   0.0064422  -0.00357159 -0.00563385  0.00372585  0.00468633 -0.00248581
  -0.00487869  0.00865234  0.00355216  0.01015912  0.00207575 -0.00482898
   0.00242701  0.00012229  0.00909367 -0.01146577  0.00775083  0.0060535
  -0.00538448 -0.00277499  0.00610503 -0.00910312  0.00145445 -0.00159255
  -0.00411561  0.00027454  0.00070702 -0.00528762 -0.00576338 -0.0012781
   0.00259942 -0.00183508  0.00086516 -0.00687456 -0.0055436  -0.00280599
  -0.0018685   0.00583903  0.00270253  0.0114676  -0.00908029  0.01234054
  -0.00488254  0.00197233 -0.00591214 -0.01132888 -0.0054105   0.00230704
   0.00024998  0.00148024  0.00470948  0.01122449 -0.00796845 -0.00484323
  -0.00921879 -0.00796753  0.00263895 -0.00090086 -0.00011241 -0.00822507
   0.00235761 -0.00682119 -0.00511769  0.00418306  0.00737917 -0.00886096
  -0.00183878  0.00044445 -0.00651721  0.          0.00038021  0.0002678
  -0.00205332 -0.00401136 -0.0055195   0.00340455  0.00060599 -0.00373567
  -0.01306925  0.01036239  0.00077713  0.00024167  0.0030207  -0.00094754
  -0.00085484  0.00348489 -0.0111771  -0.00510596  0.00665305 -0.00226315
   0.00709589  0.00363925  0.01056474  0.00231421 -0.00517445  0.00093986
  -0.00310814  0.00648033  0.00388584 -0.00290641  0.0041632  -0.01793014
   0.00147997  0.00011334 -0.00243217  0.00932509 -0.00053346 -0.00023842
  -0.00999705 -0.00216998]]

Final Loss: 0.2782
Distance Metric: 4.7970
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 318

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.00282766  0.02604706 -0.03563789 ... -0.0010826  -0.01643342
  -0.02195654]
 [ 0.00469595  0.04038528  0.02880368 ...  0.04013553  0.01415219
  -0.01700804]
 [-0.01593924 -0.01279795  0.0205655  ... -0.00908592 -0.03897711
  -0.02572323]
 ...
 [ 0.02153249 -0.03068601  0.0191147  ... -0.00463246 -0.02643352
  -0.03996376]
 [-0.00569555 -0.03208422  0.02061871 ...  0.01698955  0.04056401
   0.03948544]
 [ 0.01827006 -0.01693591 -0.02803531 ... -0.03216601  0.02625671
   0.03612178]]
layers.1.weight: [[ 0.         -0.02477827 -0.02645706 ...  0.02990614  0.01261907
  -0.00732377]
 [ 0.01694763  0.0041021   0.00469448 ... -0.01519242  0.01363964
  -0.02966708]
 [-0.02711376 -0.02569017  0.00170348 ...  0.0261785   0.01793207
  -0.01720656]
 ...
 [-0.0301515   0.01429927  0.02342448 ... -0.01681316  0.00682381
   0.02804565]
 [-0.01586296  0.01392587  0.00171554 ... -0.02018862  0.01215632
   0.00310221]
 [-0.02851697 -0.02232028 -0.00602335 ... -0.00257317  0.00037377
   0.02889526]]
layers.2.weight: [[-0.11377469 -0.10079448 -0.15524442 -0.1522764  -0.1514233  -0.1472976
  -0.0993944  -0.17401567 -0.16931364 -0.1407951  -0.125      -0.125
  -0.16225058 -0.12449373 -0.11883542 -0.10219971 -0.14049569 -0.12543185
  -0.1216423  -0.09274236 -0.10705271 -0.11584014 -0.10227248 -0.12787902
  -0.14150497 -0.1392971  -0.14088112 -0.14020444 -0.1016383  -0.13730808
  -0.08345008 -0.13676016 -0.125      -0.11899294 -0.09982219 -0.12475397
  -0.07973637 -0.125      -0.1088487  -0.13453078 -0.16138713 -0.12297782
  -0.08891138 -0.07605553 -0.11059956 -0.10195599 -0.08222563 -0.16761908
  -0.11532054 -0.11877757 -0.125      -0.125      -0.11413742 -0.125
  -0.11485659 -0.15635458 -0.11142269 -0.09509135 -0.13025495 -0.15411331
  -0.13457523 -0.0816158  -0.09506286 -0.16442648 -0.12227281 -0.1438821
  -0.15367755 -0.10276131 -0.1620376  -0.09298731 -0.10970709 -0.17699201
  -0.08283379 -0.0931315  -0.11299471 -0.11113663 -0.14598233 -0.10687345
  -0.07393952 -0.13346875 -0.12110451 -0.13667858 -0.11061656 -0.15360534
  -0.11688434 -0.09167928 -0.11204948 -0.11306304 -0.10780062 -0.08163634
  -0.14971088 -0.14388093 -0.125      -0.11872442 -0.10585081 -0.14119576
  -0.08043643 -0.10279321 -0.11376719 -0.13385926 -0.16829677 -0.07004971
  -0.125      -0.125      -0.125      -0.12383975 -0.13350053 -0.11902104
  -0.14158785 -0.125      -0.08301028 -0.15440486 -0.1588751  -0.15058246
  -0.10801108 -0.125      -0.15022972 -0.125      -0.14875431 -0.13866228
  -0.10661128 -0.08780287 -0.12663364 -0.12170973 -0.125      -0.08735408
  -0.13616024 -0.12213288]]

Final Loss: 0.0000
Distance Metric: 5.1480
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.04705704  0.00716276  0.00871623 ...  0.00179814 -0.00692017
  -0.00822788]
 [ 0.0567996   0.00211852  0.01703566 ...  0.00909439  0.00860002
  -0.00017783]
 [ 0.01641175  0.00891391  0.         ... -0.02093509  0.00828946
   0.00410496]
 ...
 [-0.03179462  0.02445021 -0.00844851 ... -0.01343781  0.00596959
   0.00884871]
 [-0.03347997 -0.0199501   0.00125193 ...  0.01442381  0.02154642
   0.01157738]
 [-0.01081826  0.00642801 -0.00993142 ...  0.01407901 -0.00123879
   0.00864997]]
layers.1.weight: [[ 0.00132765 -0.00804724 -0.00288356 ... -0.00304145  0.02505743
   0.02217085]
 [ 0.01369552 -0.01244769  0.01478839 ... -0.00138479  0.02164225
   0.01167441]
 [ 0.0424823   0.01320651 -0.02361052 ...  0.01423361 -0.00010826
  -0.02773328]
 ...
 [ 0.00919552 -0.01150861  0.024428   ...  0.02318682 -0.03401406
  -0.01039039]
 [ 0.00986587 -0.015362    0.0349993  ... -0.00500444  0.00857488
   0.00782553]
 [ 0.02046044 -0.02778088  0.00679448 ...  0.0142158  -0.02923242
  -0.00119332]]
layers.2.weight: [[ 0.08445912  0.1222172   0.11649294  0.11992221  0.01413769 -0.15005812
  -0.31481197  0.05809426 -0.49105814  0.01369901  0.2898135   0.6813953
   0.0493521  -0.03641195 -0.31477222 -0.2299551   0.08174337 -0.24135071
   0.46149233  0.67327476  0.10280947  0.05328088  0.05372654 -0.2624104
  -0.19912434 -0.09850195 -0.02037594  0.1898571   0.31129938  0.35970166
   0.04584575  0.10100948 -0.02333813 -0.40872857 -0.90908664  0.3681668
  -0.27182072 -0.00186537 -0.17936835 -0.3023135   0.3366253  -0.18828185
   0.33619803 -0.04210759  0.16395688 -0.0770836  -0.09005085  0.37415996
  -0.03236114  0.34694132  0.34374166 -0.02125742 -0.11195494  0.2512352
   0.18065195  0.7105125   0.23263867 -0.11292475 -0.12935561 -0.38238335
  -0.13842624  0.01356418  0.12756121  0.02635999 -0.47486448 -0.24266902
   0.36083236 -0.20234752 -0.5496727  -0.07724308 -0.3162048  -0.18278342
   0.18665771 -0.20742764  0.16402572  0.28153515 -0.39225593  0.4042314
  -0.1375337   0.22058474 -0.11756994  0.12966894  0.24472743 -0.08243661
   0.36560732  0.49055037 -0.01878251  0.03466799 -0.14953634  0.16309059
   0.11400153 -0.0486496   0.11626073 -0.07021837  0.13099444  0.12749591
   0.0418017   0.5646845  -0.10267486  0.15321758 -0.20894161 -0.09265003
   0.03131477 -0.18317202  0.07717097  0.1391809   0.28854564 -0.22508317
  -0.2893339  -0.21797383  0.43114093 -0.12859033 -0.28300336 -0.00735309
   0.24649993  0.5086922  -0.37671998  0.10819124 -0.13416386 -0.09109642
   0.3172956   0.00231362  0.34096012 -0.03916397  0.02059844  0.0793545
  -0.34214905 -0.36839816]]

Final Loss: 0.0000
Distance Metric: 7.2841
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.03640117 -0.01145121 -0.01652918 ... -0.06823588  0.05677119
  -0.06673541]
 [-0.29312938 -0.02425115 -0.0621343  ...  0.00661804  0.00780577
   0.01721378]
 [-0.06121517 -0.02512096 -0.02460681 ...  0.00111629  0.03268956
  -0.01031502]
 ...
 [ 0.04759831 -0.028477    0.01716588 ... -0.06893127  0.00773532
   0.04562804]
 [ 0.05853242  0.00760945  0.04466245 ... -0.02628161  0.05762276
  -0.04595134]
 [ 0.03531014 -0.0070581   0.00842691 ... -0.05862385  0.00463846
  -0.0284631 ]]
layers.1.weight: [[-0.02996538 -0.04783375 -0.04585743 ...  0.04934689 -0.03599151
  -0.01108348]
 [-0.03050078 -0.02106736 -0.03693193 ...  0.0446626  -0.00493559
  -0.02806152]
 [ 0.03320471  0.01175546  0.00305482 ... -0.04591641  0.00330855
   0.01277102]
 ...
 [ 0.02400625 -0.01780929  0.00394063 ... -0.01768687  0.019227
   0.04292065]
 [ 0.03763488 -0.01917388  0.00524618 ...  0.00595037 -0.01113114
  -0.02383078]
 [-0.00791127  0.03373272 -0.03429052 ... -0.04828077  0.0156872
  -0.05099923]]
layers.2.weight: [[ 0.03523894 -0.05790509  0.03495813  0.05152338 -0.04993081 -0.03603128
   0.01934508 -0.01229517  0.0449294  -0.04895526 -0.03950601 -0.0389987
  -0.02676894 -0.006327    0.01009589  0.03496845 -0.0264393  -0.03086642
   0.02269352  0.0611702   0.01965803  0.01803775 -0.0682129  -0.0045103
   0.02927576  0.00026909  0.054564   -0.05054101 -0.03360771 -0.03718941
  -0.03933222  0.00062602 -0.01488267  0.02089366  0.0748356  -0.00488107
   0.01506789 -0.0327929   0.00222218  0.03601304 -0.03779937 -0.01067832
  -0.01016264  0.06552871 -0.01021672  0.02775855  0.01450477 -0.02420216
  -0.01988914  0.02164226  0.04428498  0.00500412 -0.0444773  -0.01741842
  -0.01071483  0.06726077 -0.02907078  0.04302637  0.03386613  0.01808958
   0.00351912 -0.06152053 -0.04600275 -0.04059655  0.04359996 -0.01123834
   0.0217638   0.00839097  0.05554975  0.00032427  0.01500565  0.00409949
  -0.00125196  0.00347897 -0.03085816 -0.02989789  0.01976993 -0.00982089
   0.00561096 -0.02367812 -0.03290883 -0.02712994 -0.00182924 -0.06084114
   0.02232633  0.03882021 -0.07851038 -0.04953006  0.00535423 -0.05559427
  -0.06009751 -0.00809318 -0.04350393  0.06749817 -0.0295951  -0.02898357
  -0.05789607  0.01588495 -0.0010494  -0.0258697   0.00222072 -0.00413693
   0.01555668 -0.03226758 -0.02278736 -0.04910411 -0.01092143  0.04917798
   0.01498635  0.0230552  -0.01668147  0.01080785  0.03679679 -0.05778893
   0.0137002   0.02110199  0.00430895 -0.04986428  0.00992878  0.00146795
  -0.00867236 -0.01176389 -0.04429037  0.07853217 -0.00154725 -0.04059016
   0.01745858  0.01728731]]

Final Loss: 0.0000
Distance Metric: 6.3684
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00363482 -0.03449489  0.00019014 ...  0.0198435  -0.00712557
  -0.01074154]
 [-0.016756   -0.01402166  0.02068632 ... -0.01668405  0.03757983
  -0.03065096]
 [ 0.03432486 -0.00788865 -0.02348579 ... -0.04137933 -0.01969175
  -0.01691049]
 ...
 [-0.0285495   0.00758607  0.02117317 ...  0.01361824 -0.02051404
   0.00661077]
 [-0.01219951 -0.02648669  0.02503728 ...  0.02841039  0.00756687
   0.03360792]
 [ 0.02023747 -0.0276768   0.00283129 ...  0.02285902 -0.00365137
   0.00350657]]
layers.1.weight: [[ 0.01716147  0.0059091   0.03087714 ... -0.02720527  0.02706249
  -0.00290729]
 [ 0.0097698   0.02296066 -0.01793574 ...  0.00034244 -0.02909093
   0.02623301]
 [-0.02445145 -0.02815452  0.01993331 ... -0.02517176  0.01700343
  -0.01655885]
 ...
 [ 0.01603973  0.02210959 -0.01245323 ...  0.00895285 -0.00668572
   0.01549412]
 [ 0.00503178  0.0060767  -0.01843685 ...  0.02905338 -0.01518297
  -0.02260111]
 [-0.0248248  -0.0280424   0.03139134 ... -0.00437787  0.0291564
   0.00628968]]
layers.2.weight: [[-0.0966524  -0.1556657  -0.10199781 -0.06830055 -0.12867096 -0.10776348
  -0.15510204 -0.11207691 -0.16527772 -0.08053852 -0.16784571 -0.12812819
  -0.05968426 -0.1534646  -0.15828754 -0.09633128 -0.1300872  -0.08896144
  -0.18790185 -0.13915563 -0.14722852 -0.16047658 -0.1256319  -0.10494275
  -0.08230542 -0.13292454 -0.13043347 -0.10333834 -0.07131077 -0.11773589
  -0.11561682 -0.1753603  -0.12858832 -0.09041452 -0.1272637  -0.06767587
  -0.16580595 -0.14354903 -0.13153754 -0.16815864 -0.11205925 -0.1257408
  -0.09675096 -0.07243248 -0.10316475 -0.11611436 -0.1518177  -0.1348514
  -0.09915969 -0.1372585  -0.10882711 -0.12940416 -0.08095019 -0.13983345
  -0.09414399 -0.12015248 -0.12933205 -0.1383502  -0.09761402 -0.11082056
  -0.11652068 -0.16555373 -0.13887896 -0.13768227 -0.0691139  -0.13737318
  -0.06767562 -0.09201483 -0.09649462 -0.10841466 -0.15432672 -0.15376006
  -0.16644791 -0.09205928 -0.10702549 -0.11266797 -0.11578669 -0.06293319
  -0.10320843 -0.11443838 -0.13432151 -0.11296267 -0.11292846 -0.1606171
  -0.13577406 -0.09765757 -0.1330856  -0.11243578 -0.1404395  -0.13830076
  -0.14224656 -0.12988576 -0.15608086 -0.09530353 -0.08918618 -0.1599755
  -0.16050567 -0.09429131 -0.16309842 -0.16922142 -0.10770172 -0.16642536
  -0.08637776 -0.14886941 -0.15070756 -0.15741539 -0.0757974  -0.1179196
  -0.1245889  -0.10746311 -0.13370441 -0.09617821 -0.14730522 -0.15160008
  -0.157153   -0.13974302 -0.08700811 -0.10420547 -0.10717188 -0.1506608
  -0.09561177 -0.10021513 -0.16550714 -0.09821986 -0.09689855 -0.11401951
  -0.14969105 -0.11731398]]

Final Loss: 0.0009
Distance Metric: 5.6429
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.02715247  0.00804558 -0.01228974 ...  0.00016015  0.00220432
   0.        ]
 [ 0.00896326  0.00855325 -0.02298462 ... -0.01079182 -0.02503745
  -0.00135427]
 [-0.00485292  0.01076239  0.00495695 ...  0.01198613 -0.00693282
  -0.01202743]
 ...
 [ 0.03064711 -0.00497184 -0.04485084 ... -0.00452086  0.00970941
  -0.0166457 ]
 [ 0.00454433  0.00210096 -0.00665076 ...  0.00323949  0.00290621
   0.03225898]
 [ 0.03123547  0.00146264 -0.02352832 ...  0.00537748  0.00399296
  -0.00717549]]
layers.1.weight: [[-0.00642715  0.00714217  0.06534115 ...  0.00589156 -0.01071022
   0.03513752]
 [-0.02653774  0.02671707  0.00388984 ...  0.00373391  0.0098576
  -0.04059638]
 [ 0.00645284  0.00806545 -0.02390344 ...  0.01027155 -0.05312477
   0.05815635]
 ...
 [-0.03012534 -0.01552949 -0.00120407 ... -0.03719858  0.02528727
  -0.01881145]
 [ 0.02958281  0.03005119 -0.00639112 ... -0.00784793  0.03065459
   0.0086226 ]
 [-0.00214899 -0.01167701  0.05651157 ...  0.00689488 -0.00048514
  -0.01135643]]
layers.2.weight: [[ 0.38516092  0.05418987  0.305819   -0.7406859   0.21076575  0.42938992
   0.03662547 -0.08522678 -0.00247496 -0.15185076  0.21164608  0.02249906
  -0.06867681  0.05956622  0.05548742 -0.38694888  0.0726494  -0.19376159
  -0.02434441 -0.00495168  0.16984448 -0.16203327 -0.30779028  0.38574973
  -0.22943895  0.44293508  0.02360759 -0.1760863  -0.0471653  -0.03220351
  -0.14943722  0.07954466 -0.26637593 -0.5026829   0.02502224  0.87151355
  -0.06479649 -0.18319021  0.1759973  -0.00200598  0.0927549  -0.06375439
  -0.35055265 -0.01423322 -0.03538714 -0.0123029   0.05563787  0.13549064
  -0.29877865 -0.12049159 -0.31142417 -0.24702546 -0.59538287  0.11106351
  -0.2121534  -0.10733017  0.37174448  0.18278264 -0.17759018 -0.16489212
  -0.5071974   0.26823902  0.40461537  0.34777227  0.69194007  0.06761926
  -0.6811422  -0.38879296 -0.4413941   0.35662025 -0.02516986 -0.10189603
   0.04057446 -0.17604612 -0.04852194 -0.51831406 -0.09107643 -0.13198517
  -0.03445551 -0.02307541 -0.18720317  0.05447121 -0.07017059 -0.04884979
   0.19755217 -0.28153753  0.05280774  0.228785    0.19882175  0.2930683
   0.3597406   0.05667431  0.10545816 -0.24748069 -0.2227941   0.24974096
   0.03472291  0.36921215  0.0960326  -0.08253668 -0.3043031  -0.07537888
  -0.26277065 -0.01350104  0.12133063  0.13508758 -0.08144419  0.03475781
   0.19337763 -0.0319399   0.7303567  -0.21497989 -0.1805667  -0.06450754
   0.08328912 -0.0629679   0.18607953 -0.33744     0.30315635 -0.17600164
  -0.12449867 -0.13381582  0.00424621 -0.20122494 -0.5183728  -0.22195348
   0.29934713 -0.08855805]]

Final Loss: 0.0004
Distance Metric: 7.1779
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1999

================================================================================

