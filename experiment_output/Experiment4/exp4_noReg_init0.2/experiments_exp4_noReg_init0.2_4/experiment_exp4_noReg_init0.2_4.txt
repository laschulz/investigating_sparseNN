Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.1761496   0.12786046  0.04995842]]]
layers.1.weight: [[[-0.11127473 -0.22520292]]]
layers.2.weight: [[[0.1571855  0.21326886]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00793006  0.00391082 -0.01927163 ...  0.00350851  0.01398929
   0.01525001]
 [ 0.00114396  0.00053168  0.02194975 ...  0.0047528   0.03214555
   0.01852037]
 [-0.03610313 -0.02446907 -0.00781378 ... -0.03842812 -0.02579675
  -0.01252794]
 ...
 [ 0.04033734  0.00210878  0.03835332 ...  0.0405752  -0.03646636
  -0.01923246]
 [ 0.02197102 -0.01821337  0.00267821 ... -0.01994771 -0.01169765
  -0.00357529]
 [ 0.02503644 -0.02067965  0.00654317 ... -0.007823    0.02965656
  -0.01016637]]
layers.1.weight: [[-0.01888691  0.00742623  0.02142589 ... -0.01654462 -0.02769893
  -0.02574802]
 [ 0.00646063 -0.00735455  0.01236854 ... -0.02630304  0.00844196
  -0.01928328]
 [ 0.01966697  0.01778563 -0.01579494 ... -0.00766258  0.01516971
  -0.03070188]
 ...
 [ 0.01239032  0.02302168 -0.00568842 ...  0.0182493  -0.00540449
  -0.00754966]
 [ 0.02419426 -0.02708383 -0.00191912 ...  0.0064986  -0.01489014
   0.00759903]
 [ 0.02730404 -0.0193307  -0.02017371 ...  0.02020404  0.00728357
  -0.02933211]]
layers.2.weight: [[-0.01565217  0.00252513 -0.02931008 -0.02413739 -0.00466855 -0.02312217
  -0.03801269 -0.03170011 -0.03505518  0.01067935  0.03985544  0.00692278
   0.0396287   0.04599879 -0.02188956  0.00151884  0.03570227  0.02695928
  -0.03092964  0.01901187  0.03428964 -0.00971362 -0.00557077 -0.00639702
   0.0260787  -0.03397091  0.00036642 -0.00075973 -0.00971798 -0.02930331
  -0.02180959 -0.03103097  0.05475727  0.02520048 -0.02344962 -0.01551929
   0.04454829 -0.01574732  0.01990003 -0.02026402  0.0312645   0.03182883
   0.04604138  0.02469205  0.0199126  -0.01553142  0.01893613 -0.00096554
   0.03292963  0.03225464 -0.044245   -0.0116733  -0.01165905 -0.02204943
   0.00355569 -0.02966006 -0.02803163  0.00964525 -0.03163948  0.02608001
  -0.04139739  0.02055621  0.0310777  -0.02439762  0.02965327  0.04234961
  -0.0290548  -0.0193884   0.0220178   0.0323786   0.04566647 -0.02523256
  -0.00950408  0.04018021  0.00091519  0.03707556 -0.01214843  0.02499078
  -0.04371694  0.04044376 -0.02286715  0.01914701  0.0015863   0.05697001
   0.03294979  0.00195055 -0.02528069  0.03267726  0.0070469  -0.01042412
   0.02159111 -0.03053145 -0.0306378   0.00320961  0.01196733 -0.02432364
  -0.01354251  0.03341435 -0.00050013 -0.03366503 -0.04186602  0.02211929
  -0.03202208 -0.00322686 -0.00486911  0.04040543 -0.0451849   0.03074412
   0.03952831  0.00069276  0.04407156  0.04122524 -0.0261492   0.04174348
   0.00083747  0.05626277 -0.01904359  0.0207218  -0.04915549 -0.00084284
  -0.01600927  0.04254997  0.04746204 -0.0013561   0.02652032 -0.05338101
  -0.02403792 -0.03670251]]

Final Loss: 0.0000
Distance Metric: 4.0479
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.31002194  0.22888324  0.1960614  ...  0.19171864  0.08800996
  -0.08583534]
 [-0.07814644  0.02554177  0.36983335 ... -0.2077905   0.03971058
   0.14919956]
 [-0.27272922 -0.08567926 -0.25613993 ...  0.25834772 -0.17496975
   0.18038487]
 ...
 [ 0.03245969 -0.18762274 -0.1643811  ...  0.15779935 -0.12291343
   0.29182762]
 [ 0.35942683  0.00610483  0.14980456 ... -0.03928233 -0.05016452
   0.03107833]
 [ 0.1563524  -0.152475    0.07791339 ...  0.07560893 -0.13642737
  -0.34155506]]
layers.1.weight: [[-3.22714061e-01 -1.05990723e-01 -3.73962671e-01 ...  1.13358386e-01
  -4.74543907e-02  1.59251884e-01]
 [-1.12371221e-02 -8.44084620e-02 -5.77509403e-02 ... -2.05021333e-02
   1.63040280e-01 -9.15342718e-02]
 [ 1.72501117e-01 -6.47810400e-02  2.22195566e-01 ... -2.71018296e-01
  -2.15859938e-04  1.96011886e-01]
 ...
 [ 2.46052325e-01  0.00000000e+00 -1.09643355e-01 ... -1.91055357e-01
  -1.51930705e-01 -5.98281324e-02]
 [-2.72026420e-01 -1.09934844e-02  2.26407032e-02 ...  4.11978364e-01
   2.09942833e-02 -2.17606828e-01]
 [ 2.84482948e-02  1.11429602e-01  1.58465385e-01 ... -1.09277911e-01
   8.68610069e-02 -1.26149133e-01]]
layers.2.weight: [[-0.16811876 -0.28349096 -0.11126987  0.0885291   0.138224   -0.26008254
   0.10667228  0.10191059 -0.20834877 -0.09981421  0.12355618 -0.12134602
   0.08440454 -0.12273122 -0.10087934  0.11047416  0.09538195  0.10909435
   0.0998309   0.0882782   0.10612386 -0.378298   -0.22663215 -0.27233857
  -0.11850303 -0.19860178  0.09174705 -0.1062361  -0.70976484 -0.3115814
  -0.4296552  -0.37061414 -0.12618326 -0.09352255  0.09916487  0.0761013
  -0.12104242 -0.11038028 -0.14469849  0.084543    0.09233037 -0.11006064
   0.07666124  0.09066139  0.09044522 -0.5241177   0.11000619  0.11427497
  -0.12804489 -0.2868359  -0.17074397 -0.11621242  0.08544543 -0.17326395
  -0.27061763 -0.2761624  -0.18095623 -0.13434239 -0.18310729 -0.17658196
   0.11666547  0.09373883  0.08689581  0.08874478 -0.12536967  0.13695486
   0.09545521 -0.3263358  -0.11107677 -0.43660644 -0.11582041  0.09187258
  -0.4825086   0.11002624 -0.21659514 -0.3493379   0.08966129 -0.21443266
   0.0952884  -0.12631245  0.08468023 -0.27980962  0.12417012 -0.15427679
   0.07893622 -0.3062666  -0.5277327  -0.695725   -0.22783035 -0.26016843
   0.16684641  0.10664821  0.10466037  0.08686531 -0.24451415  0.09209345
   0.09578798  0.09527496 -0.11936895 -0.6113359  -0.2235536   0.08835226
   0.1168063   0.11529667 -0.11329749  0.09968484 -0.2915734  -0.17360567
  -0.44006103 -0.28051603 -0.13184673  0.09803236  0.08300725 -0.10324757
   0.08420253 -0.27918762  0.0737488  -0.33964822  0.10193897 -0.19767223
   0.11497811 -0.11442323  0.08333984 -0.1259555   0.0958204   0.08247388
   0.09055631 -0.11231177]]

Final Loss: 0.0075
Distance Metric: 29.4127
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.02186328  0.02350684  0.00634977 ... -0.02257486 -0.01918641
  -0.0046633 ]
 [ 0.04145984  0.01498185 -0.01220647 ...  0.02695843 -0.03765878
  -0.02212501]
 [ 0.01170898 -0.05438211  0.00461471 ... -0.00327836  0.00280861
   0.02508108]
 ...
 [-0.00817329  0.01911776  0.01187772 ...  0.01549993 -0.02524468
   0.00963337]
 [-0.02391996 -0.02072064 -0.01990454 ... -0.01996685 -0.00903943
  -0.00923952]
 [-0.02662882  0.007767   -0.0198504  ...  0.01365162  0.00290117
   0.02357316]]
layers.1.weight: [[ 0.01717384 -0.03362972  0.0258003  ...  0.02352477 -0.05085791
  -0.05345666]
 [-0.04458282  0.02977405 -0.00632785 ... -0.00071497  0.03660047
  -0.03910511]
 [-0.01995567  0.03052927  0.02305225 ... -0.02606922 -0.01348969
   0.02641032]
 ...
 [ 0.04801791 -0.02243219  0.02201727 ... -0.01118642  0.03452638
   0.0247117 ]
 [ 0.0175078  -0.02711903  0.04654568 ... -0.01592508  0.03183664
   0.00837983]
 [ 0.00023451  0.007225   -0.03331209 ... -0.03478508 -0.0329137
   0.04656493]]
layers.2.weight: [[-0.00416096  0.0049634   0.00250051  0.00381757  0.         -0.00101221
   0.00396764  0.00431708 -0.00472914  0.00505426 -0.00677218 -0.00147907
  -0.00054645  0.00385496  0.00469797  0.00477368  0.00461492 -0.00061315
   0.00044266 -0.00172719 -0.00067016  0.0063465   0.00234524 -0.01053724
  -0.00244586 -0.00306832 -0.00514884  0.00234532  0.00774635  0.00262012
   0.00159156  0.00242663  0.00011524 -0.00853214  0.00214     0.00233129
   0.00049221  0.0072541   0.00680749 -0.00667697 -0.00475128  0.01317586
   0.00082856  0.00307505 -0.00557819  0.00077594 -0.00350275  0.00110269
  -0.00481258 -0.00335011  0.          0.00391353  0.00205546  0.00364982
   0.00297256 -0.00022649  0.0022833  -0.00192906 -0.00675713  0.00925928
  -0.00435284 -0.00296684  0.0028161   0.00286428  0.00348472 -0.00330393
   0.01164538 -0.00189381 -0.00297423 -0.00543128 -0.00894495 -0.00089279
   0.00898495 -0.00204032 -0.00402969 -0.00614893  0.00169214 -0.00407569
  -0.0001566   0.00027839  0.00186684  0.00281712 -0.00607236 -0.0056108
  -0.00622562  0.00566775  0.00567888 -0.00980265 -0.00146765 -0.00014424
   0.00236539 -0.00462061 -0.00234914 -0.00354428 -0.00512979 -0.00100631
   0.00016491 -0.00144873  0.00107552  0.00606514 -0.00216493  0.00212886
  -0.00089848  0.00237226  0.00197456 -0.00159101  0.         -0.00873444
   0.00235437  0.00670525  0.          0.00434661  0.0040667  -0.00128851
   0.00011838 -0.00188127 -0.00364278  0.0048277  -0.00212199 -0.00656165
  -0.00194716 -0.0020785  -0.00257216 -0.01329752  0.00133036  0.00727327
  -0.00450839  0.00107482]]

Final Loss: 0.2437
Distance Metric: 4.9569
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 243

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.07045025 -0.00548652 -0.06210179 ...  0.03614796 -0.02750543
   0.0194224 ]
 [ 0.03749512 -0.00110735 -0.01506322 ... -0.03824329  0.03101678
   0.0369872 ]
 [-0.03133787  0.0081759   0.00035441 ... -0.0321449  -0.00487879
  -0.03892522]
 ...
 [ 0.05077245 -0.01219072  0.04238782 ... -0.03475481  0.0014203
   0.03943735]
 [ 0.03372413  0.02521518  0.00677875 ...  0.01645239  0.0314079
  -0.00317698]
 [-0.01704461  0.01025462 -0.01793022 ...  0.0158688  -0.0184454
   0.01425443]]
layers.1.weight: [[ 0.0086424   0.00669243  0.02172585 ...  0.02740943 -0.01492496
   0.00918903]
 [ 0.01351521  0.02140914  0.01847755 ... -0.00576094 -0.01556421
  -0.02410982]
 [-0.00767199 -0.01695524 -0.00768568 ... -0.0134445  -0.00290247
  -0.02766542]
 ...
 [-0.0023862  -0.00101484  0.00362312 ... -0.02917818 -0.00972809
   0.00430608]
 [-0.01637346 -0.0240451   0.01634982 ...  0.02510959  0.02105639
   0.00807853]
 [-0.00158739 -0.02051791  0.00567736 ... -0.01478098  0.00383551
  -0.01998025]]
layers.2.weight: [[-0.08384507 -0.07587029 -0.07713083 -0.1102925  -0.04718634 -0.07165989
  -0.11692622 -0.11303264 -0.08006818 -0.03844021 -0.12984489 -0.09439266
  -0.04654504 -0.09920853 -0.09005667 -0.06333965 -0.0374122  -0.07577369
  -0.12716687 -0.12736636 -0.05489201 -0.08925121 -0.05140385 -0.04402248
  -0.1238182  -0.11960912 -0.10330812 -0.0447372  -0.06087763 -0.12599376
  -0.02222746 -0.01912261 -0.06226053 -0.08635165 -0.12133723 -0.06032532
  -0.01583144 -0.09606231 -0.14345482 -0.05484192 -0.09097388 -0.09665436
  -0.04406055 -0.06418789 -0.09973843 -0.1223375  -0.11010779 -0.06411355
  -0.0901693  -0.11153307 -0.10516269 -0.14607336 -0.07803438 -0.1323614
  -0.03243037 -0.02139912 -0.11481055 -0.06280646 -0.07929832 -0.09555535
  -0.05910715 -0.15103315 -0.11556227 -0.09395809 -0.08312099 -0.0654574
  -0.11765805 -0.06296229 -0.04708456 -0.08369177 -0.09319299 -0.08463544
  -0.12474847 -0.14059398 -0.066843   -0.06275768 -0.13180614 -0.04037265
  -0.11446734 -0.05485895 -0.10322183 -0.12901077 -0.10044646 -0.06891956
  -0.03644337 -0.04629615 -0.10055882 -0.04865306 -0.04994413 -0.10885783
  -0.13028024 -0.06213998 -0.11342229 -0.09279057 -0.04869571 -0.09700473
  -0.09650847 -0.12735823 -0.11746821 -0.1069989  -0.05443946 -0.05582187
  -0.08663179 -0.10613196 -0.15090828 -0.03754505 -0.12344223 -0.06899182
  -0.06725508 -0.06429732 -0.11037516 -0.00975623 -0.03451253 -0.0282147
  -0.07380065 -0.07192565 -0.11196743 -0.12310487 -0.1320398  -0.04932982
  -0.07749467 -0.15242563 -0.0978433  -0.05051655 -0.06429157 -0.06521246
  -0.08341619 -0.11554712]]

Final Loss: 0.0001
Distance Metric: 5.3081
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.00199005  0.00801817  0.03117118 ... -0.00644666  0.00288456
  -0.02266045]
 [-0.02418204  0.00097088  0.02203628 ...  0.02535854  0.01079428
   0.01002505]
 [ 0.00143327 -0.02138449 -0.01656118 ... -0.02014053  0.00777117
  -0.05578942]
 ...
 [-0.0356564   0.02087377 -0.01262486 ...  0.01066226 -0.08213996
   0.01064811]
 [ 0.01586542  0.01824317  0.0119205  ...  0.01003235  0.00314589
  -0.0037703 ]
 [ 0.02398089  0.00699878 -0.01058005 ... -0.03901647  0.04702049
  -0.0178549 ]]
layers.1.weight: [[ 0.03722198  0.02163128 -0.0127451  ... -0.00426628  0.00161074
   0.00220826]
 [-0.02234293 -0.00680656 -0.02218167 ... -0.00619766  0.00902732
  -0.01009958]
 [-0.02382118 -0.01321159 -0.01699944 ... -0.00545181 -0.0186747
   0.00743074]
 ...
 [ 0.01572091 -0.00592943  0.05552309 ... -0.0264639   0.03145828
   0.04134759]
 [-0.0123664   0.03938426 -0.00777344 ... -0.04717411  0.00387911
   0.02218228]
 [-0.00368579  0.01180123  0.02084138 ...  0.00060015  0.03576184
  -0.02818064]]
layers.2.weight: [[-0.33242217 -0.22623663  0.29108217 -0.12066299  0.34203932  0.7063373
  -0.03722341  0.02820335 -0.08090094  0.01547379 -0.42581987 -0.0749072
  -0.03800382 -0.09609383  0.00695681 -0.16021176  0.3205741  -0.15149057
   0.42316273 -0.2545181  -0.3967726   0.250884   -0.37408653 -0.19205768
   0.36771795  0.02731011 -0.67749107  0.4861318   0.04926841  0.33240178
  -0.03767164  0.18343863 -0.3669156  -0.15393148  0.01920108 -0.1676772
   0.09512809  0.07430911  0.21258107 -0.22103022 -0.23413825  0.11980961
   0.06461836  0.16077495  0.41367012 -0.24374108  0.00965973 -0.23377214
   0.3676622  -0.34431872  0.37977284 -0.02378301  0.18724573  0.2086705
   0.29623622 -0.0973917   0.03179877 -0.17332451 -0.2523005   0.02908961
  -0.12245949  0.22430596  0.21458095 -0.41272718 -0.17944352  0.28789884
   0.25252616 -0.14184473 -0.10735058  0.2390608   0.60086     0.39201307
   0.09828208 -0.38292977  0.3125812   0.21687903 -0.34783795 -0.09650414
  -0.05709708 -0.2939976   0.38889766 -0.01925043  0.14519016  0.11458358
  -0.31054538  0.17089497  0.15621231  0.55692977  0.12067724  0.20916283
  -0.27858981 -0.06232269 -0.09815152  0.3409761   0.2851374  -0.0706389
   0.14873622 -0.27903292 -0.15685283 -0.10414486  0.54580915  0.0133203
   0.12715265  0.1705209   0.00466683 -0.36338896 -0.33871794  0.42537478
  -0.29855835 -0.5802825   0.11921839  0.1956248  -0.01007232 -0.02840521
   0.3253463   0.06135606 -0.19938338  0.07348886 -0.3642317  -0.15947385
   0.4054163   0.18810312 -0.09039469 -0.10710136 -0.1912671   0.13078871
  -0.11422293 -0.07188995]]

Final Loss: 0.0000
Distance Metric: 7.3746
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 895

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.03283282  0.00217879 -0.06027755 ...  0.01863605  0.
  -0.00429909]
 [ 0.05003998 -0.01075486 -0.06735392 ...  0.03542763 -0.0121783
   0.03437643]
 [-0.03069664  0.02529935 -0.00741638 ...  0.04811319 -0.05708526
  -0.01091771]
 ...
 [-0.01214227 -0.05253383 -0.04186278 ...  0.0135251  -0.00663721
   0.06218768]
 [-0.05718824  0.0600078  -0.02673345 ... -0.02703691  0.03729955
   0.00998307]
 [-0.00103999  0.00174548  0.02312942 ... -0.00164734  0.0023459
  -0.04668219]]
layers.1.weight: [[-0.04040109  0.03638514  0.03523072 ... -0.0433457  -0.04377557
  -0.01558277]
 [ 0.0204497  -0.03334818 -0.01819289 ... -0.02130312  0.00147929
   0.03924258]
 [-0.04185203 -0.02481284 -0.01939311 ... -0.00490816  0.03281946
   0.03612605]
 ...
 [ 0.0351603  -0.03724182  0.04312058 ... -0.03489265  0.0253318
   0.01139754]
 [ 0.04738255  0.03963304  0.01995185 ...  0.01161895  0.03742773
   0.03349215]
 [ 0.02172886 -0.02429045 -0.00362583 ... -0.02821008 -0.01700122
  -0.04429564]]
layers.2.weight: [[ 0.00214803 -0.00670087  0.04166139 -0.02079357 -0.05551781  0.04379956
   0.06126339  0.00826054  0.03768247  0.04757782  0.06139004  0.03225806
  -0.01667569 -0.00641439 -0.04216218 -0.04382773 -0.0650475   0.04612264
   0.0557125   0.05365982  0.04830763  0.01814464 -0.01178768 -0.0276855
   0.05310535  0.05253831  0.05532114 -0.0151262   0.00864776 -0.01962628
  -0.05529966 -0.04665987 -0.001164   -0.06270555 -0.01040278  0.0095119
  -0.02463632 -0.0106609   0.03232986 -0.01839557  0.01624932  0.00065653
   0.02670538 -0.03005916 -0.01148604 -0.01686143 -0.02712603 -0.05140554
  -0.03023658 -0.00710803 -0.00937147 -0.04206584  0.05591638  0.02328661
  -0.01741248  0.00839318 -0.05712924  0.04709468 -0.02990002  0.03204625
   0.04962549 -0.02767432  0.03495338  0.05790094  0.03500329 -0.07405653
   0.01016961  0.02618395  0.03581547  0.00520442 -0.04914961 -0.00963303
  -0.00039861  0.04620286 -0.01924295  0.01413172  0.04943766 -0.06134564
  -0.00453178  0.0039091   0.0244841  -0.04426655  0.0519329   0.04716483
   0.00011096  0.03911865  0.04436353 -0.04607709 -0.03775728 -0.00712567
  -0.06223556 -0.02766332  0.00886527  0.01864924  0.01975611 -0.03751617
   0.01646752  0.00891597  0.05264137 -0.01068555  0.04663895  0.04569385
  -0.02101653  0.04390873  0.04671299  0.06099292  0.02738177  0.05787743
   0.05760789 -0.01756823 -0.01727879  0.028887   -0.01152499 -0.00856626
   0.04597864  0.042437   -0.00028367 -0.01601415 -0.05938385  0.03049141
  -0.03864976  0.00598446  0.02163308  0.05221473 -0.01351937  0.0014832
   0.0496362  -0.06786118]]

Final Loss: 0.0000
Distance Metric: 6.4233
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.03282516  0.03247927  0.01010168 ... -0.02579568 -0.03127255
   0.00781295]
 [ 0.03725303 -0.00390681 -0.02396103 ...  0.02839428 -0.04038204
  -0.02323863]
 [ 0.02281886 -0.03938197  0.01213633 ... -0.00361951  0.00804319
   0.0274139 ]
 ...
 [ 0.00442514  0.01220835  0.02020065 ...  0.02293175 -0.03410847
   0.0056238 ]
 [-0.0204107  -0.02990714 -0.01889785 ... -0.01950205 -0.02842456
  -0.03294755]
 [-0.03199399  0.00774593 -0.02529648 ... -0.00608352  0.01741976
   0.02805983]]
layers.1.weight: [[ 0.01422887 -0.01857361  0.01952562 ...  0.01562906 -0.0265751
  -0.02928982]
 [-0.02881491  0.01054636 -0.00695703 ...  0.00660676  0.02495725
  -0.02358584]
 [-0.00885524  0.01411891  0.01376052 ... -0.01360919  0.00126112
   0.02272589]
 ...
 [ 0.03082127 -0.01347383  0.01469523 ... -0.00042366  0.0228795
   0.0202404 ]
 [ 0.01801618 -0.01554653  0.0250352  ... -0.01161215  0.0259804
   0.00688362]
 [-0.00020401  0.0026463  -0.01897426 ... -0.01889465 -0.0207743
   0.03150497]]
layers.2.weight: [[-0.11879611 -0.13228804 -0.16207719 -0.1785449  -0.11131261 -0.10197284
  -0.13946186 -0.1779978  -0.11389208 -0.11848615 -0.15745316 -0.11154813
  -0.17201543 -0.13844305 -0.09113154 -0.18461263 -0.15058625 -0.13836986
  -0.13809337 -0.10617814 -0.12415307 -0.08080146 -0.09234882 -0.13430235
  -0.1110996  -0.09874245 -0.16592355 -0.09184396 -0.09921137 -0.11869054
  -0.10555533 -0.10531042 -0.14896363 -0.15407565 -0.14134593 -0.10593338
  -0.15576018 -0.14041273 -0.15690775 -0.11511251 -0.13636501 -0.11454628
  -0.16287555 -0.17350486 -0.15235324 -0.10423268 -0.14664459 -0.17344888
  -0.14453426 -0.13993765 -0.13920972 -0.1455728  -0.13023059 -0.15640913
  -0.09056769 -0.17883456 -0.11771336 -0.12055518 -0.10891857 -0.11536983
  -0.18920904 -0.15477808 -0.13720968 -0.1223989  -0.10188275 -0.1771375
  -0.13321583 -0.1004513  -0.14344203 -0.10527789 -0.16740014 -0.16985948
  -0.11213672 -0.15745638 -0.10015931 -0.124084   -0.14383435 -0.13292757
  -0.14362116 -0.0959895  -0.10761614 -0.14028439 -0.1030964  -0.16894524
  -0.146999   -0.111484   -0.09240347 -0.09011633 -0.09717527 -0.11135326
  -0.15622109 -0.13624689 -0.17412944 -0.12755589 -0.12158435 -0.13201915
  -0.09776264 -0.17692752 -0.15641208 -0.10329389 -0.09853578 -0.13775675
  -0.1705241  -0.16708662 -0.08626439 -0.15195034 -0.10918389 -0.13514735
  -0.11589202 -0.09756732 -0.10554019 -0.16520967 -0.15052813 -0.16977397
  -0.12227852 -0.11927817 -0.15372328 -0.08976779 -0.11566512 -0.16673952
  -0.16874574 -0.11429673 -0.1658513  -0.12736759 -0.16993426 -0.14922778
  -0.1686711  -0.11628044]]

Final Loss: 0.0006
Distance Metric: 5.5115
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.01430209  0.0124874   0.01912149 ...  0.02647938  0.00900233
  -0.01120494]
 [-0.00606854 -0.00217673  0.00864453 ... -0.01876021 -0.00892168
   0.00213826]
 [-0.00651745 -0.02466618 -0.03781043 ...  0.0044649   0.00922624
   0.01081773]
 ...
 [ 0.00394436 -0.00417227  0.00136695 ...  0.01116602  0.00350333
   0.02407898]
 [ 0.04235123  0.02208259  0.04421888 ...  0.00300034 -0.00383451
  -0.00909088]
 [ 0.00996391 -0.01024903 -0.00234453 ...  0.00338514 -0.00598315
  -0.0198727 ]]
layers.1.weight: [[-0.03234894 -0.00884745 -0.02342645 ... -0.05437494  0.00010356
   0.00106861]
 [-0.00152411 -0.03046976  0.01380213 ... -0.00639933 -0.00819445
   0.00986517]
 [ 0.01835247 -0.03847998  0.00878299 ...  0.01500122  0.00268496
  -0.03277896]
 ...
 [ 0.04418204  0.00554888 -0.04571931 ...  0.02141442 -0.03601039
  -0.05037564]
 [-0.00101801  0.0028206   0.04745648 ...  0.02262385 -0.02374068
  -0.02211606]
 [ 0.01963626  0.0345302  -0.00170321 ...  0.02764649  0.03937937
  -0.00029425]]
layers.2.weight: [[-0.08784782 -0.32296944 -0.02925093  0.01110893  0.4272918  -0.49927217
   0.27939084  0.07873001 -0.29545715 -0.03143734  0.22381054 -0.01909576
   0.35273635 -0.05638376 -0.07405542  0.01397442  0.028145    0.04976903
   0.18046159  0.53538847  0.20427342 -0.54471487 -0.40662637 -0.23558682
  -0.14652967 -0.31879622  0.24163619 -0.0500325  -0.3040957  -0.57374763
  -0.6638228  -0.15070349 -0.18382652 -0.04744177  0.05931145  0.52565974
  -0.0496371  -0.11807302 -0.06285797  0.0027291   0.15803869 -0.14351107
   0.2963911   0.09398407  0.24139331 -0.26135316  0.13409486  0.32332793
  -0.20106524 -0.3372336  -0.20272505 -0.02827403  0.26795712 -0.02140195
  -0.45444703 -0.02758033 -0.22980183 -0.13905743 -0.15950666 -0.16973352
   0.01481684  0.172875    0.14930435 -0.01199711 -0.08582326  0.00386323
   0.02167761 -0.37711465 -0.13273624 -0.5701625  -0.06544229  0.08494108
  -0.40837726  0.04343092 -0.36163375 -0.45458966  0.20654745 -0.22705247
   0.11399556 -0.06687675  0.3881173  -0.16577649  0.02602239 -0.13002647
   0.20904458 -0.4370939  -0.54078    -0.81740785 -0.29101625 -0.2102371
   0.54694045  0.03999472  0.13443999  0.0465945  -0.36072227  0.07012297
   0.53350455  0.05939791 -0.01657293 -0.46031302 -0.17591794  0.29154634
   0.1147189   0.07021911 -0.06077819  0.05877483 -0.18886313 -0.25317574
  -0.59495234 -0.18606648 -0.11741535  0.12755123  0.16510646 -0.01233181
   0.0995528  -0.42035717  0.18302982 -0.43012825  0.2880606  -0.09404632
   0.18526493 -0.01539311  0.3738257  -0.01372958  0.10959164  0.4425621
   0.32331306 -0.07395053]]

Final Loss: 0.0003
Distance Metric: 7.1828
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1999

================================================================================

