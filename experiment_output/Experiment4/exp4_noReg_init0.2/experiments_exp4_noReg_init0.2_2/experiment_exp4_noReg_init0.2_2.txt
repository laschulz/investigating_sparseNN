Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.19312933 -0.15402651  0.07441039]]]
layers.1.weight: [[[ 0.23069094 -0.00985185]]]
layers.2.weight: [[[0.0601435  0.23056297]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.03087885  0.0294958   0.03973889 ...  0.         -0.00642608
   0.01536474]
 [-0.01910758  0.00754181 -0.02075015 ... -0.02735267 -0.0033837
  -0.02131652]
 [ 0.01918254  0.02205095  0.01712365 ...  0.03226271  0.01693561
   0.01153133]
 ...
 [-0.03757973 -0.04000418  0.02396275 ...  0.01369614  0.00799537
   0.01273598]
 [-0.01293582 -0.01345338  0.02318127 ...  0.02552706 -0.03318221
   0.03151377]
 [ 0.02305764 -0.03172134  0.00143366 ...  0.03481023 -0.03928681
   0.02504851]]
layers.1.weight: [[ 0.00183851  0.00668389  0.01864318 ...  0.0129186  -0.025166
   0.00231943]
 [ 0.0130398   0.0095767  -0.01155621 ... -0.01133652 -0.00760312
  -0.00742362]
 [-0.03129661 -0.00826815  0.0264073  ...  0.00814289 -0.00527383
   0.02835575]
 ...
 [ 0.01731665 -0.02039631  0.02644257 ...  0.02768174  0.02873185
  -0.02922455]
 [-0.03058608  0.00098967  0.01626093 ... -0.02358297  0.01801101
   0.02102875]
 [ 0.01922848 -0.03067652 -0.01149912 ... -0.00904535  0.00728127
   0.00249122]]
layers.2.weight: [[ 0.01593186  0.04111043 -0.05593234  0.01342695 -0.04631982 -0.03625432
  -0.00290518 -0.03915341  0.0087175   0.0346421   0.04560151  0.03899835
  -0.0521179  -0.03600939  0.02491563  0.02312319 -0.01875229  0.03329598
  -0.01866124 -0.00929781  0.04557902  0.03000779  0.01359471  0.02659179
   0.01342762  0.01885586 -0.02273968  0.00254286 -0.05041168 -0.00290435
   0.00372219  0.03223971 -0.03540714  0.03904608  0.02384287 -0.00158515
   0.00704499  0.01181878  0.03767195 -0.01618762  0.00822644  0.00730191
   0.03588871 -0.04390858  0.04486549 -0.00556721  0.0325122  -0.00300104
  -0.03095363  0.00911707  0.01931003  0.02344182  0.02937173  0.0285607
   0.04219967 -0.00547265 -0.03490002 -0.01043334 -0.01450233 -0.02221881
  -0.00542768 -0.02661518  0.00866704  0.01762601  0.04239428  0.01846679
   0.00632677  0.04968687  0.01252948  0.02979483  0.03928873 -0.02833849
  -0.00234794 -0.02612853 -0.01409677  0.00845277 -0.03616894 -0.03880042
  -0.03253103  0.01118053 -0.04648906  0.01402697  0.03992625  0.01715234
  -0.031533    0.04800734 -0.0362782  -0.03840615 -0.04635517 -0.00398997
   0.00344009 -0.02275428  0.02212434 -0.00551292 -0.02400177 -0.0417358
   0.04468791  0.0435797  -0.01191068  0.02630569  0.037388    0.06206092
  -0.04085482 -0.03656216  0.00919477 -0.02681615  0.01945508 -0.0204936
  -0.02713066 -0.00404727  0.01592967 -0.0049914   0.0360841   0.00014418
   0.03358297 -0.02209093  0.00123814  0.02032263 -0.007032    0.03890206
  -0.0369562   0.03860167  0.0080278   0.02890257 -0.04298143  0.03194726
  -0.02154294 -0.02029163]]

Final Loss: 0.0000
Distance Metric: 4.0724
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.07157564 -0.0447382   0.01897014 ...  0.1900757   0.3979189
   0.19961897]
 [ 0.21935117  0.21004263 -0.11429521 ... -0.17191827 -0.08332373
   0.06069627]
 [ 0.03148416 -0.26742753  0.07607684 ...  0.11465326 -0.03095306
   0.10119956]
 ...
 [ 0.10290222  0.09701464 -0.08256926 ...  0.04062443  0.16190438
  -0.17038193]
 [ 0.1510823  -0.25369814 -0.10453834 ... -0.04642794 -0.27698156
  -0.12125785]
 [ 0.07856788  0.17457695  0.15073673 ... -0.07392299 -0.11801594
   0.12688597]]
layers.1.weight: [[-0.02760708  0.1189176   0.00482938 ... -0.04166288 -0.10306183
  -0.1375268 ]
 [-0.02277221  0.00495419 -0.15659973 ... -0.14341587 -0.24022983
  -0.32615143]
 [-0.12826952  0.02515931 -0.10175674 ...  0.09598871 -0.0210914
   0.10505509]
 ...
 [-0.13332254 -0.23108211 -0.0576284  ... -0.01525255 -0.00905096
  -0.09444166]
 [-0.01508273 -0.10735217 -0.0485849  ... -0.06590951  0.03709712
   0.19240485]
 [-0.11927593  0.24067275 -0.05393277 ... -0.2100173  -0.05140081
   0.2376871 ]]
layers.2.weight: [[ 0.11309736  0.10021076  0.07689919  0.10022739  0.09580808 -0.2615888
   0.10053819  0.08980576  0.08801553  0.13778771 -0.3407844   0.08824377
  -0.11804746  0.10031787  0.08803546  0.09387671  0.08194916  0.0784503
  -0.11402562  0.09350094 -0.12065244 -0.10954541 -0.11568765  0.09831201
  -0.21236773  0.0718914  -1.0630032   0.08297252 -0.10880917  0.08716504
  -0.19105375  0.11389614  0.0818416  -0.34409595  0.08661232 -0.23861939
  -0.21032222  0.1000238   0.09304551  0.10733611  0.12977482 -0.2545217
   0.1174379  -0.13724084  0.10253591 -0.12858659 -0.13793206 -0.17304684
  -0.13250135 -0.18585834 -0.5452696  -0.13151054  0.10598627 -0.17784284
  -0.16830452  0.10047679  0.11156796 -0.13849416 -0.13283838  0.11771539
  -0.36783284 -0.10957444 -0.10537676 -0.12465241  0.08442914  0.20029232
   0.11493476  0.09371344 -0.11252765 -0.19937515  0.07860102 -0.15509172
   0.1139217   0.08696039 -0.35818377  0.1122201   0.1082568   0.08051028
   0.07943887 -0.39797977  0.07439768 -0.12178546 -0.34493095  0.08288474
  -0.24280685  0.08061015 -0.17757563 -0.1432742   0.13968916 -0.24988876
   0.11012524 -0.4322156  -0.12672222  0.09902823  0.11716165 -0.155321
  -0.11653586  0.08824557  0.0774295   0.09813137 -0.11301391 -0.1252746
   0.07550616 -0.152702    0.09145969  0.0854962   0.22741866 -0.15996519
   0.07423419 -0.18857257  0.09525428 -0.1353672   0.10874029  0.08877983
   0.11646377 -0.21429683  0.09619317  0.1097757   0.09546641  0.12536563
   0.15800552  0.09141727 -0.26377285 -0.19135231  0.07501975  0.10059008
   0.20188883 -0.12751351]]

Final Loss: 0.0083
Distance Metric: 29.7967
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.04882801  0.05284187  0.05366389 ... -0.01100898 -0.0284383
  -0.01534676]
 [-0.03037008 -0.0277172   0.0442057  ... -0.02485916 -0.03125087
   0.04066952]
 [ 0.02690178  0.00579688 -0.04114335 ... -0.0184367   0.01980629
   0.01685852]
 ...
 [ 0.01852615  0.05002971  0.01465834 ...  0.01454276 -0.03978247
   0.03771532]
 [-0.02468163  0.02341428 -0.02905575 ...  0.00921978 -0.00190857
   0.01847993]
 [ 0.02304984 -0.00480155  0.00907945 ...  0.04704413  0.03073196
  -0.00617948]]
layers.1.weight: [[ 0.01855874 -0.00777166 -0.03170795 ... -0.01682643 -0.04408636
  -0.00951913]
 [ 0.05021846 -0.00321724 -0.00474564 ... -0.03108012 -0.03378511
   0.03734251]
 [-0.05033448  0.01112058 -0.02249325 ... -0.02060185 -0.03475038
  -0.04341426]
 ...
 [ 0.01230645 -0.02423523  0.04060017 ... -0.00785444  0.0437314
  -0.02677938]
 [-0.01226325  0.03723073 -0.00328806 ...  0.02772415  0.01155262
  -0.02207673]
 [ 0.0134929   0.02370368  0.01411195 ...  0.01688094 -0.01443388
   0.03163428]]
layers.2.weight: [[-0.00596733 -0.00879909 -0.00323286 -0.00073203 -0.00771258 -0.00022126
   0.00665456 -0.00036706  0.00876825 -0.00467421 -0.0001552  -0.00365791
  -0.00270746  0.00097072 -0.00178946 -0.0028806   0.00858794 -0.00399774
   0.          0.00476445 -0.00698377  0.00278729 -0.00141595 -0.0039628
   0.00731004  0.00389657  0.00066755  0.          0.00519394  0.00256197
  -0.00082328  0.00160377 -0.00113701  0.00241719 -0.00600289  0.00411136
   0.00519817  0.00686307 -0.00174787  0.00520101  0.00530163 -0.00355511
  -0.00498037  0.00216335 -0.00035284  0.00027921  0.00107946 -0.00467521
  -0.00417679  0.00251439 -0.00192051 -0.00552014 -0.00209091 -0.00872561
  -0.00810013  0.00229875 -0.00501075 -0.00432934 -0.00491408  0.0004048
  -0.00196708  0.0082483  -0.00840978 -0.00409329 -0.00029779 -0.00265493
  -0.00492912 -0.00019479  0.00559515 -0.00271543 -0.00220161 -0.01754702
  -0.00148547 -0.00509498 -0.00113458 -0.00454144  0.00058561 -0.00329879
   0.00920292  0.0021315   0.00343323  0.00682111 -0.00482715  0.00707325
  -0.00032373  0.00443446 -0.00308378  0.00317624 -0.00378526 -0.00061914
   0.00153851  0.00645067 -0.00230902 -0.00577466  0.00227755  0.00637457
  -0.00397412 -0.00073859  0.00204401 -0.00444455  0.00689582 -0.0004391
   0.00850102 -0.00647804  0.00337181 -0.00637847 -0.00104954  0.00801599
  -0.00983995  0.0046352   0.00413066  0.00085036  0.00104026  0.00327937
   0.00428866  0.00180399  0.00016266  0.0019919   0.00815381  0.00545635
  -0.00564457  0.00510754  0.00033634  0.00130713  0.00590722 -0.00081203
   0.00204018  0.00641662]]

Final Loss: 0.2670
Distance Metric: 5.2213
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 162

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.03968016 -0.00037355 -0.00664226 ...  0.04055879  0.03188907
   0.00534974]
 [-0.0131784   0.0264249  -0.02272508 ...  0.03960786 -0.01310806
   0.00098493]
 [ 0.01070351 -0.00825541  0.02100253 ... -0.01259374  0.02616571
  -0.02489335]
 ...
 [-0.00078946 -0.00373552  0.00209424 ...  0.0251901   0.01661707
  -0.00875117]
 [ 0.03065708 -0.0074226   0.01334066 ... -0.01889153  0.0155395
  -0.03130109]
 [-0.03189676 -0.03288947 -0.00026481 ...  0.02833549 -0.01360065
  -0.02033851]]
layers.1.weight: [[ 0.00310571  0.0212044  -0.00787912 ... -0.01118703  0.02541532
   0.00853451]
 [-0.01173311 -0.01724453  0.0037255  ... -0.00859447  0.01752321
   0.0100199 ]
 [-0.00609254  0.03142096 -0.01648272 ...  0.01029498 -0.02765992
  -0.01316032]
 ...
 [ 0.03000398  0.00390625  0.02139555 ...  0.03085812  0.02927742
   0.02409177]
 [ 0.00956125 -0.0200675  -0.00059117 ... -0.01195676  0.02152083
   0.03086104]
 [ 0.0250292  -0.02391174  0.00327799 ...  0.02528536 -0.02116951
   0.01248913]]
layers.2.weight: [[-0.14152567 -0.0895576  -0.125      -0.1533604  -0.07609818 -0.12069391
  -0.11555051 -0.14546631 -0.11464814 -0.11918484 -0.14116713 -0.125
  -0.14899132 -0.12856862 -0.10260525 -0.16070016 -0.07935254 -0.13830636
  -0.08167554 -0.125      -0.15190142 -0.1185787  -0.10845384 -0.10579687
  -0.1480317  -0.1307499  -0.07912859 -0.12193396 -0.0886118  -0.12155936
  -0.14750974 -0.13259946 -0.15488023 -0.15219605 -0.125      -0.13307585
  -0.14772467 -0.09519362 -0.13429642 -0.0795854  -0.12307449 -0.09625835
  -0.125      -0.11115166 -0.16688786 -0.14908887 -0.08841591 -0.11108331
  -0.0989942  -0.125      -0.14386524 -0.08909081 -0.11447062 -0.11831723
  -0.15977798 -0.10526195 -0.11327214 -0.11744397 -0.1045699  -0.13431683
  -0.0928504  -0.10089973 -0.15742292 -0.125      -0.0939927  -0.15023948
  -0.08959937 -0.10817136 -0.16438083 -0.09589367 -0.12048377 -0.1335566
  -0.12161319 -0.08747369 -0.0925444  -0.09638777 -0.10525244 -0.15921864
  -0.14389043 -0.14229655 -0.14626591 -0.09848545 -0.09553733 -0.09400611
  -0.1325416  -0.13416187 -0.10283656 -0.125      -0.15004225 -0.08497114
  -0.0819396  -0.125      -0.13612516 -0.11410066 -0.11684281 -0.14900205
  -0.12785318 -0.11548824 -0.09816808 -0.15165134 -0.10641795 -0.15844332
  -0.0974406  -0.09811711 -0.15424629 -0.125      -0.125      -0.12450337
  -0.15353212 -0.1464041  -0.14850698 -0.14161323 -0.14385471 -0.1566493
  -0.12492453 -0.125      -0.125      -0.13154274 -0.14037283 -0.10058613
  -0.09954872 -0.125      -0.13838392 -0.125      -0.14457707 -0.125
  -0.1322882  -0.15732056]]

Final Loss: 0.0000
Distance Metric: 4.9143
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.02528878  0.01874097  0.01811624 ... -0.00067043 -0.00688888
  -0.0107264 ]
 [ 0.00601746  0.03822857 -0.03126118 ...  0.00689419  0.00822615
   0.00932076]
 [ 0.02507331 -0.00285627 -0.03205634 ... -0.03475184  0.04401921
  -0.02275633]
 ...
 [-0.05855087  0.02334828 -0.01309807 ... -0.00064372  0.01867879
  -0.02988101]
 [-0.04332302 -0.02178254 -0.02163508 ...  0.01727278 -0.01178489
  -0.00831041]
 [ 0.05063826 -0.02101443 -0.01764777 ... -0.04942404  0.00576871
  -0.03043593]]
layers.1.weight: [[-0.01604998  0.01186806 -0.00757018 ...  0.01003935  0.02542542
  -0.0599425 ]
 [-0.03791254 -0.00875539  0.02796542 ...  0.00197267 -0.04476877
   0.00211599]
 [-0.05465578  0.02717926 -0.00693526 ...  0.00388089  0.00512995
   0.02000502]
 ...
 [ 0.01385814 -0.02573205 -0.01702894 ...  0.04575995  0.01508752
   0.02083842]
 [-0.0065116   0.03045737  0.00965342 ... -0.02796668 -0.0139963
  -0.00499734]
 [-0.01071966 -0.0047419  -0.01801136 ...  0.01015505 -0.00084751
   0.03114201]]
layers.2.weight: [[-0.15664531  0.58793086 -0.01496571  0.61177886  0.15426794 -0.05666214
  -0.1839112   0.23097892  0.02867923 -0.00366888  0.1327701   0.00565311
   0.2762847   0.18042316 -0.08792073 -0.24327949  0.12466069 -0.3309575
  -0.17368595 -0.10127093 -0.20272842 -0.39285472 -0.11064421  0.31589586
  -0.1389398  -0.32363576 -0.08703804 -0.38522485 -0.15225819 -0.05429342
   0.09746925  0.39226294  0.02181181  0.20218882 -0.20848443 -0.11741383
   0.19186692  0.8476014  -0.15772267 -0.03440903 -0.39883572 -0.00611823
   0.2995404   0.2569407   0.79406476  0.01395189  0.5868012   0.12907279
  -0.02493609  0.30163243  0.22587195 -0.373644    0.20694545  0.3812289
   0.55304563  0.22980352 -0.3374429  -0.16953261  0.20689897 -0.30748597
  -0.29695296 -0.0030608  -0.14272399  0.22748013 -0.20867845 -0.42611015
   0.04030111 -0.08424637 -0.14328632 -0.12884395 -0.08424513  0.73439103
   0.04430354  0.22722767 -0.02042662 -0.21823308  0.22375904  0.04236484
  -0.44043458  0.24292342  0.40676108 -0.13695122 -0.19714446 -0.35422206
  -0.02641654  0.08967298  0.17305201 -0.07621462 -0.24075648 -0.2660309
   0.47036138 -0.03870762  0.01095289 -0.24228634  0.35048783  0.11789827
  -0.02254775 -0.20223774 -0.5070201   0.19867063 -0.18958691 -0.11105257
  -0.42154813  0.3261042   0.07684033  0.02501724  0.06065322  0.09356011
   0.36570928 -0.04873196  0.04464716  0.19744734 -0.02438672  0.49650782
  -0.29877272 -0.1290134  -0.06153419 -0.16016647  0.44282278 -0.19772601
   0.18838829  0.06512081 -0.23531935 -0.10820474 -0.13388316  0.03740273
  -0.432559   -0.06331819]]

Final Loss: 0.0000
Distance Metric: 7.4260
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.0127067  -0.02227749 -0.02152578 ... -0.05576592  0.05163938
   0.02077922]
 [-0.05483601 -0.0503878   0.05492593 ... -0.04105026 -0.03754174
  -0.05431485]
 [-0.05476411  0.00721366  0.02917088 ...  0.02062955  0.05146511
   0.01131615]
 ...
 [ 0.0256252  -0.05562613  0.0624302  ... -0.06580736 -0.03040089
   0.01010292]
 [ 0.01878499  0.02641295  0.02048516 ... -0.0138608  -0.00971179
  -0.03301015]
 [ 0.05963875  0.05437374 -0.03434668 ...  0.02653835 -0.00740152
   0.02008513]]
layers.1.weight: [[ 0.01581346 -0.04019878 -0.00970703 ... -0.01410555 -0.01568104
   0.0359451 ]
 [ 0.04865857 -0.01733553  0.00870372 ... -0.04688795  0.02675432
  -0.03479127]
 [ 0.03812074 -0.0066785   0.00648473 ... -0.00496877 -0.02598675
  -0.04054441]
 ...
 [-0.02075426 -0.00034675  0.02224766 ... -0.0141551  -0.03005785
  -0.03609692]
 [ 0.01988722  0.02164313 -0.03829747 ...  0.03481211  0.03504646
   0.00165153]
 [-0.04076851 -0.03001446  0.00932341 ... -0.00685684  0.00036924
  -0.02623504]]
layers.2.weight: [[-0.03942109  0.0385621  -0.03631096  0.0552606  -0.00151719 -0.04031554
  -0.02031497  0.00935756  0.00189042  0.05886341 -0.02863513 -0.06004444
  -0.03232309 -0.04047225  0.01178972  0.05422562 -0.02770784  0.01494914
  -0.03582413  0.01842198 -0.02484249  0.01537059 -0.04818936  0.03090445
   0.0297264   0.02009415  0.00992547  0.02791434  0.00429406  0.00896488
  -0.00717257 -0.04662124  0.02204952 -0.03651922  0.01835678 -0.0129888
   0.05379204  0.04904998  0.05005804 -0.05949547  0.02787022  0.06490161
  -0.01591726 -0.02972452 -0.0421979  -0.06118196 -0.04215802 -0.03011999
   0.00246657 -0.0046994  -0.01624266  0.02825825  0.0076309   0.
   0.04425342 -0.00565185  0.01708339  0.05583013 -0.04408997  0.00836557
   0.03185548  0.05779839  0.04714943 -0.03653423 -0.03189957  0.02646168
  -0.05333146 -0.04100638 -0.01754447 -0.05257779  0.02554767  0.04951418
  -0.00227254 -0.00503466  0.05577407  0.02433982 -0.01787639 -0.00485218
   0.04663359 -0.06529532  0.04359724 -0.01449916  0.04612942 -0.00369097
  -0.07792313 -0.02224112  0.01866192 -0.04361441  0.03790318  0.0277904
  -0.01785743  0.00489992 -0.02365837  0.03841938 -0.03148314 -0.01466569
  -0.04040531 -0.03899967  0.04102354 -0.02921712  0.00895835 -0.04594153
   0.01510027  0.00066387 -0.01852217 -0.00560582 -0.00274283 -0.04819591
  -0.01649689  0.00530807  0.00082075 -0.06426339 -0.02796171  0.04491562
   0.00196598 -0.03528549 -0.04966775 -0.04559254  0.06561235 -0.04924923
  -0.02814327 -0.05969364  0.01184266  0.01012728  0.00490265 -0.01331197
   0.06259469  0.00687609]]

Final Loss: 0.0000
Distance Metric: 6.4563
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.26665404  0.20400192 -0.05298549 ... -0.03220389 -0.00233283
  -0.02738082]
 [ 0.10625668 -0.12501575  0.07903865 ... -0.00809103 -0.04596156
   0.04112065]
 [ 0.09007274 -0.03019498 -0.00731804 ... -0.01695098 -0.00544498
   0.02299937]
 ...
 [ 0.22042453 -0.08677218  0.08518173 ...  0.04306435 -0.05094491
   0.04607142]
 [-0.17228577  0.13308367 -0.07406709 ... -0.00144953  0.02559448
   0.00746862]
 [ 0.3984849  -0.25128177  0.15237787 ...  0.07771713  0.
  -0.00308275]]
layers.1.weight: [[ 0.01905584 -0.01128903 -0.02222877 ... -0.01136716 -0.01705325
  -0.0076147 ]
 [ 0.08806198 -0.09561049 -0.06695528 ... -0.12539093  0.00726175
  -0.15055902]
 [-0.02196223  0.01394838 -0.0087117  ... -0.0162946  -0.01589438
  -0.02628192]
 ...
 [ 0.01559422 -0.06227747 -0.01282482 ... -0.05535454  0.01880301
  -0.09500546]
 [ 0.07197757 -0.08491125 -0.07061375 ... -0.10328083  0.04724844
  -0.21236363]
 [ 0.01751418  0.03848077  0.03202962 ...  0.04278384  0.0043062
   0.05456404]]
layers.2.weight: [[-2.27238471e-03 -9.07450736e-01  2.53460612e-02 -8.79499540e-02
  -1.76828474e-01 -3.45992386e-01 -6.39208779e-02 -6.11156151e-02
  -8.75286162e-02 -1.25575751e-01  5.69100976e-02 -3.94058645e-01
   2.06975322e-02 -1.96831331e-01 -8.25926781e-01 -8.61553997e-02
  -2.08423324e-02 -5.81522658e-02  4.43559326e-02 -1.09459281e-01
  -5.71814418e-01 -4.28918719e-01 -7.11684078e-02 -5.15095070e-02
   3.84258362e-03 -4.82613862e-01 -2.13271528e-02 -1.43381506e-01
  -4.77330871e-02 -1.00568511e-01 -5.40761463e-02 -1.73569992e-01
  -9.40523073e-02  2.78805522e-03 -5.65679297e-02 -1.00010045e-01
  -3.69574577e-02  4.70404979e-03 -8.20480734e-02 -7.41380528e-02
  -2.36685678e-01 -5.52564599e-02 -3.73727828e-01 -9.70097706e-02
  -1.12281188e-01 -4.97472584e-02  2.54684547e-03 -1.05191460e-02
  -4.37280864e-01 -3.57813239e-02  1.17716016e-02 -8.58633816e-02
  -5.39422691e-01 -1.17886484e-01 -1.13537765e+00 -2.44418710e-01
  -1.42055929e-01 -6.15032241e-02 -1.83973126e-02 -9.47832316e-02
  -5.73095977e-02 -4.38008010e-02 -4.06493247e-02 -9.33103915e-03
  -3.45205329e-02 -3.77930962e-02 -1.02703758e-02 -5.62498808e-01
  -5.90139106e-02  3.00292787e-03  2.95748701e-03 -5.19027337e-02
  -1.85015216e-01 -6.89337030e-02 -1.90759804e-02 -6.65905118e-01
  -4.24610108e-01 -9.33087468e-02 -7.68332109e-02 -4.75917272e-02
   1.16144354e-02 -5.64512797e-02 -9.87773761e-02  1.10854181e-02
  -3.10189221e-02 -8.12567994e-02 -9.24555063e-02 -1.49229849e-02
  -4.84924257e-01 -4.65834886e-02 -4.09818515e-02 -6.37963414e-04
  -9.08312276e-02 -1.26888955e+00 -5.16166277e-02 -4.48315479e-02
  -3.82378519e-01  2.32329611e-02  8.93816352e-03 -9.24095362e-02
  -5.06279804e-02 -7.53602013e-02 -3.26651148e-03 -1.50759146e-01
   1.63111985e-02 -5.21411240e-01 -1.82269275e-01  3.71424295e-02
  -1.42574340e-01 -2.82824282e-02 -9.49754342e-02 -6.59485608e-02
  -7.10386038e-02 -1.43799737e-01 -7.74379671e-02 -4.97262627e-02
  -5.23184299e-01 -2.18054742e-01 -7.69195855e-02 -4.99164052e-02
  -1.25384080e+00 -8.31631273e-02 -7.21594840e-02 -7.45939538e-02
  -1.67182744e-01 -2.64719665e-01 -1.11278176e+00  2.56393403e-02]]

Final Loss: 0.0011
Distance Metric: 16.1209
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.00172946 -0.01045966  0.00240833 ...  0.00804055  0.01986893
   0.02338167]
 [ 0.01417562 -0.00026627  0.00292276 ... -0.0025949  -0.00263076
   0.0096638 ]
 [ 0.00036672 -0.04044821  0.00996966 ... -0.0009864  -0.01044993
   0.02052263]
 ...
 [-0.02101861  0.01113814 -0.01103288 ... -0.0113547   0.00223825
   0.00218426]
 [ 0.02102621  0.00121557  0.00182878 ...  0.01340603 -0.00334881
  -0.00561991]
 [ 0.00559909  0.00596444  0.02370378 ... -0.00743563 -0.00226285
   0.00716598]]
layers.1.weight: [[-0.02269648 -0.00739554  0.01132554 ...  0.01623211  0.01384642
  -0.00103445]
 [ 0.04400656  0.02553779 -0.00793923 ...  0.01866277  0.00803543
  -0.00378091]
 [ 0.00515551  0.03446247 -0.01265919 ...  0.01841331  0.0294415
   0.        ]
 ...
 [-0.03233872 -0.01091176 -0.0517561  ...  0.0065412  -0.00240145
   0.03444084]
 [ 0.01332625 -0.03668268 -0.01152931 ... -0.01783978 -0.02348807
   0.01308292]
 [-0.03003805  0.03278241  0.00952673 ... -0.04995031  0.02357088
  -0.01602593]]
layers.2.weight: [[ 0.00938893  0.10868184  0.5524779   0.16682637  0.09401908 -0.03119631
   0.07754231  0.05041577  0.32819194  0.14833392 -0.46539626  0.12291089
  -0.07617407  0.01576545  0.27566016  0.31753063  0.22210741  0.10080213
  -0.0841729   0.04013489 -0.02195853 -0.07235941 -0.1737011   0.24957286
  -0.2401946   0.28434443 -1.064713    0.19950761 -0.02398599  0.1433337
  -0.16573967  0.22429875  0.31346545 -0.39151946  0.05673855 -0.32546943
  -0.34795427  0.3711391   0.3397718   0.20355268  0.36511037 -0.40607533
   0.38992938 -0.15363905  0.24618907 -0.02979437 -0.01521043 -0.1450488
  -0.03886856 -0.48267668 -0.92629516 -0.23343128  0.00406242 -0.15760356
  -0.01951492  0.1465749   0.11300161 -0.03663161 -0.16538179  0.08370365
  -0.25090036 -0.05796047 -0.02436297 -0.16696724  0.21930619  0.01468098
   0.06636111  0.13494249 -0.09058415 -0.34160572  0.81624484 -0.15988073
   0.20916447  0.43627238 -0.63038653  0.01519597  0.37537268  0.4252492
   0.6588414  -0.69937325  0.7240276  -0.01008246 -0.2674969   0.21185358
  -0.41610175  0.23110089 -0.16339055 -0.3083483   0.24208362 -0.3128094
   0.1201971  -0.6050493  -0.11162762  0.00750273  0.25576976 -0.28174657
  -0.0746282   0.59897655  0.05310651  0.08368704 -0.04285751 -0.16597828
   0.41122705 -0.07223745  0.1091272   0.15133743  0.41875893 -0.22779256
   0.33921155 -0.23171182  0.22224842 -0.15721363  0.30267495  0.06624446
   0.09242851 -0.26179725  0.04686383  0.15147457  0.19585283  0.25594318
   0.19509947  0.20727032 -0.2866039  -0.15496233  0.08239328  0.14943664
   0.2977483  -0.15559916]]

Final Loss: 0.0011
Distance Metric: 7.5816
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.01860888  0.05601315  0.0397699  ... -0.05256935  0.05586417
   0.06095824]
 [ 0.01989366  0.06716247 -0.01727374 ...  0.04822618 -0.05762444
   0.03541242]
 [-0.0192211   0.01602059  0.06928591 ...  0.0393266   0.06512535
  -0.01402247]
 ...
 [ 0.0150549   0.02823314 -0.04877243 ...  0.04909081  0.02596315
  -0.02682777]
 [ 0.06645009 -0.00655501 -0.063012   ...  0.00029304 -0.01590299
  -0.01560562]
 [ 0.01192976 -0.04825067  0.02710085 ... -0.0630016   0.00822465
  -0.0308491 ]]
layers.1.weight: [[ 0.05040847  0.03810459 -0.02709161 ... -0.03074795  0.02377468
   0.04124219]
 [-0.0342903   0.01633872 -0.02797231 ...  0.01924795 -0.00213106
   0.04419514]
 [ 0.01798249  0.00966735 -0.02696029 ...  0.00734603 -0.02115187
   0.01810835]
 ...
 [-0.01709733  0.01760184  0.0352823  ...  0.02872206 -0.00862089
   0.02657329]
 [-0.00036757 -0.0312337  -0.04749004 ... -0.04849321  0.00637914
   0.03001503]
 [ 0.03994995  0.01710701  0.04588772 ...  0.03209506  0.04205775
  -0.01750418]]
layers.2.weight: [[-0.0093505   0.00523948 -0.05357349  0.00539239  0.0107495   0.00418034
   0.01962604  0.01781523 -0.03954067  0.04738027 -0.04515267 -0.0203884
  -0.05719989  0.01108341 -0.04594604 -0.06376901 -0.0134352   0.01607703
   0.00880471 -0.04380399 -0.04566677 -0.00055649  0.04488315 -0.02469908
  -0.00282259  0.07662501 -0.04568867 -0.00613017 -0.01065598  0.03021969
  -0.060657    0.00300669 -0.02373422  0.05581658 -0.03779867 -0.04156614
  -0.06381993 -0.02879863  0.03502413  0.04367267 -0.04016171 -0.01860515
   0.05576691 -0.04186554  0.01252951 -0.02972434 -0.04784558  0.05806494
   0.01017012  0.01913231 -0.05813261  0.04076274  0.05542285 -0.00756921
  -0.02072396  0.03043854 -0.04088347 -0.01709201  0.05369786  0.03493149
  -0.01366026 -0.06349681 -0.05753133 -0.0252953   0.07609364  0.05502673
   0.03027911  0.03994396  0.02012942  0.04447357 -0.07257007 -0.05450544
  -0.02399609 -0.02210099  0.04122224  0.07497716 -0.01036402 -0.05216527
   0.03167341 -0.00111688 -0.05308217 -0.05307017  0.04378651 -0.04562772
  -0.01971268 -0.02477144  0.01185887 -0.06202219  0.06460719  0.00105673
   0.05470928 -0.02469041  0.0183109   0.02515464 -0.04802871  0.05811826
  -0.05199104  0.00458532 -0.04065172  0.04902883 -0.00012405 -0.02315506
   0.045284   -0.00530387 -0.04461812  0.03152399 -0.03402263 -0.05137071
  -0.01357461  0.01393701 -0.01985455  0.02327588  0.06103247 -0.04208965
   0.0407933  -0.02227272  0.00251031 -0.02246704  0.02983764 -0.01939705
  -0.06583753 -0.04194123  0.02808178 -0.04274432 -0.05274305  0.01322904
   0.05371614  0.06372225]]

Final Loss: 0.0000
Distance Metric: 6.2410
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 1999

================================================================================

