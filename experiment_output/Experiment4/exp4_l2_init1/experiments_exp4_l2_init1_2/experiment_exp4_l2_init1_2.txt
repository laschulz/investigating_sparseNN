Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.02830268  0.0320347  -0.00974801 ... -0.02724277  0.03003541
  -0.00830319]
 [ 0.00067734 -0.00143456  0.00380686 ...  0.03611409 -0.0412735
   0.01190958]
 [-0.0067828   0.00767239 -0.00233413 ... -0.00653439  0.00720609
  -0.00199094]
 ...
 [-0.03180256  0.03600221 -0.01095622 ... -0.03060436  0.03373851
  -0.00932843]
 [-0.00568207  0.00642716 -0.00195529 ... -0.00547398  0.00603682
  -0.00166792]
 [-0.02234803  0.02528773 -0.0076941  ... -0.02151831  0.02372667
  -0.00655739]]
layers.1.weight: [[ 0.00070569 -0.00842192  0.0001075  ...  0.00080445  0.
   0.00053883]
 [ 0.00061727 -0.00748324  0.         ...  0.00070494  0.
   0.00046923]
 [-0.00926495  0.09831489 -0.00223231 ... -0.01042647 -0.00187548
  -0.00730388]
 ...
 [-0.00198118  0.0201595  -0.00053032 ... -0.00222062 -0.00045669
  -0.00157654]
 [ 0.         -0.001745    0.         ...  0.          0.
   0.        ]
 [ 0.00512288 -0.055285    0.00116705 ...  0.0057759   0.00096632
   0.00401964]]
layers.2.weight: [[-0.01753122 -0.01555071  0.20751248 -0.00255733  0.19815649 -0.22722907
  -0.26253456 -0.03507269  0.18646863  0.01941491 -0.06098099 -0.24452421
   0.03057179 -0.38503078 -0.07305684  0.1497192   0.10055193  0.21977589
   0.0531492  -0.18967775  0.19677928 -0.19890109 -0.22980332  0.1313559
   0.00708125 -0.0789116   0.09806719  0.18537311  0.0950384   0.15422218
  -0.06709554  0.14801718  0.39314127  0.13186967 -0.06817495 -0.41893116
   0.2793192   0.1559213   0.16311836 -0.11166991  0.37148845 -0.19531101
   0.10999149 -0.26088792  0.43339136 -0.10940335 -0.3159902  -0.2833403
  -0.15808049 -0.19636208 -0.01309721  0.0070641  -0.15051621  0.08964867
   0.2839346  -0.01029575 -0.10116409 -0.22037764  0.02202033 -0.32353163
  -0.30598918  0.22226228 -0.19422902  0.04122312 -0.25663304  0.08310484
   0.11144243  0.1103195   0.0113185   0.20979652  0.23836544 -0.03368797
  -0.01839356 -0.07539775 -0.13117005  0.01022181  0.0305047  -0.07293205
  -0.11127892 -0.0513325   0.00860654  0.02887616 -0.01345443  0.12621589
   0.03075562 -0.11863874 -0.13410914 -0.00529136  0.07085136  0.1786387
   0.31028324 -0.25081584 -0.05832548 -0.08929316  0.03237474 -0.1280638
  -0.10219103  0.15267238  0.11841673 -0.24634786 -0.3059399  -0.22052622
   0.05009355  0.1068861   0.03855829  0.05442843  0.0266684   0.27118075
  -0.15623994 -0.10417838 -0.06058063  0.04553403  0.03139558 -0.1003902
  -0.03513167  0.00591828  0.2045722  -0.0920259   0.1318854   0.14518243
  -0.0038468   0.03308104  0.44252792  0.06622747  0.2079217   0.04276804
  -0.00344265 -0.11642256]]

Final Loss: 0.0003
Distance Metric: 10.6610
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 2177

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[-0.0001331   0.         -0.00049078  0.         -0.00014914  0.
   0.00013557 -0.0002114   0.00016158 -0.00017366  0.          0.
   0.         -0.00019378  0.          0.          0.          0.00023613
  -0.00015691  0.00016076  0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.0001791   0.          0.          0.
   0.          0.         -0.00023967  0.          0.         -0.00011741
   0.00012176  0.          0.          0.          0.          0.
   0.          0.         -0.00019371  0.          0.          0.
   0.          0.          0.00014766  0.          0.         -0.00017219
   0.         -0.00015621  0.          0.00028231  0.          0.00014919
  -0.00021516  0.          0.          0.00022048  0.00016536 -0.00014327
  -0.00018721  0.00014621  0.          0.          0.00022553 -0.00037087
   0.         -0.00025765  0.00012223  0.         -0.00013435 -0.00011111
   0.          0.         -0.00026034  0.00015142 -0.00032027 -0.00013022
   0.00020482  0.00016496  0.00012278  0.00029304  0.          0.
   0.          0.00012352 -0.00012616  0.00039375  0.          0.
  -0.00016553 -0.00022554  0.         -0.00023296 -0.00010479 -0.00017673
  -0.00016501  0.00015708 -0.0002551   0.          0.         -0.00034691
   0.00017859  0.          0.00014812  0.          0.00011664  0.
   0.00030072 -0.0001438   0.          0.00013888  0.00010851 -0.00028231
   0.          0.        ]]

Final Loss: 0.2550
Distance Metric: 7.0258
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 975

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-4.0754848e-04  2.6263209e-04  8.7271590e-04 ...  2.3508092e-04
  -7.0088514e-04  0.0000000e+00]
 [-5.7310070e-04  0.0000000e+00  1.3351839e-03 ...  5.8524829e-04
  -1.2705016e-03  1.3123358e-04]
 [ 2.1281952e-01 -2.7963558e-01 -2.5761411e-01 ... -3.9351299e-02
  -2.6318470e-01  3.3777320e-01]
 ...
 [-2.6856171e-04 -1.5540922e-04  7.9238339e-04 ...  3.4460731e-04
  -7.0877530e-04  0.0000000e+00]
 [-1.6749104e-03 -1.8368212e-04  4.6555167e-03 ...  2.1022013e-03
  -4.4429512e-03  2.6834488e-04]
 [-1.4972346e-03  6.0633721e-04  4.7839559e-03 ...  2.2368717e-03
  -4.3147197e-03  3.7268520e-04]]
layers.1.weight: [[-0.00131934  0.00031044  0.00280445 ...  0.00021568  0.00114317
   0.00160787]
 [-0.00129134  0.00157741  0.00430509 ... -0.00030595  0.00192691
  -0.00059752]
 [ 0.00073772 -0.00206548 -0.00429131 ...  0.00057864  0.00180938
  -0.00141234]
 ...
 [ 0.00068432  0.00164704 -0.00058917 ...  0.00047928  0.00048777
   0.00080213]
 [ 0.00173867 -0.00196443 -0.00767196 ...  0.00219769 -0.00018334
  -0.00033051]
 [ 0.00043787  0.00226773 -0.00537763 ... -0.00183335  0.00181112
   0.00200831]]
layers.2.weight: [[-4.0088207e-04 -6.2617986e-04  7.1111257e-04  5.9219939e-04
   3.1439305e-04  8.0391450e-04 -5.3507031e-04  3.5579360e-04
  -6.1313203e-04 -1.8404101e-04  1.2486831e-03  0.0000000e+00
   1.2911691e-03  0.0000000e+00 -1.5632894e-03 -1.3649925e-03
   6.3608447e-04  0.0000000e+00 -9.2585583e-04  2.2861750e-04
   3.3412012e-04  1.4943785e-03  4.8557174e-04  8.6313288e-04
   8.4422919e-04  0.0000000e+00  2.8079242e-04 -5.9453101e-04
   7.5585692e-04 -2.1361420e-04 -9.7106583e-04 -2.8940433e-04
   5.5663753e-04  0.0000000e+00  1.5050017e-03  5.0230511e-04
  -1.0097992e-03  4.0226083e-04  9.4556430e-04 -4.1634130e-04
   9.4783184e-04  1.9751471e-03 -3.2224806e-04 -4.5148103e-04
   0.0000000e+00  4.7204187e-04 -5.8644981e-04 -1.5051825e-03
  -3.0007161e-04 -3.2728704e-04  0.0000000e+00  1.3011866e-04
  -1.9757025e-04  1.3441536e-01  1.4790309e-03  5.7106308e-04
   0.0000000e+00  6.5493333e-04  1.5704091e-04 -5.5130327e-04
   1.1863363e-03  0.0000000e+00  3.2332187e-04  6.2130654e-04
  -6.3485833e-04 -2.8860511e-04 -1.0533444e-04  1.1157101e-03
   3.1148904e-04 -3.7695107e-04 -9.1933925e-04  0.0000000e+00
  -8.0130529e-04  2.2892172e-04 -2.8453971e-04 -5.6815351e-04
   4.1780446e-04  1.0123177e-03 -1.4480465e-03  5.1042711e-04
   6.8032724e-04  0.0000000e+00 -7.1915757e-04  8.4747182e-04
   1.6444596e-04  1.3378571e-03  1.0863781e-03  4.9000350e-04
  -4.9782038e-04  0.0000000e+00 -5.0352624e-04  8.9405739e-04
  -1.1407326e-03  1.5524810e-04 -8.3226658e-04 -7.6824136e-04
  -4.9150630e-04  8.3406153e-04  6.6406408e-04 -2.4758172e-04
  -8.0822490e-04  6.8243034e-04 -3.9406287e-04 -4.1998221e-04
   7.3889812e-04 -3.2824362e-04  0.0000000e+00  2.9555382e-04
  -7.0599146e-04  8.8251277e-04 -2.4677298e-04  2.3942356e-04
  -1.2127956e-03  1.1020299e-04  6.2101980e-04  7.5646274e-04
  -4.7684117e-04 -2.3488267e-04  1.3737038e-03  5.1834609e-04
   3.4959233e-04 -6.6265493e-04  7.4502896e-04 -9.6783903e-04
   3.2806571e-04  0.0000000e+00  1.1843825e-03  8.3115435e-04]]

Final Loss: 0.2519
Distance Metric: 12.1192
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 501

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00780999 -0.00949419  0.0035162  ... -0.02602082  0.02489102
  -0.01066976]
 [ 0.05148348 -0.05645805  0.02037    ...  0.00525472 -0.01322822
   0.00523573]
 [-0.07581524  0.09626273 -0.026296   ... -0.17228399  0.18416852
  -0.07768394]
 ...
 [ 0.05027305 -0.05517431  0.01985121 ...  0.0043862  -0.01232182
   0.00488831]
 [-0.06192909  0.07218946 -0.02224691 ... -0.10542995  0.12000106
  -0.05019066]
 [-0.00406476  0.00350577 -0.0007424  ... -0.03529132  0.03734425
  -0.01602728]]
layers.1.weight: [[-0.00729475 -0.02443313  0.0296391  ... -0.02396856  0.02193119
  -0.00258654]
 [ 0.00365903  0.01298615 -0.016249   ...  0.01273319 -0.01217898
   0.00109917]
 [-0.00255346 -0.00836981  0.00980072 ... -0.00821191  0.00730369
  -0.00095799]
 ...
 [-0.00833632 -0.0279667   0.03408575 ... -0.02743517  0.02518199
  -0.00294229]
 [ 0.03815288  0.11042716 -0.05567567 ...  0.10840032 -0.06641749
   0.01944746]
 [-0.00841091 -0.02822204  0.03440611 ... -0.02768522  0.02541585
  -0.00296786]]
layers.2.weight: [[ 1.1736081  -0.64226115  0.39847082  0.7763786   1.018761   -0.19629486
  -6.9416537  -0.21984555  0.3727384   0.8348187  -0.29462662  1.1023101
  -0.02404609 -0.38157624  0.85806566  0.37471494  0.7993277  -1.0231956
  -0.2771875   0.01481946  1.415836    0.83463115 -0.01134162 -0.1080686
  -0.9584214  -0.15071148 -1.068399   -0.9831238   0.98848045  0.03607798
  -0.08424164  0.20582959 -0.65635645  0.81167465  1.2362036   0.9281526
   0.05846322  0.57549185 -0.72131556  0.13980763  1.0985947  -1.1137602
   0.31954387  0.8269517  -1.0927504   0.53975725 -1.040285   -0.15587275
   0.3504802   1.754093   -0.47361907 -0.93491566 -0.25818035  0.4220729
   0.41894206 -1.8848265  -1.0711894  -0.06634808 -0.6208483  -0.8050549
  -0.6226207   0.75451756  1.023072    0.5019694   0.11335623 -0.8260753
  -1.1886297   0.24310136 -1.5253416  -0.20837131 -1.3437337  -1.0665175
  -0.49183062 -1.1443805  -0.22562742  1.7518975   0.67612183 -1.3702948
  -1.9450198   0.96824396  0.99777704  0.9130634   0.2647915   1.2681363
   1.546595    0.3827293   0.39975086  1.1633873   0.17303714 -0.6227168
   0.91794574 -1.289413    0.52127117  0.22214176 -0.5422099  -0.08535181
   0.18869585  0.55927044 -0.28343874 -0.28605297 -0.6991571   1.053318
  -0.4406014  -0.43646637  0.10828481  1.2213322  -0.86730784  0.28181365
  -1.1273534  -1.2213542   0.9637627   0.62680525 -0.883941    1.641934
   0.20440753  0.35033748  0.726354    0.29731882  1.067801    1.1255815
  -0.14967115 -0.91039526  0.03906989 -0.04646973  0.2707034   1.3421642
  -4.983776    1.3542722 ]]

Final Loss: 3.3465
Distance Metric: 38.8278
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 1632

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.         -0.00010944  0.         ...  0.          0.
   0.        ]
 ...
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[ 0.          0.00029783 -0.00064147  0.         -0.0008259  -0.00046642
  -0.00070475  0.00032616  0.         -0.00012082 -0.0007554   0.00030242
  -0.00039679 -0.00078379  0.         -0.00068764 -0.00015105 -0.00078982
  -0.0003384   0.00040319  0.         -0.00071055 -0.00025826 -0.00084107
  -0.00107975  0.00054472  0.00040675  0.          0.00026678 -0.00055561
  -0.00050274 -0.00026155  0.00046911  0.00022805 -0.00028295 -0.00032387
   0.00041296  0.          0.00017597 -0.00078765  0.00013679 -0.0003526
   0.00059025  0.00062452  0.000404    0.00015624 -0.00077759 -0.00066103
   0.00057349  0.00052769 -0.00057    -0.00020698  0.00148702  0.
  -0.00017452  0.00053063  0.00044865 -0.0002564  -0.00068149  0.00059086
  -0.00041222 -0.00065481  0.00075833 -0.00038672 -0.00075236 -0.00027717
  -0.00041539 -0.00062886  0.00050653 -0.00034828  0.00057273 -0.00029645
  -0.00060395  0.00039985  0.00037314  0.00071277  0.00018342 -0.00011718
  -0.00067458  0.00110863 -0.00050633 -0.00025751  0.000372    0.00077749
   0.         -0.00035053  0.00032491  0.00020776 -0.00031375 -0.00054844
  -0.0003411   0.          0.          0.00068711 -0.00030756  0.
   0.00015936  0.00107241  0.00056136 -0.00047783  0.00045378  0.00063765
  -0.00073406  0.          0.          0.00039046  0.00042914  0.00100117
  -0.00044657 -0.00068441  0.00060779 -0.00040558  0.00025621 -0.00090528
  -0.00064073 -0.00014332  0.         -0.00016006  0.00028427 -0.00010751
   0.00050276  0.00092554 -0.00087869  0.00091217 -0.0003177   0.0002063
   0.0002982   0.00012853]]

Final Loss: 4.9758
Distance Metric: 7.0320
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 837

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.05227377 -0.05540851 -0.00624267 ...  0.02656577 -0.04594856
  -0.01756415]
 [ 0.06670777 -0.02136402  0.05342451 ...  0.09653414 -0.08854881
  -0.00532708]
 [-0.10554157  0.11302006  0.00312054 ... -0.06866314  0.08958723
  -0.06523119]
 ...
 [ 0.06198509 -0.07736962  0.01625097 ...  0.06917573 -0.06514733
   0.00379547]
 [-0.25090393  0.22801194 -0.11149912 ... -0.03964991  0.05185349
  -0.10558665]
 [ 0.05195095 -0.03360317  0.02018613 ... -0.01589003 -0.06485683
   0.01002279]]
layers.1.weight: [[ 2.0800121e-03 -9.5958228e-04 -2.8209933e-03 ...  1.2454372e-03
  -1.3135566e-02  3.4718085e-03]
 [ 9.1333076e-04 -3.9075702e-04 -1.2869894e-03 ...  5.6126877e-04
  -5.9023211e-03  1.5696367e-03]
 [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00
   1.4213685e-04  0.0000000e+00]
 ...
 [-1.6330510e-02 -8.9951448e-02  9.6446788e-03 ... -2.8608721e-02
  -2.1169774e-01 -3.3724241e-02]
 [-7.8865973e-04  3.4866933e-04  1.0579684e-03 ... -4.8093978e-04
   4.9850354e-03 -1.3028590e-03]
 [ 4.0406685e-02  3.9251745e-02 -8.0203705e-02 ...  6.0934275e-02
  -5.5271469e-02 -6.3639041e-03]]
layers.2.weight: [[ 3.6471230e-03  1.6401315e-03  0.0000000e+00 -4.7382084e-01
  -3.4925172e-01 -1.3394060e-03  4.4308910e-01 -1.6936171e-03
  -2.0541265e-03 -4.9433843e-03 -3.4463871e-03  2.6482961e-01
   3.8039898e-03 -4.3400549e-04 -2.5783470e-03 -3.0887360e-03
   3.3376319e-03  2.2424681e-03 -2.2531431e-03  1.6785509e-04
   2.0645724e-03  3.2053867e-03  6.7604454e-03 -2.2561690e-03
  -4.7018778e-04 -6.3943961e-03 -1.5838949e-03  3.2727043e-03
  -3.1922457e-01 -3.6049327e-03 -1.0018429e-03 -3.6371644e-03
   3.9448920e-03 -1.3927837e-03 -2.9906412e-03 -4.0684468e-01
   4.6031967e-01 -4.4709416e-03 -1.9437448e-03 -3.2835767e-01
  -5.3017412e-04  4.5347208e-01  5.1105334e-03 -2.6233512e-01
  -4.9956506e-03 -3.4473878e-01  2.9198930e-01  2.5047087e-03
  -1.9379752e-03 -4.8282039e-03 -1.5379678e-03  3.3125746e-01
   1.8240995e-03  6.9822953e-04  2.8809973e-03  2.4838964e-03
  -2.4395722e-03 -2.8017008e-01 -2.4209471e-01  4.1930375e-01
  -1.0453874e-03 -1.5077407e-03 -1.0078705e-03 -1.3792866e-01
   1.9156433e-03 -2.0116828e-03 -3.4758833e-03 -2.4487257e-03
  -1.8247372e-03  5.4763355e-03  3.9958353e-03  1.2629039e-03
   2.8715150e-03 -2.0434521e-04  3.2445739e-03  5.1121855e-01
   2.2397141e-03  1.0212716e-03  3.6891229e-03 -7.7151565e-04
   0.0000000e+00 -2.2834782e-03 -2.0723434e-03  1.7539116e-02
   1.7710997e-03  2.7893342e-03 -3.0696485e-03 -4.9334862e-03
  -2.9618030e-03 -2.9617682e-01 -2.3278017e-03 -2.2381146e-03
   2.0557754e-03  3.3762839e-01  4.0568149e-01 -1.2791017e-03
  -3.4746989e-03  2.9835384e-04 -3.0235186e-01 -2.8531605e-01
  -1.7865131e-03 -5.1887741e-04  1.7549864e-04 -2.3416842e-04
  -3.1409645e-01  2.8084149e-03  1.0262395e-03  4.6391501e-03
   2.7489838e-01  3.7428329e-04  6.2805293e-03  3.6248175e-04
   2.2130024e-03 -7.1758608e-04  8.8577397e-04 -1.0946675e-03
  -2.9877253e-04 -4.5852493e-03  2.9503557e-01  0.0000000e+00
   6.3494424e-04 -2.0967200e-03 -3.4627782e-03  3.6322001e-01
  -3.1526003e-03 -2.5903037e-01 -1.3827187e-03  2.6883796e-01]]

Final Loss: 3.5820
Distance Metric: 26.5101
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 930

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.01268779 -0.01152644  0.0051954  ... -0.01299274  0.01783051
  -0.00510637]
 [ 0.10826276 -0.11540742  0.03742373 ... -0.03529624  0.03831819
  -0.01003924]
 [ 0.01074267 -0.01437296  0.0040874  ...  0.01820194 -0.01782571
   0.00642807]
 ...
 [ 0.00436254 -0.00823947  0.00154267 ...  0.02968832 -0.03266117
   0.01010781]
 [ 0.01216348 -0.01537926  0.0046914  ...  0.01750985 -0.01712603
   0.0065312 ]
 [ 0.00981575 -0.00690233  0.00417867 ... -0.01691414  0.02111408
  -0.00677347]]
layers.1.weight: [[-3.1598631e-02 -5.9443748e-01 -1.3761030e-03 ...  1.8298472e-03
  -2.0114447e-03 -4.0824275e-02]
 [-4.7568788e-04  2.3196954e-02 -7.0750999e-04 ... -8.5134245e-04
  -7.5769011e-04 -5.1610504e-04]
 [ 2.5261519e-02 -3.9626876e-01 -1.8542191e-02 ... -3.8236372e-02
  -1.7379100e-02  2.8428420e-02]
 ...
 [-4.6824137e-04  2.4347257e-02 -7.1284117e-04 ... -8.6435571e-04
  -7.6635240e-04 -5.1099621e-04]
 [-4.5388890e-04  2.6447730e-02 -7.2178373e-04 ... -8.8729861e-04
  -7.8140781e-04 -5.0085387e-04]
 [-4.7182880e-04  2.3799218e-02 -7.1034988e-04 ... -8.5819553e-04
  -7.6225970e-04 -5.1345932e-04]]
layers.2.weight: [[-3.040147   -0.06077245 -3.4001575  -1.4017171  -0.06317618 -0.05971816
  -0.06213701 -0.06239741 -0.06340697 -0.06294354 -0.06027345 -0.06144037
  -0.06137056 -0.06187579 -0.0594249  -0.06196295 -0.06108561 -0.06180448
  -0.06000815 -0.06081012 -0.0549588  -0.06038407 -0.05958219 -0.06102179
  -0.06186007 -0.06003713 -0.06084155 -0.05920277 -0.06106394 -0.05948151
  -0.0615087  -0.06155621 -0.05917078 -0.06126187 -0.05893511 -0.06102249
  -0.0615923  -0.06122996 -0.06316524 -0.06353027 -0.05860336 -0.06157899
  -0.04714667 -0.06171659 -0.06243515 -0.06823554 -0.06195952 -0.06168915
  -0.05877059 -0.05963114 -0.06154459 -0.06280613 -0.06112296 -0.05897257
  -0.06044992 -0.06210228 -0.05940264 -0.0600225  -3.8095636  -0.05986075
  -0.0591368  -0.06062728 -0.06120961 -0.06158524 -0.06146799 -0.0561
  -0.06238455 -0.06284703 -0.06009786 -0.06016032 -0.0627418  -0.05908613
  -0.06215497 -0.06211424 -0.06166448 -0.06033191 -0.06074718 -0.05919361
  -0.06143462 -0.06003564 -0.05950603 -0.0623554  -0.06134797 -0.06151257
  -0.06016869 -0.0598216  -0.05873491 -0.05954551 -0.06203092  7.4709067
  -0.06002583 -1.1310925  -0.05872762 -0.06104032 -0.06083913 -0.06226186
  -0.05972296 -0.05879328 -0.05968371 -0.06176556 -0.06836602 -0.06059947
  -0.06319626 -0.06049033 -0.06232924 -0.0603406  -0.06069055 -0.06025955
  -0.06050405 -0.06167795 -0.06156998 -0.0628125  -0.062345   -0.0630485
  -0.06024353 -0.06167583 -0.06004244 -0.06053053 -0.06072179 -0.06138153
  -0.05956908 -0.06241519 -0.06317855 -0.05971299 -0.06219624 -0.06253744
  -0.0657492  -0.06169708]]

Final Loss: 0.1888
Distance Metric: 33.6404
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 1794

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.00011756 0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 ...
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.00014278 0.         ... 0.         0.         0.        ]]
layers.1.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ... -0.00011009  0.
   0.        ]
 ...
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[-0.00066105  0.00062969  0.00055607 -0.0004825   0.          0.00016221
   0.00028727  0.00059795  0.00025565  0.00022679  0.00046094  0.00025903
  -0.00018456  0.0004293   0.00045489  0.00084798  0.         -0.00023413
   0.00095354 -0.00048503  0.00016393 -0.000109    0.          0.00085093
   0.00025045 -0.00069381  0.00067649 -0.00107798  0.          0.00027626
   0.00038111  0.00056923  0.          0.          0.         -0.00027854
   0.          0.00027051 -0.00068972  0.          0.00049953 -0.00029344
   0.         -0.00070209 -0.0006814   0.00045753 -0.00025027  0.00013302
   0.         -0.00014719 -0.00019883 -0.00028877  0.00069392 -0.00082248
  -0.00040659  0.0003994  -0.0005413   0.          0.00019177 -0.00027459
   0.00035813  0.00073427  0.00080979 -0.00050051  0.0007676  -0.00010572
  -0.00047728  0.00035577  0.          0.00088472  0.          0.00019715
  -0.00038665 -0.0001003   0.0005392  -0.00073448 -0.00045758  0.
   0.00033939  0.00015976 -0.0003481  -0.0002338   0.          0.00031877
  -0.00043115 -0.00061027  0.          0.00041134  0.00025151 -0.00029652
  -0.00110244 -0.000829    0.          0.00020688 -0.00019469 -0.00041926
   0.00029545  0.          0.00037989  0.00080638 -0.00100715  0.00013346
   0.         -0.00073896  0.00074152 -0.00019326 -0.00075663  0.00044489
  -0.00034313  0.00037459 -0.00018426 -0.00050275  0.00044316 -0.00030942
  -0.0001266   0.          0.          0.00043815  0.00025408  0.00026043
  -0.00011811 -0.00012483 -0.0006856   0.00014665 -0.0003203   0.00056822
   0.00037619  0.00026094]]

Final Loss: 0.3674
Distance Metric: 7.0302
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 863

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.00041501  0.00103636  0.00038943 ...  0.0638232  -0.0667848
  -0.01632591]
 [-0.00061288  0.00135238  0.00041006 ...  0.06332713 -0.06627148
  -0.01616945]
 [-0.00165972  0.00088349  0.00039461 ... -0.02754737  0.02891191
   0.00710168]
 ...
 [ 0.00195952 -0.00293778 -0.00065833 ... -0.05392089  0.05635047
   0.01365003]
 [ 0.00592085 -0.00726114 -0.0014123  ... -0.1274435   0.13390486
   0.0319426 ]
 [-0.01508105  0.01550782  0.00363918 ...  0.01247125 -0.01262871
  -0.00230004]]
layers.1.weight: [[-0.00084964 -0.00101696  0.00145325 ...  0.00124271  0.00187836
   0.00097523]
 [ 0.00050276  0.00060073 -0.00085331 ... -0.00073221 -0.00110833
  -0.00056983]
 [ 0.00110481  0.00132497 -0.00190628 ... -0.00162373 -0.00245032
  -0.00128634]
 ...
 [-0.00135677 -0.00163154  0.0023687  ...  0.0020072   0.00302198
   0.00161013]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.00027121  0.00032386 -0.000459   ... -0.00039437 -0.00059721
  -0.00030597]]
layers.2.weight: [[-3.72254886e-02  2.19302569e-02  4.86564487e-02  6.11147657e-02
   1.04934558e-01 -9.21348669e-03  4.66767102e-02 -4.77341749e-02
  -1.00503296e-01 -6.85795397e-02 -8.96514207e-03 -1.83462780e-02
  -7.30773434e-03 -5.27895279e-02  2.69198492e-02 -4.37003607e-03
  -4.12086286e-02  3.71852741e-02 -2.86984481e-02 -3.29054147e-01
   5.20233018e-03  5.39428107e-02 -1.52008254e-02  4.30354439e-02
   3.70643064e-02  2.32893880e-02 -2.27356795e-02 -3.11552193e-02
   1.96650438e-02 -1.94897000e-02  1.66817103e-02  1.22015094e-02
   3.76918502e-02  4.11968073e-03  4.65710051e-02 -3.68114710e-02
   2.92074084e-02 -6.00660499e-03 -2.95473933e-02 -3.05173472e-02
  -4.67702448e-02 -4.47022431e-02 -3.96028869e-02  2.34786849e-02
  -7.25508258e-02  3.18685919e-02  4.63702418e-02  6.01318330e-02
   3.35009359e-02 -3.55524477e-03  6.03956431e-02 -6.85800146e-03
   5.34910932e-02  6.73150551e-03 -2.03954820e-02 -6.69324473e-02
  -1.17771951e-02 -3.27261426e-02 -2.82806549e-02  2.51184776e-02
   1.04884827e-03  2.56959349e-02 -2.59323865e-02 -7.69456550e-02
   5.98998815e-02  3.99517305e-02 -4.31694314e-02 -9.25857015e-03
  -9.54111945e-03  4.58805025e-01 -1.35016525e-02  1.28866760e-02
  -1.61455497e-02 -1.56668797e-02 -5.17974943e-02  1.47622684e-02
  -1.62764732e-02  1.18979411e-02 -4.22468083e-03  8.10529944e-03
   2.93811760e-03  2.95157805e-02 -8.29860754e-03 -6.62379339e-03
  -7.19244778e-02  4.19410765e-01  8.33363771e-01  4.44583334e-02
  -4.72737942e-03 -6.45171329e-02 -5.03167361e-02 -6.65125728e-04
  -5.99600151e-02 -4.85821925e-02 -4.47654575e-02  2.68990733e-02
   1.80007860e-01  5.46475351e-02 -4.58005667e-02 -1.12722802e-03
   2.92324051e-02 -2.21661367e-02  1.46375354e-02 -5.14168777e-02
  -5.06763607e-02  4.26865779e-02  2.38397648e-03  1.15108669e-01
   6.57338575e-02 -8.33642706e-02  8.55457261e-02 -3.36230248e-02
  -2.45791883e-03  4.70190421e-02  3.09258066e-02 -1.38832200e-02
  -4.70683165e-03  8.66993796e-03  8.73061828e-03  3.90680619e-02
  -6.05452769e-02  1.09410834e+00 -8.86646006e-03 -1.41817350e-02
  -4.46858443e-02 -6.01949096e-02  8.92317388e-04  1.18115610e-02]]

Final Loss: 0.0003
Distance Metric: 7.4558
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 2712

================================================================================