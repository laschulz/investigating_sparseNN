Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.00726888  0.00787478 -0.00256866 ... -0.00651771  0.0073755
  -0.00232578]
 [ 0.01545732 -0.01674516  0.00546587 ...  0.01385726 -0.01568196
   0.00494583]
 [-0.01150231  0.01246058 -0.00406582 ... -0.0103128   0.0116703
  -0.00368035]
 ...
 [-0.0114233   0.01237588 -0.00403803 ... -0.0102422   0.01159022
  -0.0036552 ]
 [ 0.00635766 -0.00688735  0.002246   ...  0.00570017 -0.00645081
   0.00203417]
 [-0.02825342  0.03060752 -0.01001688 ... -0.02532001  0.02865759
  -0.00904208]]
layers.1.weight: [[-0.00244457  0.00507106 -0.00384555 ... -0.00381947  0.0020568
  -0.00943655]
 [-0.00088581  0.00164503 -0.00135757 ... -0.0013488   0.00062999
  -0.00324047]
 [-0.00271372  0.00566479 -0.00427549 ... -0.00424649  0.00230438
  -0.01050853]
 ...
 [-0.0017718   0.00358641 -0.0027706  ... -0.00275208  0.00143733
  -0.00675681]
 [-0.00035701  0.00050777 -0.00051821 ... -0.00051522  0.00016092
  -0.00116156]
 [-0.00262327  0.00546541 -0.0041309  ... -0.00410293  0.00222123
  -0.01014883]]
layers.2.weight: [[ 2.19049603e-01  7.35220760e-02  2.44376287e-01 -2.90689655e-02
  -2.40513131e-01 -9.15763751e-02 -2.57513434e-01  1.63211197e-01
  -9.85147059e-03  5.09465374e-02 -1.41741514e-01 -1.52498126e-01
  -3.48782897e-01  2.30699807e-01 -2.39326984e-01 -1.28980532e-01
  -2.13119373e-01 -3.37366422e-04  2.88956642e-01  2.21223965e-01
   1.23240016e-01  8.19917619e-02 -2.43488133e-01 -1.08868994e-01
  -1.71921343e-01 -9.67061892e-03 -6.59152120e-03  2.91920137e-02
   2.23169744e-01  2.50015080e-01  2.20232159e-01 -2.01358855e-01
   1.19150318e-02  2.78352469e-01 -3.22283089e-01  1.72118738e-01
   5.74321263e-02 -2.42210366e-02 -1.64593652e-01  8.80392566e-02
   3.24434102e-01  1.92132294e-02 -1.51405483e-02  2.27534026e-01
   1.04181029e-01  1.27944469e-01 -1.12688452e-01  1.27906784e-01
   1.48306549e-01 -3.50684971e-02 -2.24649310e-01  8.47816169e-02
   1.29035562e-01  2.86564171e-01 -8.75702277e-02  8.30312669e-02
   3.91114652e-02  2.59164870e-01 -1.04270848e-02  4.29688320e-02
  -1.40698656e-01  6.43163398e-02 -1.13672979e-01 -1.01570614e-01
   3.90047766e-03 -1.36469483e-01  3.11807513e-01 -1.00214705e-01
  -4.89749461e-02 -1.29559889e-01 -1.08409606e-01 -6.16325215e-02
   1.77938908e-01  7.98942298e-02 -2.81130850e-01 -3.17658871e-01
  -2.37417117e-01 -9.61698741e-02  1.63459688e-01 -9.97358933e-02
   9.08765942e-02 -1.78456828e-01  2.75863945e-01  7.38102645e-02
  -2.32497245e-01 -2.51756221e-01 -5.30988872e-02  1.82600290e-01
  -1.85528230e-02  1.17227629e-01 -1.82626233e-01  3.03978473e-02
  -1.07969880e-01  1.31164923e-01 -1.02953129e-02 -1.96407944e-01
   1.75459161e-01 -1.68877274e-01  2.89752096e-01  3.45499255e-03
  -3.38869601e-01  6.59879670e-02  2.64623314e-01 -1.98893204e-01
  -6.81457371e-02 -4.13293630e-01  2.59553015e-01  2.32194066e-02
  -2.90697455e-01  1.28668204e-01  3.11053097e-01 -1.47501886e-01
   1.34308696e-01  8.42031613e-02 -7.18462318e-02 -1.54452864e-02
   1.05020784e-01 -1.24973983e-01  6.70407293e-03 -5.81630282e-02
  -2.17400640e-01 -4.82048607e-03 -9.43816975e-02  8.13014582e-02
  -5.73452376e-02  1.55925691e-01  2.50592995e-02  2.35855460e-01]]

Final Loss: 0.0003
Distance Metric: 10.8048
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 1824

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 ...
 [ 0.          0.          0.         ...  0.         -0.000138
   0.        ]
 [-0.00010565  0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.          0.          0.00083351  0.00018801 -0.00029904 -0.00047264
  -0.0001522  -0.00019868  0.         -0.00014304  0.00022626 -0.00015103
  -0.00039469  0.00013948 -0.00117338 -0.00026309  0.00105212  0.
  -0.00020263  0.          0.         -0.00027646  0.00025123  0.00075012
   0.00061462  0.          0.         -0.00046018  0.         -0.00020219
  -0.00022953  0.          0.00035128  0.          0.00015446 -0.00029299
   0.00011644  0.          0.         -0.00066402 -0.00012502 -0.00063431
  -0.00111842  0.00114375  0.          0.00030362  0.          0.
   0.         -0.00032473 -0.00111099  0.         -0.00021195 -0.0012042
  -0.00036215 -0.00015181  0.          0.00034985 -0.00034839  0.
   0.0009301   0.00036186  0.0002701  -0.00026126 -0.00052358 -0.00042188
   0.00013368 -0.00030381  0.00069633 -0.00021518  0.00026162 -0.00024007
   0.00077619  0.00039662  0.00075982  0.          0.         -0.0002004
  -0.00010227 -0.00067061  0.00044756  0.00027765  0.00014514 -0.00082873
  -0.00041421  0.00049688  0.00010296  0.          0.         -0.00014892
   0.00026952 -0.00060555  0.          0.00033439 -0.0003601   0.00018837
  -0.00060905  0.00065053  0.00053744  0.00035491 -0.00013263  0.
   0.00089255 -0.00026746  0.00029829  0.00019101  0.00021121  0.00089254
   0.00019915  0.00026322  0.00010495 -0.00017133 -0.00034722  0.00012137
  -0.00051388  0.00023397  0.          0.00027564  0.          0.00053461
   0.000248    0.00028276 -0.00010696  0.00055345 -0.00066344  0.00015731
   0.          0.00020041]]

Final Loss: 0.2551
Distance Metric: 7.0305
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 859

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.00200022 -0.00049008  0.00251507 ... -0.00063667  0.00233681
   0.00260667]
 [ 0.00049065  0.         -0.00030614 ...  0.         -0.00035089
  -0.00038588]
 [ 0.00038697  0.00064166 -0.00303182 ...  0.00238586 -0.00323248
  -0.0035719 ]
 ...
 [ 0.00145641  0.00084123 -0.00336544 ...  0.00210645 -0.00344191
  -0.00383471]
 [ 0.00043317 -0.00023699  0.0011984  ... -0.00142163  0.00143552
   0.00156716]
 [-0.00080976  0.          0.00012801 ...  0.00043631  0.00011884
   0.00014169]]
layers.1.weight: [[ 0.00161587 -0.00248569 -0.00367663 ... -0.00318611  0.00220417
  -0.00091152]
 [ 0.00197825  0.00135622 -0.00242872 ...  0.00343952  0.
   0.00224982]
 [ 0.00032672 -0.00065784 -0.00141977 ...  0.00131237  0.00050422
   0.00222254]
 ...
 [ 0.00163664  0.00098898 -0.00284094 ... -0.00324857 -0.00250519
   0.00034037]
 [ 0.0014584   0.0027951   0.0038876  ...  0.00161558 -0.00202039
   0.00113218]
 [-0.0026482  -0.00262265  0.00262918 ...  0.00180059  0.
   0.00044048]]
layers.2.weight: [[ 1.79954339e-03 -8.88838549e-04 -4.14174609e-03 -1.41688634e-03
   1.22086247e-04 -2.40384554e-03 -4.73909685e-03  5.83403395e-04
  -3.22216307e-04  4.69892891e-03  4.55191266e-03 -3.42788029e-04
  -4.56796866e-03  1.78485014e-03 -3.60940909e-03  1.74260326e-03
   0.00000000e+00 -1.29595713e-03 -2.79923761e-03 -1.94101303e-03
  -9.10923409e-04 -3.43215070e-04  1.89383363e-03 -8.41333880e-04
  -2.61308579e-03 -5.89149166e-03  5.15440572e-03  2.72097136e-03
  -1.77124958e-03  2.03807885e-03  1.16635964e-03 -4.97477036e-03
   5.51841035e-03  0.00000000e+00 -1.16658746e-04  4.98038996e-03
  -2.88373837e-03 -1.08078959e-04 -2.33040703e-03 -5.74622909e-03
   2.90523656e-03 -6.73925562e-04 -1.64921361e-03  5.59420232e-03
   4.25199512e-03  1.13220913e-02 -1.08910096e-03  4.77110362e-03
   1.99494930e-03 -9.82080936e-04  3.54658137e-03  2.43711192e-03
   2.18487010e-04  1.40910165e-03 -2.24052672e-03 -2.08236370e-03
   9.47964203e-04  4.39375872e-04  0.00000000e+00 -2.22175178e-04
  -1.66934263e-03  3.41099314e-03  2.26957709e-04  1.52962108e-03
   5.93409175e-04 -5.68431744e-04  3.37499636e-03  2.63446389e-04
  -1.81765808e-03 -1.39864197e-03  4.08714265e-03  0.00000000e+00
  -5.55541413e-03  7.08705105e-04 -4.62188676e-04 -6.91055262e-04
   1.03285431e-03  2.75655446e-04 -9.19583312e-04  3.91919678e-03
  -1.48323062e-03  9.93512943e-03  2.95840227e-03  1.14466692e-03
   2.76576797e-03 -3.28018470e-03 -4.93249763e-03 -6.70635491e-04
  -6.28102617e-03 -3.15940427e-03  7.02626770e-04  3.45513783e-03
  -8.76064412e-03 -2.07345118e-03 -2.80457968e-03 -3.18718492e-03
   2.74419691e-03 -2.57999334e-03  5.70179150e-03  4.06682724e-03
   2.29498441e-03 -1.65295112e-03  3.81341088e-03 -2.36675958e-03
  -4.98999725e-04 -1.56433016e-01 -3.47763579e-03 -8.54671001e-03
   6.27301924e-04 -1.81500427e-03  8.77105806e-04 -1.35404186e-03
   2.59461300e-03  1.78604689e-03  2.71515641e-03  3.33154458e-03
  -2.49750610e-03 -5.13482839e-04  5.64185728e-04 -3.23883304e-03
   1.27622683e-04  1.35139911e-03 -6.33210700e-04 -1.05734007e-03
  -3.01773497e-03  2.23798724e-03 -4.49758675e-03  2.10634130e-03]]

Final Loss: 0.2523
Distance Metric: 9.5897
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 454

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.14902978 -0.15394942  0.03217452 ...  0.26182035 -0.28737983
   0.10691017]
 [ 0.03355153 -0.03339946  0.00381605 ...  0.02357193 -0.01984353
   0.00893963]
 [ 0.08123299 -0.08214049  0.01383881 ...  0.06631447 -0.06398303
   0.0253907 ]
 ...
 [-1.0040292   1.151035   -0.3902347  ... -0.66261977  0.74726033
  -0.12959976]
 [ 0.03150639 -0.03131616  0.00313895 ...  0.02304382 -0.01883928
   0.00897215]
 [-0.09253874  0.09696317 -0.01884008 ... -0.14066756  0.14514863
  -0.05620902]]
layers.1.weight: [[ 0.03093702  0.01282246  0.01899529 ... -0.10457017  0.01353365
  -0.02103117]
 [-0.04511392 -0.01819168 -0.02702786 ...  0.14555009 -0.0190919
   0.0284937 ]
 [-0.00259792 -0.00176536 -0.00206882 ...  0.00452245 -0.00181363
   0.        ]
 ...
 [-0.01434221 -0.00644762 -0.00928899 ...  0.048939   -0.00682078
   0.00988938]
 [ 0.00485377  0.00175344  0.00289319 ... -0.02085744  0.0019096
  -0.00500235]
 [ 0.02173123  0.00922969  0.01362121 ... -0.07583787  0.00977276
  -0.01559407]]
layers.2.weight: [[-0.8306764   1.1100876   0.05172146  0.2139365   0.6619728  -0.11090135
   0.42961174 -1.191258    0.66416705 -0.45533946 -1.469105    0.20129931
   0.27321458  0.80498856 -1.1704336   0.17965733  1.184646   -0.2563047
  -0.3025399   0.84870756 -0.39748776 -0.83602214  1.0234911   3.3863745
  -0.6004173   0.548424    0.6036074   0.11442178  1.5515058  -1.034705
   0.7848618  -1.2189784  -1.2085091  -1.2531279  -0.5328174   0.21079795
  -0.08839256  0.30955493 -1.341864   -0.42417562 -0.3893505  -0.5093364
  -0.07510726  0.733156   -0.23551625  0.9531467  -1.2884362   0.572566
  -1.3806986   1.1029502   0.9036346   1.1152413   0.529208    0.9975602
   1.2023735  -0.27371743 -0.09406506 -0.11603843  0.3212422   1.6141443
  -0.19212243 -0.68602735 -0.46606553 -0.20998743  1.3579936   1.4254868
   0.5560753  -4.7285137   0.91202426  0.0866956  -0.41807067 -0.7690826
  -0.5176485  -0.21148518  0.79163665 -0.4193502   0.42061773 -0.141423
  -0.71332306  0.07194428 -0.50401413 -1.0063088  -1.1787488   1.711377
  -0.43183312  0.94565797  0.4685839  -1.0405115  -0.74280995  0.8543165
  -0.9794225   1.1265757   0.39360717 -0.93705213 -0.5924804   0.38142318
  -0.11355171 -1.1663718   0.72529966 -0.29285675  0.39312235 -0.32356954
   0.4268644  -1.1162609  -1.0900742   0.5344196  -0.92186844 -0.14471178
  -1.4787613  -0.5972586   1.0451324   5.715274   -0.19896197  1.204563
  -0.5496913  -4.641551   -0.06982718 -0.35377324 -0.69092065 -0.9133087
  -0.86927396  0.3112238  -0.24907893  0.26711464 -0.79990727  0.41198528
  -0.16890885 -0.61974907]]

Final Loss: 3.2576
Distance Metric: 39.1283
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 1077

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 ...
 [-0.00015505  0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.          0.         -0.00020728 -0.00014872  0.          0.00086243
   0.00030579  0.         -0.00021895  0.          0.00056981  0.
   0.         -0.00039814 -0.00032607  0.0004962  -0.00023629 -0.00062948
   0.          0.00036318 -0.00042643  0.00020284  0.0001113  -0.00021556
   0.00040555 -0.00034218 -0.00010907  0.0007254  -0.00030564  0.00048805
   0.         -0.00051762 -0.00030574 -0.00033714 -0.00014235 -0.00037689
   0.00020277  0.00031991 -0.00016634  0.00013859  0.0006365  -0.00050175
   0.00047001  0.00015804  0.00031307  0.00018841 -0.00076171  0.00028886
  -0.00066346  0.00046422 -0.0002773  -0.0005132  -0.00024043 -0.00047941
   0.00061998  0.00018756 -0.00028762  0.00013877 -0.0002091   0.00048609
  -0.00051624  0.          0.00013921 -0.00041351  0.          0.00027881
   0.00011401  0.00010933  0.00025183 -0.00028069  0.          0.
  -0.00013786  0.          0.         -0.00059275 -0.00026331  0.
   0.         -0.00046013  0.          0.         -0.00034469 -0.00021933
   0.00010052 -0.00016944  0.          0.         -0.00027855 -0.00077967
   0.         -0.00042202 -0.00020227 -0.00024635  0.00062847  0.
   0.00022268  0.00019569  0.         -0.00022622  0.00038746  0.
   0.          0.00056016  0.          0.00051191  0.0002405  -0.00010146
   0.          0.00021082  0.          0.00016703  0.00044406 -0.00020051
   0.00012974  0.00075398  0.          0.00018204  0.00040165 -0.00044692
   0.00011697  0.00017138 -0.00049195 -0.00039176 -0.00029836 -0.00014452
   0.00031982  0.        ]]

Final Loss: 4.8762
Distance Metric: 7.0304
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 899

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.06339479 -0.06099772  0.01779611 ...  0.0602846  -0.07254689
   0.02128648]
 [ 0.05824877 -0.05935874  0.01825047 ...  0.04884236 -0.06146895
   0.01740959]
 [-0.05557339  0.06264924 -0.01724955 ... -0.04568303  0.06042877
  -0.01258647]
 ...
 [ 0.11703747 -0.0401092   0.0592441  ...  0.33594888 -0.30306798
   0.00053916]
 [-0.09579657  0.08087649 -0.011508   ... -0.08225454  0.07926808
  -0.0600052 ]
 [ 0.07541561 -0.0617686   0.0222634  ...  0.09022457 -0.09993175
   0.02484462]]
layers.1.weight: [[-0.00513962 -0.00531668  0.00527024 ... -0.00073763  0.0037322
  -0.00476115]
 [ 0.05350314  0.0554131  -0.05224248 ...  0.03519221 -0.01872466
   0.04906967]
 [-0.00370243 -0.00412631  0.00439559 ...  0.          0.0032335
  -0.00395495]
 ...
 [ 0.00349692  0.00340751 -0.00385892 ...  0.         -0.00294785
   0.00322628]
 [-0.00171921 -0.00194558  0.00189252 ... -0.00048816  0.00181502
  -0.0016254 ]
 [-0.0110467  -0.01458063  0.01803656 ... -0.06311905  0.00943172
  -0.01176086]]
layers.2.weight: [[-8.76045972e-03 -3.17358673e-01 -6.93808636e-03  3.99303973e-01
   3.12650064e-03 -1.32219475e-02 -4.12198622e-03 -1.03320992e-02
  -7.84454588e-03 -1.49978651e-02 -4.01870767e-03  1.42916320e-02
   1.06970062e-02 -3.29769015e-01 -2.22456962e-04 -1.45363645e-03
   3.55960608e-01 -3.46251547e-01 -2.83227768e-03 -1.10261356e-02
   2.64023477e-03 -1.12610925e-02  7.52593018e-03 -1.20446868e-02
  -1.42090051e-02 -3.16527665e-01 -1.09899156e-02  1.42514696e-02
   6.07475452e-03 -4.10546064e-01  5.65547636e-03 -1.29489172e-02
  -1.37744108e-04  9.02714150e-04  1.29338074e-02  4.76633804e-03
  -3.20528448e-01  2.34168589e-01 -7.36573897e-03 -1.18508041e-02
  -3.54821864e-03 -3.18655744e-03 -1.66849128e-03 -1.96544051e-01
  -7.91954435e-03 -4.38971387e-04  2.96625078e-01  3.27263564e-01
  -7.33487122e-03  7.53150322e-03 -8.94935429e-03 -4.21940023e-03
  -9.56664514e-03  2.92347729e-01 -1.17420843e-02  9.56442859e-03
  -1.31831784e-02 -3.69164441e-03 -2.32398612e-04 -4.34372062e-03
   1.64338667e-03  1.16155054e-02 -2.28706456e-04 -9.04961582e-03
  -7.71403359e-03  4.76532709e-03  3.29738617e-01  9.25438665e-03
   3.99933070e-01 -3.45767140e-01  2.40946538e-03 -1.11045828e-02
  -1.42486189e-02 -2.11492460e-03 -5.80308074e-03 -3.96111468e-03
  -8.66363477e-03  1.64959370e-03 -5.88031486e-03  7.81449303e-03
   7.04779988e-03 -1.38992341e-02  3.64473462e-01 -9.37895756e-03
  -1.80203132e-02  6.18585618e-04  3.48484784e-01  7.34564429e-03
  -4.02629703e-01  6.54722215e-04 -1.36549864e-02 -4.20630304e-03
   2.38134433e-03 -2.97778338e-01  1.27427662e-02 -5.25608361e-01
  -1.45590119e-02 -1.41117647e-02  5.46578784e-03  1.67491511e-02
   1.38814570e-02 -1.09272785e-02  8.14795773e-03  1.25903077e-03
   3.07987690e-01 -4.17423714e-03  5.11216402e-01 -7.27570709e-03
   9.69425030e-03 -5.63677371e-01  2.68799998e-03 -1.15464851e-02
   5.13796788e-03  4.43872988e-01 -1.39842229e-02 -1.49043638e-03
  -1.14482809e-02  9.37259663e-03 -6.27727062e-03 -1.96769740e-02
  -8.19148310e-03  5.03513170e-03  3.91317066e-03  1.23861404e-02
   1.18343811e-02  6.01476151e-03 -3.65672144e-03  4.04815555e-01]]

Final Loss: 3.4934
Distance Metric: 24.2721
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 689

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00757247 -0.00507508  0.00370005 ...  0.01970355 -0.01809356
   0.0048019 ]
 [-0.00408725  0.00295336 -0.00263745 ... -0.05068386  0.05351837
  -0.01651787]
 [-0.00810189  0.01015954 -0.00218262 ...  0.00499257 -0.00342596
   0.00041506]
 ...
 [ 0.01203129 -0.00903758  0.00575104 ...  0.02892972 -0.02732182
   0.00767514]
 [ 0.00832055 -0.00638122  0.00346148 ...  0.00911551 -0.00720116
   0.00146431]
 [ 0.0151643  -0.01286962  0.00613548 ...  0.01630616 -0.01427609
   0.00361218]]
layers.1.weight: [[-0.00010197  0.000219    0.00062342 ... -0.00018977 -0.00012025
  -0.00029104]
 [-0.00012019  0.00018769  0.00057484 ... -0.00020456 -0.00013808
  -0.00030185]
 [ 0.          0.00028543  0.00072687 ... -0.00015786  0.
  -0.00026848]
 ...
 [-0.00013072  0.00016975  0.00054743 ... -0.00021283 -0.00014806
  -0.00030795]
 [ 0.          0.00027004  0.00070314 ... -0.00016512  0.
  -0.00027379]
 [-0.00010463  0.00021454  0.00061687 ... -0.00019184 -0.00012272
  -0.00029262]]
layers.2.weight: [[-0.04388841 -0.04286675 -0.04606698 -0.03981789 -0.04047292 -0.04634273
  -0.04389402 -0.04432851 -0.04544261 -0.03888232 -0.05399754 -0.04834985
  -0.03882869 -0.04555877 -0.04580124 -0.04135432 -0.04099382 -0.0405681
  -0.04374241 -0.04530628 -0.0518236  -0.04192486 -0.04242846 -0.03823769
  -0.04399908 -0.04073605 -0.04385679 -0.04133736 -0.0446     -0.0430678
  -0.03929431 -0.04055134 -0.04366772 -0.04240869 -0.039199   -0.04487201
  -0.04149872 -0.03976219 -0.04450967 -0.03992647 -0.04229084 -0.04376612
  -0.04239639 -0.04257206 -0.04135133 -0.03941746 -0.03863014 -0.0378713
  -0.03897402 -0.04372055 -0.04668225  2.825028   -0.04427284 -0.04416568
  -0.0389828  -0.04112951 -0.04424575 -0.0377945  -2.6645212  -0.04321007
  -0.0419507  -0.04411288 -0.04158542 -0.04296944 -0.04162676  3.4543352
  -0.03914478 -0.04515295 -0.0392735  -0.04549688 -0.04482573 -0.04652775
  -0.04349231 -0.03870053 -0.03596758 -0.0398639  -0.0461193  -7.522318
  -0.04002026 -0.04087975 -0.04531554 -0.04415044 -0.05277002 -0.04033916
  -0.03973883 -0.04189952 -0.04143358 -0.04145997 -0.04526222 -3.324541
  -0.03774225 -0.0380929  -0.03972528 -0.03924575 -0.04484925 -0.04482798
  -0.08951803 -0.04310715 -0.04492313 -0.03887687 -0.04339802 -0.04181868
  -0.04144143 -0.04241622 -0.03851805 -0.0404089  -0.04302733 -0.04689806
  -0.04378179 -0.04693253 -0.04492245 -0.04322481 -0.05096655 -0.04219092
  -0.04929855 -0.03873565 -0.04780328 -0.05361856 -0.04119214 -0.04632403
  -0.03740442 -0.04270181 -0.04302329 -0.03825608 -0.04557475 -0.04228707
  -0.04556846 -0.0437469 ]]

Final Loss: 0.1882
Distance Metric: 33.4664
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 1443

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.00010366  0.         -0.00013303  0.          0.00011614  0.
  -0.00010514  0.          0.          0.         -0.00017725  0.
   0.          0.          0.          0.00025722  0.          0.
   0.          0.         -0.0001819   0.          0.          0.00019831
   0.          0.00010754  0.00017468 -0.00036737  0.0001965   0.00015428
   0.          0.          0.          0.00013153  0.         -0.00026689
   0.          0.          0.00025054  0.00053759  0.         -0.00013091
  -0.00012269  0.          0.          0.         -0.00010717 -0.00032187
   0.00011024  0.00023535  0.00019972 -0.00010916  0.00030254  0.
   0.          0.00021679  0.00045548 -0.00018589 -0.00029242  0.00023369
   0.00011668  0.          0.00019942 -0.00012179 -0.00014084 -0.00010462
   0.          0.          0.0002328   0.00019541  0.         -0.00019787
   0.          0.          0.00028718  0.00016615  0.          0.
   0.00052442 -0.00029098  0.00015022 -0.00029038  0.000159   -0.00023521
   0.00022342  0.          0.          0.          0.          0.00017235
  -0.00034457 -0.00017583 -0.00020899  0.         -0.00024187  0.00043821
  -0.00010686  0.          0.         -0.00033664  0.          0.
   0.         -0.00017107  0.          0.          0.          0.
  -0.00038955 -0.00038746 -0.00014145 -0.00030464 -0.00011033  0.00010586
   0.0002184  -0.0003332  -0.00019721  0.0002351   0.00013176  0.00034598
   0.00027925  0.          0.00026217 -0.00033107 -0.00031733  0.00023125
   0.         -0.00023327]]

Final Loss: 0.3664
Distance Metric: 7.0263
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 946

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.00950796  0.00365     0.00725393 ... -0.00068208 -0.00023079
  -0.00054306]
 [-0.00686627 -0.00200922 -0.0058066  ... -0.00751824 -0.00324992
  -0.00564223]
 [ 0.01017214  0.00391849  0.00775277 ... -0.0007675  -0.0002625
  -0.00062493]
 ...
 [-0.00663387 -0.00258856 -0.00504731 ...  0.00080922  0.000287
   0.00065058]
 [ 0.01795204  0.00680921  0.01374125 ... -0.00054478 -0.00012935
  -0.0003936 ]
 [ 0.00646084  0.00188362  0.00515479 ...  0.00453054  0.0017529
   0.00310073]]
layers.1.weight: [[ 0.         -0.0017511   0.         ...  0.         -0.0002535
   0.        ]
 [ 0.          0.000782    0.         ...  0.          0.00011357
   0.        ]
 [ 0.          0.0011998   0.         ...  0.          0.00017405
   0.        ]
 ...
 [ 0.          0.00300026  0.         ...  0.          0.00043119
   0.        ]
 [ 0.          0.00252868  0.         ...  0.          0.00036455
   0.        ]
 [-0.00013374 -0.00422937  0.         ...  0.         -0.00060212
   0.        ]]
layers.2.weight: [[ 2.3710918e-02 -1.0648121e-02 -1.6305709e-02 -2.8105516e-02
  -3.1135678e-02 -5.0631508e-02  1.5376748e-02 -7.0162535e-02
  -1.5998164e-02  3.9225444e-02  5.8562852e-02  5.3968880e-02
  -3.4171719e-02  3.6986951e-02  6.5050252e-02  2.6859421e-02
  -2.7056422e-02 -2.1114503e-03  6.3282929e-02 -3.0286208e-02
   3.5871200e-03  3.9950013e-03 -5.5071533e-01 -3.2023031e-02
  -2.3941822e-02 -3.3938885e-02 -5.7354052e-02 -1.5110911e-02
   1.1825521e-02  6.5469749e-02 -1.2096186e-02 -5.4893486e-02
  -2.3744438e-02 -5.8907480e-03  5.5382594e-02  3.2713812e-02
   6.5802254e-02  4.7119070e-02  8.0286354e-02  2.5911495e-02
  -1.9596281e-02 -3.6130015e-02  3.2151595e-02 -3.4375440e-02
  -5.0349742e-02  5.2158821e-02  6.8858288e-02  4.0273657e-01
  -6.8121105e-02  3.4895316e-02  5.6900211e-02 -6.4418972e-02
   6.1497390e-02 -3.2745693e-02  2.5363395e-02  7.3035280e-03
   5.5846661e-02  3.8117049e-03  7.2242141e-02 -6.7054711e-02
  -1.1600975e-02 -6.4675972e-02  1.4818627e-02  6.2079363e-02
   5.3776954e-03  6.5742292e-02 -9.4449287e-03  6.6366009e-02
  -3.4878366e-02  1.1632130e-02 -2.5721230e-03 -6.4130954e-02
  -2.4947286e-02  5.9770331e-02  4.9509205e-02  2.0124123e-02
   2.9201478e-02  1.1183545e-02  4.7863081e-02  5.4067239e-02
  -5.3574577e-02  4.0966380e-02 -6.9718309e-02  4.8519291e-02
   5.2039899e-02  1.9746743e-02 -6.7190312e-02 -4.9272564e-04
  -1.4655100e-02  1.0697824e-02 -5.3880263e-02 -4.5295623e-03
  -6.6941082e-02 -4.0468603e-02 -5.0037853e-02 -6.4071715e-02
  -7.0187455e-01 -6.0346529e-02  4.0096056e-02  6.4710818e-02
   5.9296772e-02 -3.2649107e-02 -1.7333236e-02 -5.2960873e-02
   2.5942782e-04 -3.2430515e-02  4.1525822e-02  5.2858740e-02
  -3.6658145e-02  3.1343985e-02 -2.3047520e-02 -3.0536341e-02
  -6.9696158e-02  6.1423127e-02  2.6151419e-02 -5.5847067e-02
  -2.2456514e-02 -7.3697105e-02  0.0000000e+00  7.2370611e-02
   3.3085819e-02  1.2190080e+00 -3.7749700e-02 -3.3954699e-02
   5.0275818e-02 -4.0105183e-02 -3.3989832e-02  5.5532414e-02]]

Final Loss: 0.0002
Distance Metric: 6.8054
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 2414

================================================================================

