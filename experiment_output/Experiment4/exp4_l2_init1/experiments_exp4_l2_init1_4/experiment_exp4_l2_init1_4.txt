Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.00997878  0.01070096 -0.00318804 ... -0.00860244  0.00990256
  -0.00322166]
 [ 0.0080946  -0.00867997  0.00258628 ...  0.00697817 -0.00803232
   0.00261297]
 [-0.00441364  0.00473329 -0.00140984 ... -0.00380492  0.00438021
  -0.0014249 ]
 ...
 [-0.00687069  0.00736835 -0.00219494 ... -0.00592317  0.00681853
  -0.00221825]
 [ 0.01940296 -0.02080432  0.00620042 ...  0.01672685 -0.01925125
   0.0062659 ]
 [ 0.0424258  -0.04546526  0.01357339 ...  0.03656723 -0.04206111
   0.01372533]]
layers.1.weight: [[ 0.00113582 -0.00104483  0.00046394 ...  0.00076045 -0.00241488
  -0.00525998]
 [ 0.00286903 -0.00242315  0.00123839 ...  0.00195801 -0.00574811
  -0.01265282]
 [-0.00183444  0.00136569 -0.00084841 ... -0.00128355  0.00337621
   0.00755136]
 ...
 [-0.00060887  0.00034845 -0.0003139  ... -0.00044406  0.0009499
   0.00219897]
 [ 0.00243944 -0.00207907  0.00104725 ...  0.00166163 -0.00491816
  -0.01081337]
 [-0.00036445  0.00014994 -0.00020595 ... -0.00027589  0.00047313
   0.0011443 ]]
layers.2.weight: [[-8.19279924e-02 -1.99152201e-01  1.20229520e-01  8.66496265e-02
  -2.44013175e-01  0.00000000e+00  6.80056885e-02  3.15848261e-01
   6.60669208e-02  8.98947939e-02 -3.83764505e-02 -3.84299010e-01
   1.19842656e-01  8.93473849e-02 -5.38648404e-02 -2.69740492e-01
  -9.02929828e-02  2.33774617e-01  2.23448928e-02 -6.92780092e-02
  -3.10554534e-01  2.69516885e-01  1.94012821e-01 -9.31849778e-02
  -2.56789457e-02  3.11618507e-01  6.47121146e-02  1.98267609e-01
  -9.27040875e-02 -4.91559692e-02  3.17152888e-01 -1.74155399e-01
   1.58025175e-01  6.98123723e-02  1.74537763e-01  1.81380302e-01
   2.77105898e-01 -8.26569498e-02 -4.99896035e-02  1.51176929e-01
   2.63317645e-01 -4.02446091e-02  1.77235708e-01  8.19226503e-02
  -1.98448107e-01 -8.09043273e-02  1.95970729e-01  1.18784457e-01
  -6.77766278e-02 -2.40791291e-01  2.62923211e-01  1.23979941e-01
   4.24487814e-02 -1.69878468e-01 -9.53157991e-02 -4.69545946e-02
   3.07007492e-01 -1.60514742e-01 -1.90676585e-01 -4.89234505e-03
  -3.40613462e-02 -1.76893666e-01  1.57595370e-02  4.16579598e-04
   8.72336179e-02 -2.47322172e-01  9.07113925e-02 -2.01698598e-02
  -4.32861149e-01 -4.46450971e-02  2.25035532e-04  3.31640989e-02
  -1.21990569e-01 -2.47110869e-03 -1.17215281e-02  1.14364646e-01
  -4.22491394e-02 -2.32568353e-01  3.02951396e-01 -1.44586936e-01
   2.45525092e-01 -8.66866037e-02 -3.36449705e-02  1.67316034e-01
   1.42673030e-01  1.58844084e-01 -1.40115887e-01 -4.73708026e-02
   1.62694782e-01 -2.87263781e-01 -2.99134031e-02 -6.33060466e-03
  -1.72313526e-02  2.78627515e-01 -9.13516879e-02 -3.01758677e-01
  -8.18675160e-02 -2.94705182e-02 -9.41753164e-02 -2.74463087e-01
   9.37037095e-02 -2.61163175e-01 -2.54213691e-01  2.71277249e-01
   1.11107968e-01  1.15010075e-01 -1.95244327e-01 -2.29351334e-02
  -1.10299923e-01  2.76780903e-01 -6.61325604e-02 -1.53996542e-01
  -4.48656408e-03  3.11132930e-02 -1.19536646e-01  4.19756889e-01
  -9.48995575e-02  5.52399904e-02  2.10488290e-01 -1.82410553e-01
   2.79245470e-02  1.34081617e-01 -9.09991860e-02 -4.28988993e-01
   6.38803318e-02  3.59260626e-02 -1.69943660e-01  1.92939043e-02]]

Final Loss: 0.0003
Distance Metric: 10.7742
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 2172

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.00020592 -0.00053815  0.00021744 -0.00057788  0.00051705 -0.00042593
  -0.00054076 -0.00046916  0.          0.00026968  0.          0.00061334
   0.          0.          0.00035065 -0.00032213  0.00063919  0.00026694
  -0.00019581  0.00021596  0.          0.         -0.0002115  -0.0002859
   0.00026755  0.          0.          0.00048695  0.00036316 -0.00035799
  -0.00034202 -0.00025469  0.00012223 -0.00047927  0.00016192  0.0003382
   0.00039843 -0.00016414 -0.0001809  -0.0003416  -0.00036404 -0.0003303
  -0.00032595  0.00049572 -0.00029451  0.00084763 -0.00058841 -0.00097095
  -0.00028797  0.          0.00036012 -0.00043552  0.00064911 -0.00025275
   0.00047384  0.00011806  0.00028531  0.          0.00045323  0.00033327
   0.00019789 -0.00044022  0.0004932   0.00015475 -0.00058825  0.00036394
  -0.00033403  0.         -0.00039835  0.00031565 -0.00040829  0.00042489
   0.00016534  0.0002572   0.00027679  0.         -0.0003144   0.00026323
   0.0006097  -0.00062368  0.         -0.00060636  0.00074349 -0.00053346
  -0.0004284   0.         -0.00065257  0.00027444  0.00044553 -0.00047366
  -0.00027595  0.00017844  0.         -0.00026211  0.          0.00018573
   0.00030408  0.000841    0.         -0.00038598 -0.00020233 -0.00056934
   0.000602   -0.00035002 -0.00035541  0.          0.00057575  0.00012399
   0.0003731  -0.00021325  0.00026012  0.         -0.00036595 -0.00036756
   0.         -0.00037756 -0.00023689  0.          0.         -0.00011563
  -0.00056419  0.00034393 -0.00016815  0.00038147 -0.00026314 -0.00034137
   0.00013119  0.00083125]]

Final Loss: 0.2551
Distance Metric: 7.0282
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 880

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 2.72738069e-01 -2.92831119e-02 -2.81289607e-01 ...  1.96139783e-01
   7.72412643e-02 -4.59715009e-01]
 [ 2.43299827e-02  3.47844332e-01  2.09616646e-01 ... -4.39024270e-02
   1.28605291e-01 -2.05443855e-02]
 [-7.18963682e-04  3.99266661e-04  8.13255610e-04 ...  1.24707678e-03
   2.07424338e-04  5.62843343e-04]
 ...
 [-2.46837991e-03  1.10554486e-03  2.59872386e-03 ...  3.96513101e-03
   1.03884877e-03  2.71911430e-03]
 [-7.36981537e-03  3.34700383e-03  7.83369411e-03 ...  1.19666485e-02
   2.97731487e-03  7.77019421e-03]
 [ 2.11881823e-03 -9.47902387e-04 -2.21104780e-03 ... -3.36479489e-03
  -9.06418543e-04 -2.32418068e-03]]
layers.1.weight: [[-0.00028126 -0.00034855 -0.00104339 ... -0.00017637  0.0005031
   0.00101662]
 [-0.00083797 -0.00101662  0.         ... -0.00124069 -0.00067758
   0.00062991]
 [-0.00326574 -0.00371599 -0.00050024 ...  0.00088705 -0.00051139
   0.00017274]
 ...
 [-0.00044231 -0.0005321   0.         ...  0.00093653  0.00064891
  -0.0012077 ]
 [ 0.00012281  0.         -0.00051181 ... -0.0002626  -0.00048786
  -0.00118651]
 [ 0.00110161  0.00124747  0.00115275 ... -0.00032499  0.00059697
  -0.00060671]]
layers.2.weight: [[ 2.6112972e-04  6.7237922e-04  2.3484891e-03 -2.0432548e-04
   8.1929757e-04  1.4387590e-03 -1.0682369e-03  1.2333172e-04
  -3.1741569e-04  1.5706397e-04  5.0954073e-04  2.3010620e-03
  -1.4662961e-03  2.1997586e-04 -1.0738150e-03 -3.9002139e-04
  -1.1377779e-03 -2.7771187e-03  6.3027960e-04  1.1644405e-03
  -2.0199590e-03  1.0943506e-03  2.0456698e-04  0.0000000e+00
  -5.5198779e-04 -1.1689311e-03  2.4389017e-03  1.2269238e-03
  -1.6946676e-04  0.0000000e+00 -1.3100760e-03 -1.3988030e-03
  -1.4455852e-04 -1.0753002e-03 -1.2030385e-03  1.9911148e-03
  -5.7204943e-03 -2.2933660e-03 -2.2678049e-03 -1.0044686e-03
  -2.7165600e-04  1.0826847e-03 -5.2167947e-04  2.5114068e-04
  -7.0743228e-04  1.0756304e-03 -1.2890646e-03 -2.3991149e-03
   0.0000000e+00 -2.9185386e-03  6.6426786e-04 -9.4167376e-04
   5.4510252e-04 -1.2601510e-03  3.8007437e-04  1.2940628e-03
  -2.0746139e-03  1.1152752e-03  1.7106001e-03  2.7408581e-03
  -6.6008605e-04 -2.7987445e-03  0.0000000e+00  6.8107608e-04
  -1.2344754e-03  3.3147237e-04 -2.1870413e-03  6.9333444e-04
   1.4547737e-03  1.3138390e-03  6.2153046e-04  0.0000000e+00
   0.0000000e+00  5.4253399e-04  1.2485832e-03 -1.6688497e-03
  -2.6568116e-04  0.0000000e+00  2.4083767e-03 -2.8424102e-03
   5.6002679e-04  5.1456591e-04 -1.6216604e-03 -8.3655003e-04
   2.6938485e-04  2.0633342e-03  1.2610012e-01  2.0128874e-04
  -1.2286092e-04  1.4471213e-03 -1.1938879e-03 -3.2115185e-03
   1.6877919e-03  1.3910486e-04 -3.0208626e-04 -8.4296323e-04
   2.1184349e-04  3.4568450e-04  3.7618284e-04 -1.4293690e-04
  -1.2841339e-03 -3.4654220e-03  0.0000000e+00  2.0370218e-03
  -2.5276392e-04 -8.6471811e-04 -2.8587659e-04  7.4773066e-04
   0.0000000e+00 -2.4400703e-03  0.0000000e+00  4.0181764e-04
   1.3406848e-03 -9.4884174e-04 -4.7420210e-04  6.0303207e-04
   8.1679114e-04  1.0961111e-03  6.2372087e-04 -3.6908220e-04
   2.7463940e-04  1.2770069e-03  1.4589601e-03 -1.8876670e-03
   1.3394040e-04  3.7545315e-04  0.0000000e+00 -8.1830996e-04]]

Final Loss: 0.2521
Distance Metric: 10.3372
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 559

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-5.1967736e-02  6.6015765e-02 -1.8227296e-02 ... -2.3393840e-02
   2.6312780e-02 -6.0004219e-03]
 [-1.8622645e+00  2.0693197e+00 -6.3222253e-01 ...  7.0212477e-01
  -7.6549488e-01  2.4058212e-01]
 [-4.4648546e-01  5.1534587e-01 -1.6264059e-01 ... -1.9909301e-01
   2.2863108e-01 -6.3478716e-02]
 ...
 [ 3.3750269e+00 -3.6386623e+00  1.1496414e+00 ...  3.0239069e-01
  -3.5138693e-01  1.0588398e-01]
 [-7.0072301e-02  8.5635342e-02 -2.4175193e-02 ... -2.9688414e-02
   3.0212281e-02 -2.8944018e-03]
 [-2.7028453e-02  3.6170885e-02 -9.5605049e-03 ... -1.3487073e-02
   1.6261812e-02 -5.1697241e-03]]
layers.1.weight: [[ 0.00589953 -0.08741973 -0.08959592 ... -0.12253856  0.0078236
   0.00302084]
 [ 0.01944706 -0.24493288 -0.25237468 ... -0.34400895  0.02495323
   0.01114722]
 [-0.00395276  0.03635129  0.03764322 ...  0.05198222 -0.00478652
  -0.00267087]
 ...
 [-0.00266002  0.01259308  0.01353821 ...  0.01874574 -0.00302044
  -0.00214011]
 [ 0.00817549 -0.11229349 -0.11533226 ... -0.15769362  0.01068728
   0.00442402]
 [-0.00432107  0.04282632  0.04422388 ...  0.06104815 -0.00530329
  -0.00282526]]
layers.2.weight: [[ 0.33299935  0.9431543  -0.14463107  1.081522    1.331801    0.48918527
   0.58816224  1.411583    0.5112979   0.6444693  -0.13106187  0.6976947
   0.44234505  0.35869727 -1.8550047   1.1522284   0.5094546  -0.05533713
   0.10491677 -0.9620891  -0.9539911   1.0250406  -0.6286175  -0.30779076
   1.0978384  -1.3292704  -0.41032848  0.39645895  1.0849644   0.57583493
  -1.9893628   0.88236743 -0.38585374 -0.15683033 -0.04167755 -0.5353121
   1.3223059  -0.6135251   0.62689173 -1.6812878  -1.315098   -1.0117954
   0.20158961  0.20758806 -1.0468903  -2.270811    0.0161598   0.53626114
  -0.6313374  -0.4963925  -1.345679    0.33130074  1.3764762   1.7715136
   0.4053522  -0.31889376  1.7585603   1.3466794   0.57405907  0.5328774
   1.3656533   0.3024021   1.8797961  -1.4417665   1.3387994   2.077475
   0.48027596 -0.56837225 -0.614864   -0.13015749 -1.3369964  -0.67230344
   0.07751054 -1.6061898   1.3165323  -0.15236641 -0.7994726   0.07239255
   0.78383136  0.9952665   0.692736    0.3000618  -1.2326338   1.4381663
   0.01356216 -0.86765474  1.6285983  -2.115836    1.5801635   0.29787567
  -0.499263   -0.847099    0.5451935  -0.39127225 -1.2031451  -1.1821481
   1.4386183   0.4459031  -0.4597395   0.02343087 -0.6456046  -2.0152633
  -0.7996494  -0.6998685   0.1686074  -0.82492906 -1.2428402  -0.7329826
   0.00306893 -0.97735006 -1.4259908  -0.8538964   1.2738699  -0.08185022
   2.046646    0.99982363  1.5473919   0.85294497 -0.7342463  -1.076491
   0.67720884  1.0991693   1.0492084  -1.8743252  -0.9277451  -0.05419408
   0.42965937 -0.1692901 ]]

Final Loss: 3.3775
Distance Metric: 38.0147
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 1000

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.00012961 0.         ... 0.00012046 0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.00011191]
 ...
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]]
layers.1.weight: [[0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 ...
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.00011168 0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]]
layers.2.weight: [[-0.00050817  0.00021535  0.00137101  0.          0.00039215  0.00025648
   0.00029693 -0.00119895 -0.00068547  0.00025464 -0.00121396 -0.00046682
   0.00073458 -0.00020232 -0.00080837 -0.00085492 -0.00036588 -0.00063299
  -0.00033563 -0.0006017   0.         -0.00088598 -0.0003309   0.00069939
  -0.00018187  0.00019958  0.          0.0001426  -0.00078313 -0.0003557
  -0.00099427 -0.00062412  0.00020276  0.00046905  0.00129196 -0.00039542
   0.00069449 -0.00049866  0.00059649 -0.00072555 -0.00022726  0.
  -0.00039139 -0.0001923   0.00019677 -0.00013758 -0.00125856  0.00121524
   0.00099462  0.00078872 -0.00050427 -0.00048573  0.         -0.00026329
  -0.00077467  0.         -0.0002427  -0.00032969 -0.00021139  0.00032504
  -0.00043456 -0.00027267  0.00044879 -0.00035261  0.          0.0005042
  -0.00020096  0.00032444 -0.00036473  0.         -0.00041282 -0.00010534
   0.00075058  0.00081345  0.00017355 -0.00060782 -0.00042619  0.0002263
   0.0006872  -0.00041753 -0.00060099  0.00075558  0.00063995 -0.00010468
  -0.00025671  0.00028967  0.         -0.00035331  0.000499    0.00090611
   0.         -0.0003626   0.         -0.00144278 -0.00064168 -0.0001186
  -0.00032896 -0.00013945 -0.00032049  0.00064232 -0.00095624  0.
   0.00017738  0.          0.          0.00057382  0.00037412  0.00059625
   0.          0.00079072 -0.00113597  0.          0.00038818  0.
  -0.00047207  0.00070683  0.00052745 -0.00012029 -0.00037393 -0.00030795
  -0.00068957  0.0005276   0.00023296  0.          0.00030314 -0.00058438
  -0.00015902 -0.00108851]]

Final Loss: 5.0152
Distance Metric: 7.0333
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 827

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.40643927  0.01828395  0.33275035 ...  0.15263799 -0.04972763
   0.29732788]
 [-0.54514754  0.34598124  0.12120029 ...  0.22047928 -0.10351737
  -0.07430764]
 [-0.04612331  0.18308696 -0.02397662 ... -1.2229649   1.4533787
  -0.32715762]
 ...
 [ 0.05701713 -0.08159224  0.02603516 ...  0.0383404  -0.05374462
   0.01816377]
 [ 0.3072491  -0.25351503  0.1280559  ... -1.1763391   1.1835828
  -0.3524892 ]
 [ 0.05977229 -0.07222313  0.02875043 ...  0.0407176  -0.05095238
   0.02660871]]
layers.1.weight: [[-1.3867067e-02  1.0552274e-02  3.9440293e-02 ...  2.2677924e-03
   6.1627403e-03  2.2414480e-03]
 [-2.3429117e-03  1.6987248e-03  6.2302435e-03 ... -2.9914372e-04
   1.1469728e-03  1.0752614e-03]
 [-7.3596048e-03  4.9586981e-03  1.9798543e-02 ...  7.2649732e-04
   2.7394737e-03  5.6491717e-04]
 ...
 [ 1.3639873e-01 -1.0548453e-01  5.9025955e-01 ...  7.7074312e-02
   4.3303087e-01  7.4814253e-02]
 [-1.8000235e-01  1.6145036e-01 -4.4859499e-01 ... -7.7805467e-02
   2.1420466e-02 -6.7797199e-02]
 [-1.1519849e-02  8.5619288e-03  3.1046335e-02 ...  3.3828660e-03
   5.2838381e-03  1.2898314e-03]]
layers.2.weight: [[ 9.13060643e-03  1.49232906e-03  4.56310064e-03 -8.75248760e-03
   8.02130345e-03 -8.21574405e-03 -7.85734679e-04 -3.62074047e-01
   8.16464052e-03  1.01564324e-03  3.15834675e-03  5.71094314e-03
   2.13097222e-03 -2.25235894e-01  7.74277560e-03  8.98337085e-03
   5.84948482e-03  3.28482990e-03  2.29309383e-03  2.24222359e-03
  -2.54417211e-01  4.66534495e-03  5.39925927e-03 -6.78793015e-03
   2.86693901e-01  2.11245611e-01  1.70460634e-03  7.72435591e-03
   9.11864825e-03  8.80764890e-03  1.64328885e-04  2.47411504e-01
  -2.57617593e-01 -5.94198378e-03 -7.05303508e-04  5.86018339e-03
  -7.94028118e-03 -1.39850727e-03 -1.58169935e-03 -3.09449941e-01
  -1.00261308e-02  1.11744483e-03  1.29296212e-03 -4.02331471e-01
  -1.14000938e-03 -3.08325529e-01 -2.12236843e-03  4.71929908e-01
   1.42051755e-02 -2.60616117e-03 -2.95516819e-01 -3.52446705e-01
   3.27329501e-03 -8.45733471e-03 -1.11302175e-02  3.90469399e-03
   2.81504095e-01  4.52812672e-01 -7.54419668e-03  3.00479541e-03
   6.74953172e-03 -5.82765695e-03 -3.10407817e-01  1.21413730e-03
  -6.64917426e-03  5.56193199e-03 -3.14908653e-01 -4.24596202e-03
   6.76385500e-03 -5.41374320e-03 -7.18782016e-04  3.47896785e-01
   7.98712671e-03 -9.17079020e-03 -3.03947598e-01  1.23228006e-01
   4.71230317e-03  2.66135156e-01 -9.23177414e-03  1.07039139e-02
   8.48878641e-03 -4.87868814e-03 -3.29657137e-01  3.04768607e-03
   0.00000000e+00  5.84588479e-03  8.43424257e-03  2.33117631e-03
  -6.09407201e-03 -3.50340515e-01 -2.27752388e-01 -2.25988659e-03
  -4.89671202e-03 -4.40346589e-03 -7.19347829e-03 -7.53029389e-03
  -3.58878314e-01  1.73327457e-02 -2.00032769e-03 -2.02182028e-03
   3.76813905e-03  7.57183367e-03  1.90695468e-02  4.34926990e-03
  -7.26980250e-03  7.34815607e-03 -1.24722056e-03 -2.60601752e-03
   4.40595061e-01  5.09404251e-03 -8.17133579e-03  4.90222931e-01
   1.87935308e-03 -4.94470727e-03  3.21987063e-01 -5.86548030e-01
   5.31314034e-03  6.04737503e-03  2.76272088e-01  2.23937110e-04
  -1.09581249e-02 -3.71261826e-03 -6.40902156e-03 -1.12664711e-03
  -6.20964402e-03 -3.30490291e-01  4.05640721e-01  7.25072296e-03]]

Final Loss: 3.6129
Distance Metric: 25.7042
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 584

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00508004 -0.00918655  0.002301   ...  0.00412253 -0.00249198
   0.00038398]
 [ 0.00503866 -0.00912604  0.00214449 ...  0.00240816 -0.00077922
  -0.00014919]
 [ 0.00472923 -0.00858587  0.00172666 ... -0.00120176  0.00284787
  -0.00124519]
 ...
 [ 0.00481281 -0.00871452  0.00198273 ...  0.00152751  0.00011866
  -0.00039838]
 [ 0.00452354 -0.00821308  0.00156413 ... -0.00219149  0.00385256
  -0.00153023]
 [ 0.00433723 -0.00788487  0.0013684  ... -0.00367364  0.00534977
  -0.00197806]]
layers.1.weight: [[0.00083186 0.00091371 0.00113209 ... 0.00099797 0.0012167  0.00131868]
 [0.00069632 0.00076961 0.000965   ... 0.00084492 0.00104042 0.00113172]
 [0.00084872 0.00093178 0.00115324 ... 0.00101731 0.00123905 0.00134247]
 ...
 [0.00092853 0.00101689 0.00125292 ... 0.00110824 0.0013445  0.00145492]
 [0.00081685 0.00089785 0.00111369 ... 0.00098124 0.00119728 0.00129816]
 [0.00074513 0.00082158 0.00102486 ... 0.00089999 0.00110358 0.00119865]]
layers.2.weight: [[-0.10744803 -0.10006686 -0.10840781 -0.10394327 -0.10149395 -0.10387927
  -0.10645581 -0.10379282 -0.1023805  -0.10284976 -2.460437   -0.1074674
  -0.10476264 -0.10102446 -0.10133587 -0.10411409 -0.10347243 -0.10244802
  -0.10492656 -0.10576671 -0.10572985 -0.10026546 -0.1053609  -0.10015995
  -0.10143972 -0.10120528 -0.10055321 -0.1048874  -0.10650185 -0.09846478
  -0.10056935 -0.10022397 -0.10784813 -0.10576389 -0.10221113 -0.10366891
  -0.1009914  -0.10844248 -3.7500718  -0.10859288 -0.10037522 -0.10391182
  -0.10461442 -3.2185395  -0.10489828 -0.10314475 -0.09995804 -0.09657695
  -0.10539467 -0.10797101 -0.10739018 -0.0989649  -0.11469693 -0.09827167
  -0.10522969 -0.10014161 -0.10469317 -0.10842564 -0.09808568 -0.10847488
  -0.09876515 -0.10191885  2.5625944  -0.10351755 -0.10607009 -0.0992469
  -0.10319763 -0.10415021 -0.11382138 -0.09961084 -0.10923397 -0.10684048
  -0.10072095 -0.10415018 -0.09632047 -0.10206467 -0.10725442 -0.10341544
  -0.10569236 -0.10791156 -0.09849998 -0.10130946 -0.10394578 -0.10577775
  -0.10033993 -0.10529719 -0.10635825 -0.10402343 -0.10713353 -0.10482885
  -0.09870508 -0.10372619 -0.10268441 -0.10739666 -0.10032794 -0.10116956
  -0.10650898 -0.09842237 -0.10392462 -0.1033451  -0.10534389 -0.10262568
  -0.10071585 -0.10315318 -0.10565593 -0.10601205 -0.10060673 -0.12042078
  -0.10238148 -0.09601272 -0.10165279 -0.10150553 -0.10061131 -0.1006088
  -0.10017519 -0.09633526 -0.10214385 -0.10018626 -0.10392788  7.466829
  -0.11144843 -0.10168851 -0.10503862 -0.10013621 -0.10367569 -0.11302693
  -0.10661919 -0.10267528]]

Final Loss: 0.1881
Distance Metric: 33.9078
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 1531

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 ...
 [0.         0.00013611 0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.00010481 0.         ... 0.         0.         0.        ]]
layers.1.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [-0.00016273  0.          0.         ...  0.          0.
   0.        ]
 ...
 [ 0.         -0.00010989  0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[-0.00033646 -0.000167    0.00084309 -0.00051244 -0.00058818 -0.00080839
   0.00034636  0.00065777  0.00064899  0.00012167  0.         -0.00041471
  -0.00026532 -0.00035737  0.00039804  0.00086663  0.00080636 -0.00060541
  -0.00018174  0.00115076  0.          0.00037428  0.00037683  0.00100647
  -0.00112269  0.          0.0011775   0.00042892 -0.0002291  -0.00010763
  -0.00083666 -0.00027317  0.         -0.00045474  0.          0.00040925
   0.00054951  0.00096538  0.          0.00065687  0.00054549  0.0007995
   0.00012146  0.00031857  0.00011696 -0.00058234 -0.00031186 -0.00040255
  -0.00035146  0.          0.00015281  0.00015814  0.00065759  0.00014798
  -0.00050377  0.00042515 -0.00034149  0.00032881 -0.00065622 -0.0003411
   0.00044268 -0.00052651  0.0004345   0.00012665  0.         -0.00016533
  -0.00019983  0.00090111  0.00020767 -0.00054754  0.0004613   0.00111215
  -0.00011868  0.          0.00015917 -0.00027902 -0.00032602  0.0004092
  -0.00060253  0.          0.00038945  0.00066833 -0.00026127 -0.00075644
   0.00062707 -0.00057432 -0.00054666 -0.00024474 -0.000902    0.
   0.00020413 -0.00079884  0.00150234  0.00024131 -0.0012721  -0.00022656
  -0.0005935   0.00027687 -0.00029533 -0.00054141 -0.00062597  0.000921
  -0.00025151  0.00080325 -0.00018135  0.00043804 -0.00051224  0.00018167
   0.00050214  0.00087949  0.          0.00088627 -0.00058126  0.00035029
   0.          0.000292    0.00010564  0.         -0.00038156  0.0002503
   0.00030936  0.00039707  0.00151142  0.00053834 -0.00025411  0.00029093
  -0.00047763 -0.00030358]]

Final Loss: 0.3663
Distance Metric: 7.0336
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 825

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.00350184  0.0002954   0.02372018 ...  0.00131668  0.00018232
   0.00813468]
 [ 0.00881799  0.00051314  0.06118221 ...  0.00040224  0.
   0.00189632]
 [ 0.00578602  0.00034343  0.04146162 ... -0.00276952  0.
  -0.01786584]
 ...
 [-0.00123028 -0.00035955 -0.00772313 ...  0.00292173 -0.00017181
   0.01827345]
 [-0.00370237 -0.00021695 -0.02707048 ...  0.00300089  0.
   0.01932502]
 [ 0.0104122   0.00063141  0.07141153 ...  0.00199448  0.
   0.01188386]]
layers.1.weight: [[-0.00013639 -0.0002152   0.         ...  0.          0.
  -0.00031987]
 [-0.00138344 -0.00215824  0.         ... -0.001035   -0.00069885
  -0.00324471]
 [ 0.00011666  0.00018408  0.         ...  0.          0.
   0.00027361]
 ...
 [ 0.00047074  0.00074189  0.         ...  0.00033157  0.00021677
   0.00110404]
 [ 0.          0.          0.         ...  0.          0.
   0.00011938]
 [ 0.          0.          0.         ...  0.          0.
   0.00013647]]
layers.2.weight: [[ 7.9388833e-03  8.1105940e-02 -6.7902920e-03  3.9507258e-03
   2.7638928e-03  1.0905056e-02  3.9254598e-02  6.1377538e-03
  -3.3232506e-02  2.6919747e-02 -1.4860697e-02 -2.5785118e-03
  -9.1306670e-03 -1.4468405e-03  1.2065230e-01 -7.7732213e-02
   1.6422683e-02 -1.8497154e-03  2.1207837e-02 -7.9238396e-03
   2.2299517e-02  6.8278862e-03 -6.7190160e-03 -1.8774953e-02
   2.2953877e-02  1.1524583e-02  1.8049551e-02  1.2754801e-03
  -2.5174377e-02 -1.0781230e-02  2.3173105e-02 -4.7861272e-03
   1.1573198e+00  2.6729440e-02 -8.4959278e-03 -9.4308043e-03
  -5.1894584e-03 -1.7054204e-02 -1.5373456e-01  1.1713724e-02
  -2.4365868e-01 -1.7616166e-02  7.6249436e-02 -1.4944017e-02
   1.4993464e-02 -8.6970413e-03  2.1343756e-02  1.3753525e-02
  -1.1075654e-02  9.7470038e-02 -9.4130158e-04 -1.8597312e-02
   5.0228931e-02  6.9464371e-03 -7.1376450e-03 -2.7265916e-02
   1.3156814e-03 -9.0842508e-03 -5.1166341e-03 -1.9274419e-02
   7.7381367e-03 -1.8580558e-02 -1.7420161e-03  4.2622769e-03
  -4.1146381e-03  2.2149442e-03 -2.7778354e-03 -1.2448904e-02
   2.3924142e-01  1.0395091e-02  5.1614228e-03 -1.3196728e-02
   1.2612529e-02  7.8585856e-03 -4.9859830e-03  5.6376630e-03
   4.1614873e-03  2.6885973e-02  3.5471868e-02  7.4566263e-03
   1.6123064e-02  8.6761583e-03 -1.4101961e-02  6.4721354e-03
  -5.7445276e-01  1.9435165e-02  5.0476310e-03 -3.5440005e-02
   1.7751120e-02 -2.0000210e-02 -3.4094846e-03 -6.3289419e-02
  -6.5628360e-03 -1.1414889e-02 -2.1863177e-02 -1.7212557e-02
   1.5250459e-02 -8.9199342e-02  0.0000000e+00 -2.4030991e-04
   2.1101039e-02  4.1594431e-03  6.3725919e-01 -1.7309306e-02
  -1.7146265e-02 -5.2194283e-03  1.9669762e-02  4.5851455e-03
   6.8382494e-04  3.3921178e-03  3.7564203e-02  2.8154099e-02
   5.8422792e-03  4.5173850e-02 -2.8199306e-02 -7.9797383e-04
   4.2116176e-03 -1.6471362e-02  8.4661413e-03 -3.4834664e-02
  -1.4332065e-02  3.3943724e-02 -1.8323487e-02  4.3689050e-03
   1.1815583e-02 -2.7419342e-02 -2.9625592e-03 -3.3867811e-03]]

Final Loss: 0.0002
Distance Metric: 6.8082
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 1923

================================================================================

