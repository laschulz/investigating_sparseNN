Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.00557016  0.00615365 -0.00183811 ... -0.0051498   0.00573348
  -0.00164973]
 [-0.01388974  0.01534333 -0.00458291 ... -0.01283659  0.01429701
  -0.0041121 ]
 [-0.03630391  0.04008797 -0.01196622 ... -0.03348314  0.03739475
  -0.01073428]
 ...
 [ 0.02618602 -0.02892151  0.00863634 ...  0.02417711 -0.02695981
   0.00774669]
 [ 0.00464016 -0.00512588  0.00153126 ...  0.0042883  -0.00477463
   0.00137302]
 [ 0.00293573 -0.00324287  0.00096886 ...  0.00271289 -0.0030207
   0.00086847]]
layers.1.weight: [[-0.00102156 -0.00245895 -0.00638981 ...  0.00447756  0.00073828
   0.00044446]
 [-0.00011345 -0.0001648  -0.00030514 ...  0.          0.
   0.        ]
 [ 0.0005836   0.00157099  0.00427163 ... -0.00319423 -0.00062531
  -0.00042348]
 ...
 [-0.00082634 -0.00196394 -0.00507527 ...  0.00352624  0.0005666
   0.00033401]
 [ 0.00347876  0.00870057  0.0229815  ... -0.01649977 -0.00291476
  -0.00184729]
 [ 0.00133471  0.00342253  0.00913273 ... -0.00665344 -0.00122166
  -0.00079489]]
layers.2.weight: [[ 0.11300876  0.00409357 -0.07749781  0.20922121  0.03875556 -0.06340041
  -0.16737331  0.10892788 -0.1203027   0.02707267 -0.1411905   0.01943986
  -0.05490299 -0.16219085 -0.29446664  0.23277546 -0.20010255 -0.06203232
   0.07654913  0.3171536  -0.30431554  0.28420272  0.00254103  0.22041366
   0.07899895 -0.00987191  0.07515598 -0.15071352  0.12361951 -0.05282494
   0.05846604  0.19083649  0.05145606 -0.04257499  0.23011437  0.14200753
   0.14249119  0.14375539  0.03275111  0.29000074 -0.1101881  -0.03255259
  -0.14107297 -0.13078359 -0.12287037 -0.1952939  -0.06574348 -0.08356353
  -0.15001212  0.04844084  0.10749169  0.08846866 -0.10693692 -0.07445401
   0.02121981 -0.13435335 -0.05146487 -0.06091154 -0.02744811  0.04952505
  -0.00300421  0.1693402  -0.11003586  0.21662478 -0.30864954  0.15886259
   0.06838401 -0.29710186  0.12921378 -0.0703097  -0.28058788  0.15446904
  -0.30123276  0.08646112 -0.08768842  0.15152887 -0.24474454  0.3825959
   0.04168371  0.04663023 -0.12757586 -0.21166342  0.14425398  0.13601145
  -0.0534678   0.13767019  0.08290514  0.2783438  -0.13823652 -0.0235338
   0.1210355  -0.1200684   0.01618076  0.0716292  -0.23916109  0.11216173
   0.11806588 -0.22147512  0.03683756  0.29475668  0.31914505 -0.28907776
   0.04774196  0.05387654  0.12643641  0.06906117  0.23155858 -0.39831063
   0.18391871  0.14184022 -0.05718473  0.37565985  0.12457941  0.21053287
  -0.43398312  0.25856045 -0.19164309 -0.12309836 -0.08501709  0.17374295
  -0.19224724 -0.03851363 -0.13313545 -0.1558441  -0.15148595  0.08943349
  -0.41413006 -0.16413866]]

Final Loss: 0.0003
Distance Metric: 10.7072
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1655

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.          0.0002889  -0.000616   -0.00033762 -0.00073547 -0.00035425
   0.00043134  0.         -0.00037117 -0.00036091  0.0001813   0.00052836
   0.00018502  0.00018434 -0.00054303  0.          0.00027743  0.00058168
   0.00042022 -0.00042659  0.         -0.00091729  0.         -0.00015205
   0.00061453  0.         -0.00038201 -0.00023328  0.00061144 -0.00049535
  -0.00058895 -0.00027714 -0.0002902  -0.00010628 -0.00046134 -0.00017146
  -0.00011823 -0.00010099  0.          0.00088581 -0.00025017  0.00011156
   0.00045086  0.          0.00012284  0.         -0.00043052  0.00031791
   0.          0.00083255  0.00032414 -0.0002344   0.00010875  0.
   0.0002185   0.00035406 -0.00046274 -0.00031096  0.0001034   0.00030369
   0.00028261 -0.00049912  0.00044926 -0.00020654 -0.00042626 -0.00022707
  -0.00018776 -0.00010289  0.00078322  0.00065138 -0.00039987 -0.00040698
   0.          0.00047355 -0.00017011  0.         -0.00056955 -0.0006288
   0.          0.          0.00048016 -0.00023811  0.00020949  0.00015695
   0.00027047  0.00045447  0.          0.00028064  0.00011315 -0.00010556
  -0.00025524 -0.00019956  0.          0.         -0.00025941  0.00024205
  -0.00010747  0.00024909 -0.00043667  0.00010419  0.00034107  0.00039121
   0.          0.          0.          0.         -0.00014276  0.00030323
   0.          0.00017545  0.          0.         -0.00010563  0.00024495
  -0.00024266  0.00039241 -0.00026978  0.00021761  0.00023203  0.
  -0.00012215  0.00016923  0.00024391  0.          0.00035501  0.
  -0.00015687  0.00010582]]

Final Loss: 0.2550
Distance Metric: 7.0272
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 896

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 1.35066360e-03  2.01118994e-03  1.88230842e-04 ...  1.39334545e-04
   4.77887865e-04  1.46438589e-03]
 [-1.29313162e-03 -1.86444446e-03 -7.43491226e-04 ... -4.69101244e-04
   0.00000000e+00 -1.54795311e-03]
 [ 3.57474876e-03  5.27074514e-03  9.62781371e-04 ...  6.51136914e-04
   9.45358654e-04  4.00373852e-03]
 ...
 [-3.63818865e-04 -5.48668148e-04  0.00000000e+00 ...  0.00000000e+00
  -1.74075860e-04 -3.81152407e-04]
 [ 1.01364926e-01 -1.95292085e-01 -2.78957188e-02 ...  4.15019155e-01
  -8.64016056e-01  2.16284275e-01]
 [-1.86960632e-03 -2.76658335e-03 -4.58075723e-04 ... -3.10279400e-04
  -5.25123847e-04 -2.08049221e-03]]
layers.1.weight: [[ 0.00029212  0.00014774  0.00028575 ...  0.00036571  0.00039366
  -0.00028692]
 [-0.00058816 -0.0002914  -0.00017303 ...  0.00027154 -0.00445769
  -0.00039063]
 [-0.00025836 -0.00050028 -0.00029161 ... -0.00043709  0.0041254
  -0.00035664]
 ...
 [ 0.00036636  0.0002412   0.00063805 ... -0.00037181  0.00586417
  -0.00034679]
 [-0.0004013   0.0003156  -0.00023142 ...  0.         -0.01730154
   0.        ]
 [ 0.00016423  0.          0.00079919 ... -0.0003069   0.00947481
   0.        ]]
layers.2.weight: [[-0.00037068  0.00412361 -0.00381387  0.00494879 -0.00687277 -0.00205081
   0.0081378  -0.00823755 -0.00679044 -0.01413176 -0.01532236  0.00191239
  -0.00264109  0.00053101  0.00338442 -0.00777155  0.00767244 -0.00678256
  -0.00386085  0.          0.00067989 -0.00510204 -0.01005425  0.00700819
  -0.00030042  0.00396007 -0.0019944   0.0010956   0.00041099 -0.00313356
  -0.00169434  0.00954454 -0.00062102 -0.0073839   0.00928431 -0.0012283
   0.00155758 -0.00136849 -0.01778045  0.0039025   0.00528981  0.01025915
  -0.00780829  0.00968275  0.0056854  -0.00509202  0.0063085  -0.00153668
   0.00511528  0.00186753 -0.00199691  0.          0.00341877 -0.01326753
   0.00090565 -0.00169041  0.17310244  0.00244128 -0.00026931 -0.00312857
   0.00353846  0.0056369  -0.00630166  0.00602918 -0.00148212 -0.00332832
  -0.01160051 -0.00604189  0.00232206  0.00078864 -0.00580759  0.
  -0.00645078  0.00786603  0.00374311  0.01031919 -0.00535939 -0.00618998
  -0.0037475  -0.00381327  0.00336387 -0.00414269  0.00032044  0.00181565
   0.00949626  0.00593272  0.00112668  0.01318032 -0.00174372  0.00192188
   0.00540751 -0.00558569 -0.00199823  0.01055585  0.00144209 -0.0106027
  -0.00608951 -0.00435767  0.00725156 -0.00799435 -0.00144599  0.00722173
   0.0002499  -0.00715079 -0.00217443 -0.00719826 -0.0082107  -0.00219437
  -0.0046974  -0.00286449  0.         -0.00183406 -0.0011635   0.0055463
  -0.00204956 -0.00036384 -0.01022277 -0.00364313  0.00040444 -0.00079718
   0.00500279  0.00438925  0.00125627 -0.00489543 -0.00143819 -0.00542012
   0.01557125 -0.00867553]]

Final Loss: 0.2522
Distance Metric: 10.6303
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 645

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.05846807  0.0653671  -0.01047298 ... -0.08853933  0.08539004
  -0.02895159]
 [-0.02435412  0.02645288 -0.00331666 ... -0.04951808  0.05130306
  -0.01814031]
 [-0.0606958   0.06690186 -0.01087904 ... -0.11631627  0.11917435
  -0.04034276]
 ...
 [ 0.00367202 -0.00658529  0.0025259  ... -0.0247783   0.03333151
  -0.01137123]
 [-0.03228959  0.00517911 -0.0666725  ... -0.7387131   0.84874225
  -0.27119035]
 [-0.02789076  0.03057791 -0.00404333 ... -0.05151282  0.05214943
  -0.01844421]]
layers.1.weight: [[ 0.00924177  0.00347592  0.00944815 ... -0.001366    0.05128882
   0.00410833]
 [-0.00458336 -0.00224203 -0.00463316 ... -0.00026519 -0.02110328
  -0.00250195]
 [-0.01386258 -0.00553919 -0.0142384  ...  0.00143797 -0.07583839
  -0.00644699]
 ...
 [-0.02091318 -0.00811026 -0.02180335 ...  0.0025399  -0.12091234
  -0.00948542]
 [ 0.00041451 -0.00052733  0.00043208 ... -0.00132137  0.007295
  -0.00042044]
 [-0.01637361 -0.00646719 -0.01690566 ...  0.00181145 -0.09138875
  -0.00754095]]
layers.2.weight: [[ 0.48497787 -0.1980574  -0.7059535   0.9827819   0.48182154 -0.9646179
   0.5188853   0.26899105 -0.85630053  0.5816411  -0.9266152   0.48848003
  -0.15197985 -0.29080933  0.38868988 -0.83614004  0.48105118  4.359528
  -0.9824768  -0.64223075  0.6916538  -0.66749144  0.37725052  0.9521022
  -1.14216     1.5325643  -0.5122569  -0.5376069  -1.1994928  -0.16834654
  -1.099196   -0.6651042   0.9146707  -1.056315   -1.7009801  -1.2132826
  -1.0427748   1.4346471   0.7187822   0.08812709 -0.46351102  0.09265658
   0.94623184 -0.31096593 -0.24921408  1.1227851   0.27524245 -1.0176789
  -5.0629106  -1.5939728  -1.0394403   0.6119587   0.70661664  0.57253397
   1.0681589  -0.9451764  -0.39003438  0.37082162 -1.3728753  -0.37475774
   0.5622946   0.32101715  1.2679973  -0.9469013   1.056392   -0.32237202
   0.21825828  0.5886824   0.9960026  -1.6692817   1.2824785   1.0932226
   0.21195316  0.11626063  0.5844298  -0.53800875  0.8325414   0.6137379
  -0.18914872  0.59010565  1.0458653   0.9004831   0.9466004  -1.4702286
   0.30951792 -1.4007746  -1.4508536  -0.52080613  0.39206678 -0.96166044
   0.59668684  0.99144566 -0.11682547  0.8376764  -0.33255517 -1.2115586
  -0.54706055  0.07924507 -0.43195796  1.0028803   1.0669994   0.3022404
  -0.16005254  0.6291649  -0.09867642  1.3203269   1.4084196   0.19592197
  -0.4193889  -0.45405507 -0.1630422   0.54916894  1.175293   -0.6524222
   1.1731607   0.31303918  0.35869148  1.0485867   0.85669035 -0.8262229
  -0.3982171  -5.4502     -0.53369427  0.29994035  0.25033832 -1.0774052
   0.08287065 -0.83813745]]

Final Loss: 3.2732
Distance Metric: 38.8071
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1227

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.          0.          0.          0.          0.          0.
   0.00010693  0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.00010438  0.
   0.          0.          0.          0.         -0.00010012  0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.         -0.00013978  0.          0.
   0.          0.          0.          0.00011093  0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.        ]]

Final Loss: 4.9002
Distance Metric: 7.0249
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1092

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.05558842 -0.19898136  0.04114176 ... -0.7559164   0.8143582
  -0.11792107]
 [ 0.06808501 -0.04160336  0.02260759 ...  0.03328099 -0.05404117
   0.01926021]
 [ 0.11321625 -0.2948018   0.17556699 ...  0.49682653  0.0262665
  -0.12276842]
 ...
 [ 0.04017251 -0.02193864 -0.02064228 ...  0.04855069 -0.06624371
   0.00604927]
 [-0.05809415  0.07437658 -0.02560578 ... -0.04729568  0.05185623
  -0.02004894]
 [ 0.05676791 -0.06737257  0.01304969 ...  0.04926431 -0.06136169
   0.01540726]]
layers.1.weight: [[-1.1629620e-04  7.1215763e-04  2.4095292e-03 ...  4.5652510e-04
  -9.7505434e-04  8.2021492e-04]
 [ 1.6063785e-04 -6.2243821e-04 -2.6062250e-03 ... -4.0470299e-04
   9.4701751e-04 -9.4111427e-04]
 [ 0.0000000e+00  3.8258685e-04  1.3991791e-03 ... -2.3994935e-04
  -4.0734335e-04  4.6110584e-04]
 ...
 [ 2.0508595e-04 -1.0506851e-03 -5.4835770e-03 ... -5.0551695e-04
   1.8772924e-03 -1.7116510e-03]
 [-2.0611584e-01 -4.5586348e-02  5.2783865e-01 ... -4.1016214e-02
   3.9455213e-02 -4.1810665e-02]
 [ 1.4447687e-04 -9.9769747e-04 -6.5674148e-03 ... -7.0612394e-04
   1.7893190e-03 -1.8047806e-03]]
layers.2.weight: [[ 1.1842456e-03 -1.3060952e-03  6.4639375e-04  1.8738915e-04
  -7.4207638e-03 -2.3725300e-04  3.8208137e-03 -3.4308047e-03
  -6.2198564e-03  6.6833611e-04  3.3925576e-03  8.7434147e-03
   4.4915560e-04  0.0000000e+00  0.0000000e+00  7.1697062e-01
   1.7343802e-04  1.7816948e-03  3.4922811e-01  3.4507695e-01
   8.8693213e-04 -2.9040772e-01 -3.3835036e-04 -1.4857287e-04
   3.4690970e-03  3.1327200e-03 -2.6735433e-04  2.5969172e-01
   4.2956862e-01 -2.6228896e-03  3.6728674e-01 -7.7856099e-04
  -1.4721103e-01 -2.9579869e-01 -8.9433155e-04 -4.7838967e-04
  -1.2346017e-03 -2.7861983e-01  3.1688288e-01 -2.0330246e-03
  -1.0459162e-03  5.5548190e-03  1.0592348e-03  2.4114695e-01
  -3.2751202e-03 -7.2614814e-04  2.8388256e-03  1.7098709e-03
  -3.0587330e-01  2.4690479e-03 -5.0823744e-03 -2.5440555e-03
   3.3779201e-04 -7.5263963e-03  4.4135493e-01  2.3180374e-03
  -1.2960249e-04  7.2145658e-03  2.2032031e-01 -3.8424775e-03
   4.4510691e-04 -3.4599903e-01  2.9468234e-03  2.5313377e-01
  -2.5621369e-03  6.7611213e-04  1.7284680e-03  6.4469278e-01
   3.7167128e-03  6.6348893e-04  1.5865989e-03 -2.2465177e-03
   6.1107208e-03  2.0710502e-03 -4.5477052e-04  4.2405063e-03
   3.5394160e-03  3.2595654e-03  4.9874778e-03 -6.9949385e-03
  -1.4434250e-03 -2.2562253e-03  2.9425704e-01 -1.1898641e-03
   4.7688982e-03  1.0804360e-03  4.5365035e-03  1.4519003e-03
  -1.9326403e-03 -4.6012700e-01 -3.9176224e-03  4.3869502e-04
  -5.5859890e-03 -2.1270618e-03 -2.4440828e-01  4.1763769e-03
  -2.3470223e-03  7.3437896e-03  1.2930887e-03 -4.6907002e-04
  -3.3824567e-03  5.6183862e-04 -6.1308598e-04  2.2730057e-03
   1.5789300e-04  2.6424544e-02 -5.0414028e-04 -2.9114810e-01
   7.1558735e-04 -2.9343763e-01  1.1871438e-03 -5.6915646e-03
   1.3445165e-03  8.5694892e-03 -3.7463635e-01  4.9099256e-03
  -5.4212245e-03 -2.5496979e-03 -1.0600872e-03 -4.0414031e-03
  -2.9919243e-03  2.6761258e-01  2.0196463e-03 -3.7529829e-01
  -3.6797607e-01 -2.7948571e-03  3.8842571e-01 -3.3709330e-03]]

Final Loss: 3.5081
Distance Metric: 25.5003
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 712

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00757077 -0.0048293   0.00039569 ...  0.00707996 -0.0075851
   0.00424307]
 [ 0.00795944 -0.00507567  0.00042368 ...  0.00805236 -0.00868044
   0.00463326]
 [ 0.00711626 -0.00488374  0.00049979 ...  0.00596129 -0.00630493
   0.00381762]
 ...
 [ 0.00673921 -0.00497577  0.00061219 ...  0.00567664 -0.00598272
   0.00364979]
 [ 0.00708039 -0.00500603  0.00056152 ...  0.00644325 -0.00685482
   0.00395458]
 [ 0.00718391 -0.00497673  0.00052793 ...  0.00641497 -0.00682081
   0.00396969]]
layers.1.weight: [[0.00021778 0.00025742 0.00018053 ... 0.00016138 0.00018995 0.00019342]
 [0.00012013 0.00015473 0.         ... 0.         0.         0.        ]
 [0.00022623 0.00026633 0.00018871 ... 0.00016924 0.00019812 0.00020167]
 ...
 [0.00031032 0.00035469 0.00026882 ... 0.00024731 0.00027914 0.00028306]
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.00026217 0.00030394 0.00022281 ... 0.00020242 0.00023259 0.00023629]]
layers.2.weight: [[ 5.3152476e-02  4.7836129e-02  5.3620003e-02  4.9747884e-02
   5.5511892e-02  6.1297636e-02  6.9455281e-02  6.7393698e-02
   5.9932768e-02  3.8411759e-02  6.2731750e-02 -1.4384131e+00
   6.2459335e-02  5.2986685e-02  4.6987962e-02 -2.2868778e-01
   4.0308438e-02  4.6699420e-02  4.6986453e-02  5.9361774e-02
   5.8994804e-02  4.0609881e-02  5.9180900e-02  5.0405700e-02
   5.1216397e-02  5.4365385e-02  5.4161031e-02  5.6637261e-02
   6.1809450e-02  5.6160249e-02  5.5092506e-02  6.4814709e-02
  -2.6550716e-02  5.0321866e-02  7.5715519e-02 -4.6408381e+00
   6.8977974e-02  6.1145719e-02  6.5671638e-02  6.8507805e-02
   6.9602087e-02  5.0478596e-02  6.7665197e-02  4.4403400e-02
   4.3970179e-02  4.0688671e-02  5.3873274e-02  5.1697802e-02
   6.1660253e-02  4.7830682e-02  5.4278675e-02  5.9026226e-02
   5.8998421e-02  2.8672414e-02  6.0815036e-02  5.0713573e-02
   6.1083052e-02  4.8149318e-02  6.2800027e-02  6.4531848e-02
  -7.5390420e+00  5.3467393e-02  4.7064036e-02  3.5927262e-02
   5.4384511e-02  3.0812960e-02  6.7776144e-02  5.9501082e-02
   5.9557728e-02 -7.8038812e-02  7.3568011e-03  3.6031287e-02
   4.5887142e-02  4.1025277e-02  6.1248973e-02  4.6550393e-02
   4.8722997e-02  5.2469455e-02  5.9139863e-02  5.3316761e-02
   6.3824467e-02  5.6493696e-02 -4.0004621e+00  5.3693097e-02
   5.0116200e-02  5.8501277e-02  5.3915627e-02  5.7783116e-02
   4.2668972e-02  6.1049514e-02  4.2752657e-02  5.3033423e-02
   6.0305737e-02  6.3002415e-02  4.5813441e-02  5.9263546e-02
   5.6115508e-02  6.4162932e-02  4.9121123e-02  5.8329787e-02
   6.3900746e-02  4.4877399e-02  5.6229364e-02  4.6200331e-02
   1.1528887e-01  3.5550032e-02  6.5928034e-02  3.3723254e-02
   5.6633253e-02  6.2223975e-02  6.9211833e-02  4.7951337e-02
   6.1115563e-02  4.0885665e-02  5.3521313e-02  4.6775110e-02
   4.1931804e-02  6.3374840e-02  6.4596817e-02  5.0044894e-02
   4.5129620e-02  5.5017851e-02  4.6907857e-02  6.8932176e-02
   5.7103071e-02  5.8320809e-02  4.4732779e-02  5.5606131e-02]]

Final Loss: 0.1889
Distance Metric: 33.3683
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1530

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.         -0.00011498  0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.        ]]

Final Loss: 0.3674
Distance Metric: 7.0248
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1114

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.01335002 -0.01353832  0.00418998 ...  0.00110928 -0.00207482
   0.        ]
 [ 0.00747959 -0.00793717  0.00222247 ...  0.01155876 -0.01243896
   0.00374742]
 [ 0.00131099 -0.00220405  0.00132455 ...  0.02156202 -0.02148864
   0.00653678]
 ...
 [-0.00441601  0.00474898 -0.00052758 ... -0.01240018  0.01356026
  -0.00462537]
 [ 0.00184763 -0.0015509  -0.00026162 ... -0.00466225  0.00404942
  -0.00119777]
 [-0.00148179  0.00234439 -0.00170842 ... -0.0179947   0.01763143
  -0.0053994 ]]
layers.1.weight: [[ 0.         -0.00012617  0.         ...  0.00018834  0.
   0.        ]
 [-0.00039986 -0.00053308 -0.00011946 ...  0.00079509 -0.00025736
   0.        ]
 [ 0.00030438  0.00040763  0.         ... -0.00060821  0.00019608
   0.        ]
 ...
 [ 0.00028082  0.00037641  0.         ... -0.00056168  0.00018094
   0.        ]
 [ 0.000192    0.00025809  0.         ... -0.00038523  0.00012378
   0.        ]
 [-0.00107849 -0.01446844 -0.02980322 ...  0.01410889  0.00746382
   0.02526352]]
layers.2.weight: [[-1.3296983e-02 -5.6072339e-02  4.2912290e-02  2.5155891e-02
   1.0550728e-02  5.7144728e-03 -1.1798215e-02 -2.8547037e-02
  -9.1102868e-03  2.2342596e-02 -3.5557112e-01 -4.2392048e-03
  -2.5969432e-03 -8.7100792e-01  7.9507008e-03 -5.6909784e-03
   4.4260570e-03  3.8967744e-02 -4.9333513e-04  6.7104556e-04
  -2.1671271e-02 -3.7612501e-03 -2.1409769e-02 -2.7536366e-02
   7.4024335e-03 -5.9763142e-03  1.2023494e-02  1.3324173e-02
   3.2830212e-02  1.4088026e-01 -8.5664894e-03  1.1341812e-02
  -1.2621780e-02 -8.7426513e-01 -2.0503286e-02 -4.8184549e-03
  -1.6817259e-02 -1.0282071e-03  1.8816972e-02 -7.6951224e-01
   5.3441473e-03  1.1387871e-02  1.4790051e-02 -2.9193865e-02
  -3.2139901e-02 -2.0021526e-04 -2.9976449e-03 -3.2874335e-02
   2.9992452e-02  2.7856501e-02 -1.6958034e-02  6.0915276e-03
   7.5290543e-03  3.6022707e-03  7.8174460e-01 -5.2045463e-03
   6.7767745e-04 -1.1087953e-02  3.1946633e-02 -2.6000079e-02
  -2.5662023e-03 -1.1895880e-02  1.8177923e-02  3.9067809e-03
  -2.4460193e-02  2.8613370e-04 -2.2545852e-02 -1.1827415e-02
  -3.3916701e-03 -2.7751178e-02 -8.5536909e-04 -1.8338939e-02
   8.5990168e-03  7.6869442e-03 -5.4863798e-03  2.7582550e-02
   9.5214443e-03 -1.3712663e-02 -3.8765110e-02  9.0644415e-03
  -2.2414109e-02 -4.9651023e-03  9.3821445e-03 -2.8784309e-02
  -3.9565451e-03  8.8687204e-03 -8.9171445e-03  2.1991044e-02
   1.1168856e-02 -3.2787189e-02 -1.9905973e-02 -1.1998262e-02
  -1.1637945e-02 -7.0350742e-01  9.8008076e-03 -1.8022748e-02
  -5.4885734e-02  3.0113999e-02  2.1667009e-02  1.0853824e-02
   7.4699223e-03  9.2805270e-03  2.8379433e-02 -1.9545821e-02
   1.8550618e-02  1.8438395e-02  5.6207036e-03  6.2060717e-04
   5.5113486e-03  9.7734982e-04 -4.4592660e-02 -8.1582665e-03
   3.2175571e-02  7.4981670e-03 -6.7118886e-03  7.5067291e-03
  -1.8015895e-02 -9.7353933e-03  1.4205439e-02  1.3339622e-02
   1.1518840e-02 -2.4387915e-02  2.8986884e-03  1.2371370e-02
   8.2711214e-03  3.9634105e-02  2.7191274e-02  6.9393784e-01]]

Final Loss: 0.0006
Distance Metric: 11.6387
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 2312

================================================================================

