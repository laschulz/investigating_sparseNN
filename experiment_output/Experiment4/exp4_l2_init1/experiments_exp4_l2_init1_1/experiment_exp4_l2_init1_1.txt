Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.02592414 -0.02824226  0.00846271 ...  0.02345611 -0.02566366
   0.00809758]
 [ 0.00753912 -0.00821536  0.00246178 ...  0.00682555 -0.00746687
   0.0023534 ]
 [ 0.01299045 -0.01415489  0.0042416  ...  0.01175989 -0.01286463
   0.00405546]
 ...
 [ 0.00292826 -0.00319077  0.00095621 ...  0.00265115 -0.0029002
   0.00091401]
 [-0.01275195  0.01389622 -0.0041636  ... -0.01154437  0.01262928
  -0.00398126]
 [ 0.00486734 -0.00530392  0.0015894  ...  0.00440689 -0.00482078
   0.0015193 ]]
layers.1.weight: [[-0.00370983 -0.00112968 -0.00189064 ... -0.00048723  0.00169837
  -0.00075733]
 [-0.0073121  -0.00214559 -0.00366951 ... -0.0008591   0.00351746
  -0.00139993]
 [ 0.00127013  0.00031465  0.00059649 ...  0.         -0.00073262
   0.00017677]
 ...
 [ 0.01651007  0.00469884  0.00818283 ...  0.00175781 -0.0082474
   0.00299427]
 [-0.007298   -0.0021416  -0.00366254 ... -0.00085767  0.00351019
  -0.00139741]
 [-0.00016976 -0.00010696 -0.00012548 ...  0.          0.
   0.        ]]
layers.2.weight: [[-0.09228116 -0.18516752  0.0342679  -0.09590042 -0.19082788 -0.090515
   0.21760368  0.08489224 -0.00229927 -0.01618506  0.28343257  0.05886367
  -0.1574124  -0.04635148 -0.03123489  0.35720995 -0.29436782  0.07406102
   0.16871376  0.01715049 -0.13806817  0.1295581  -0.12347923  0.0275322
  -0.24480473 -0.34382582 -0.08987797  0.12543254 -0.04113424 -0.12583931
  -0.21843964  0.00432373  0.21119969 -0.2977136  -0.2553603  -0.07380039
   0.15326425 -0.18224902  0.15611139 -0.37759066  0.12450801 -0.10701805
   0.09808511 -0.04513388  0.1078823   0.04985469 -0.15838929  0.12832433
   0.13765422  0.28654712  0.0985764   0.07214916  0.11641121  0.26427215
  -0.06927767  0.0740364   0.46947306  0.05133679 -0.08687396  0.16899024
   0.15581553 -0.02736014 -0.05801525 -0.1278884   0.27166474  0.15999374
  -0.11409791 -0.04569437 -0.10906176  0.1335843  -0.0151957  -0.05533034
  -0.18282895 -0.19233494 -0.06530125 -0.24106786 -0.1168091   0.09084484
   0.15504587 -0.279814    0.03079802  0.07447834  0.34987432  0.0845575
   0.02639645 -0.08678     0.22987226  0.11962847 -0.08461183  0.21068458
  -0.08555926  0.02364741  0.03318275  0.07443764 -0.1286045   0.15400109
  -0.01844614 -0.2752365  -0.17261423 -0.17666325 -0.07206532  0.39644626
  -0.10503888 -0.27818784  0.03111968  0.02309013  0.13448732  0.09189288
  -0.2892475   0.02755818  0.01499392 -0.3153578  -0.11825913 -0.15509644
   0.1291067  -0.16049346  0.278399   -0.25072277  0.20144594  0.20853026
   0.04114202 -0.03382687 -0.12356669  0.14216802  0.03622434  0.4275407
  -0.18480052 -0.00216649]]

Final Loss: 0.0003
Distance Metric: 10.6586
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 2151

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.00041691  0.00027572 -0.00013507  0.          0.         -0.00020768
   0.00052269  0.00011114  0.00022308 -0.00067544 -0.00017742  0.
   0.00038629 -0.00028953  0.00011137  0.         -0.00021664  0.00013864
   0.00033711 -0.00015618  0.          0.         -0.00016436  0.
  -0.00040888  0.00019053 -0.00056786  0.00017798  0.          0.00042788
  -0.00030104 -0.00026961 -0.00014724 -0.00028768  0.         -0.00022441
   0.00012802  0.00014606 -0.00029906  0.00044707 -0.00012759 -0.00010281
  -0.0001023  -0.00055211  0.00015047  0.00011074  0.          0.
   0.00032213 -0.00042155  0.          0.00019626 -0.00036701  0.
   0.          0.0005084  -0.00013525  0.         -0.00014796  0.00040187
  -0.00012083  0.00043764  0.          0.          0.         -0.0002209
   0.          0.          0.00024314  0.00031551 -0.00041203 -0.000377
  -0.00044292  0.          0.          0.00011275  0.00027005 -0.00019046
   0.         -0.00017142 -0.00016346 -0.00014106  0.00034934  0.00024862
   0.         -0.00024956  0.00012992 -0.00024504  0.00035793  0.
   0.         -0.00020447  0.          0.         -0.00035524  0.
   0.00025157  0.          0.          0.00036674  0.00014322 -0.00027841
   0.          0.         -0.00021654 -0.00011475 -0.00017745  0.00048499
  -0.00037797  0.00022357  0.          0.         -0.00025877  0.
   0.          0.00010753 -0.00023204  0.00021483  0.00024082  0.
   0.          0.00022622 -0.0002252   0.00047678  0.          0.
  -0.0003682   0.00018553]]

Final Loss: 0.2550
Distance Metric: 7.0283
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 922

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 4.1848086e-03  3.8784090e-03 -2.5041830e-03 ... -4.0137856e-03
   3.6835903e-03  3.7265220e-04]
 [-2.4478335e-03 -2.4674581e-03  1.7942149e-03 ...  2.6678981e-03
  -2.3032245e-03 -1.3033024e-04]
 [-1.7446683e-03 -1.6695530e-03  1.1554718e-03 ...  1.7665579e-03
  -1.6256694e-03  0.0000000e+00]
 ...
 [-1.0098483e-01 -7.9747282e-02  2.0237736e-02 ... -2.4376477e-01
  -5.6504983e-02  4.0772638e-01]
 [ 4.7076335e-03  4.6924520e-03 -3.2270288e-03 ... -4.8435689e-03
   4.3821121e-03  1.9875055e-04]
 [-1.6500453e-03 -1.7251112e-03  1.3086454e-03 ...  1.7690011e-03
  -1.5695676e-03  0.0000000e+00]]
layers.1.weight: [[ 0.00186358 -0.0008868   0.00098556 ...  0.02540116  0.00072704
   0.00110358]
 [-0.00151099  0.00137245 -0.00037856 ... -0.00116276  0.00133064
  -0.00075825]
 [ 0.00260291  0.00183838  0.0008638  ...  0.00295087  0.00106753
   0.00079201]
 ...
 [-0.00076671  0.00010153  0.00150885 ...  0.02343759  0.00132848
  -0.00346463]
 [-0.0024779  -0.00277295  0.00082475 ...  0.0049292  -0.0013943
  -0.00297033]
 [-0.00026925  0.00148494  0.00158257 ... -0.00674311 -0.00238024
   0.00030462]]
layers.2.weight: [[-0.00912315  0.00041581 -0.00104273  0.00794022 -0.0035712  -0.00097006
  -0.00267341  0.01177424  0.01017122  0.00614816 -0.00640223  0.00398403
   0.00215279  0.00686215 -0.0029274   0.00037968  0.00904349 -0.00212463
   0.00243354  0.00077417 -0.01288243 -0.0031829   0.00928719  0.01205255
   0.00159785  0.00119411 -0.00571197  0.00024183  0.01145599  0.00366445
   0.00023765  0.03687095  0.00467832  0.00330109 -0.00325823  0.00241216
  -0.00202754 -0.00509743 -0.00491953  0.00175518 -0.0011805  -0.01093043
  -0.00010631 -0.00027888  0.10544732 -0.02447029 -0.01384811 -0.00222856
  -0.00712472 -0.01715234 -0.00696609 -0.0055735  -0.00487978 -0.00214022
  -0.00107204 -0.00419089 -0.006646    0.00537927 -0.00127388 -0.00021486
  -0.0082635  -0.00018891  0.00419473  0.         -0.00298681 -0.00082032
  -0.00144959  0.0008357  -0.00204934  0.00173027  0.00152079 -0.00226661
   0.0144011  -0.00137726 -0.00476423 -0.00399525  0.         -0.00389166
   0.00138515  0.00404374  0.00010759  0.00374349  0.00144008  0.00289388
   0.00407025 -0.00420965  0.00044155 -0.00189751  0.00604787 -0.01235177
  -0.0001981   0.00293353 -0.00792409  0.00094638  0.00186737 -0.0008917
   0.00399314 -0.00239687 -0.00113983 -0.00272711 -0.00575271  0.00274621
   0.0008309  -0.00084643 -0.00823334  0.00153507  0.00257852  0.00083871
  -0.00180031  0.00484182 -0.00988779 -0.00589733 -0.00066958 -0.00224179
   0.0002073   0.01130563 -0.00124393 -0.00437982 -0.00823017  0.00043907
   0.00191491 -0.00093217 -0.0057608   0.00152013  0.00483355 -0.00832732
  -0.00179486  0.00239731]]

Final Loss: 0.2521
Distance Metric: 11.0830
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 482

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.05051357  0.05611494 -0.02501716 ... -0.19280104  0.20004325
  -0.05529626]
 [ 0.22873819 -0.2755416   0.07442109 ...  0.15160038 -0.1430431
   0.04831263]
 [-0.06594944  0.0673525  -0.03178971 ... -0.23622215  0.24785045
  -0.06431078]
 ...
 [-0.05861424  0.06241649 -0.02853178 ... -0.2154063   0.22482549
  -0.06010497]
 [ 0.37236464 -0.39812237  0.11129843 ... -0.5046969   0.5265028
  -0.16793194]
 [-0.02526744  0.03120317 -0.01466651 ... -0.12535991  0.12720981
  -0.03870065]]
layers.1.weight: [[-0.00164048 -0.01272376 -0.00129886 ... -0.00144749 -0.07087169
  -0.00275571]
 [ 0.00159052  0.01291447  0.00127487 ...  0.00141035  0.07456349
   0.00269337]
 [-0.00071954 -0.00404237 -0.00063184 ... -0.00066927 -0.02301507
  -0.00104323]
 ...
 [ 0.          0.00502827 -0.0001833  ... -0.00013263  0.03479396
   0.0004196 ]
 [-0.00193146 -0.01536693 -0.00147842 ... -0.00167758 -0.0825578
  -0.00331803]
 [-0.00148804 -0.0115242  -0.00118955 ... -0.00131884 -0.06515186
  -0.00248871]]
layers.2.weight: [[ 0.529486   -0.5635116   0.16462354 -0.5532193   0.41262737 -0.49534282
  -0.5973046   4.192202    0.04253001 -0.7910391   0.07923628 -0.74112254
  -0.40280503  3.5823262   0.72278965  0.676911    0.26680043  0.49297106
   0.388284    4.489297   -0.2117355  -0.5234401   0.06096024 -0.3072509
   0.118538    0.6017268  -0.4903676  -0.30862257 -0.745657   -0.64431536
  -0.82097685 -0.6804029  -0.5589199  -0.3621249  -0.6822908   0.13137808
  -0.33834225  0.5141509  -0.0501425   0.7012949   0.42844585  0.49150205
  -0.36303288 -0.62546307 -0.13676015  0.28149584 -0.25178012 -0.6783781
   0.01581822 -0.720417   -0.5556966  -5.4338956   0.518602    0.6728058
   0.33189103  0.5693888  -0.1037476   0.31559902 -0.70553374  0.6188846
  -3.5732574   0.23939766  0.07754683  0.48577008 -0.00569876 -0.809313
  -0.4152052  -0.01232892 -0.74680984  0.39954004  0.20538317  0.2732286
  -3.6003282  -0.6735248  -0.3275709   0.6616632  -0.6074978   0.5927308
  -0.37328535 -0.6823792   0.3436978   0.43056345 -0.48267266  0.3720523
  -0.60053     0.25946555 -0.6100234  -0.03602611 -0.37870833 -0.77921563
   0.25055116 -0.50882775 -0.37716874 -0.7563892  -0.62756634  0.48544475
   0.6089581  -0.32821804 -0.0997204   0.64749783  0.23578493  0.1489809
  -0.66559297 -0.5803682  -0.24829325 -0.5138213   0.13768458 -0.45933115
  -0.4806786   0.30617636 -0.6610737  -0.4332137  -0.85416305  1.5214779
   3.2252557  -0.23363309 -0.408779    3.5083048  -0.42806175 -0.27686933
  -0.7786618  -0.14936182  0.3437771  -0.78157985 -3.4641798  -0.26081818
   0.6243374   0.48417777]]

Final Loss: 3.3234
Distance Metric: 39.4698
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 3155

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[-0.00010249  0.          0.         -0.00015836  0.00018292  0.
   0.          0.         -0.000114    0.          0.         -0.00026336
   0.00024288  0.          0.         -0.00013116  0.          0.
   0.00031023 -0.00011033  0.          0.          0.00015469  0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.00015878 -0.00015562
  -0.00011945  0.00022299  0.          0.          0.00014395  0.
  -0.00013786  0.          0.          0.          0.00019446 -0.00010248
   0.          0.          0.         -0.00012499  0.00020698  0.
  -0.00010532  0.         -0.00014178  0.          0.          0.
  -0.00013506  0.          0.00013133  0.          0.         -0.00024656
   0.          0.00010672  0.00011693  0.          0.00022347  0.
   0.          0.          0.          0.          0.00010184  0.00013984
   0.          0.00019862  0.00027252  0.00012662  0.          0.
   0.          0.00010738  0.          0.          0.          0.
   0.          0.00012422  0.          0.          0.          0.00021404
   0.          0.         -0.00013066  0.00016861  0.          0.00044667
   0.00015412  0.00012299  0.00016998  0.00016457  0.          0.
  -0.00022544  0.00011739  0.          0.          0.00010324 -0.00022255
  -0.00011199  0.         -0.00014653  0.          0.         -0.00013839
   0.          0.          0.          0.          0.          0.
  -0.00018956  0.        ]]

Final Loss: 4.9466
Distance Metric: 7.0261
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 995

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.08613221 -0.07663439  0.03672766 ...  0.04071896 -0.05125936
   0.01248406]
 [ 0.14120659 -0.16928281  0.09173321 ...  0.03407316 -0.10535391
   0.03462442]
 [-0.06154481 -0.01953467 -0.01444256 ...  0.00295175  0.03092929
  -0.01419828]
 ...
 [-0.07418318  0.06341822 -0.03414302 ... -0.04119999  0.04478201
  -0.00675631]
 [ 1.4070485  -1.497217    0.3560665  ... -0.1562297   0.08312146
  -0.10337456]
 [ 0.3708075  -0.3732537   0.20103447 ... -0.8052742   0.58186287
  -0.27198762]]
layers.1.weight: [[ 6.7735784e-04  7.7241444e-04  3.5447587e-04 ... -6.4156624e-04
  -1.7116942e-03 -2.3843935e-03]
 [-2.3667380e-02 -3.0626750e-02 -5.0476853e-02 ...  2.7734451e-02
   5.5404282e-01  3.6642689e-01]
 [-1.7587657e-03 -2.1933925e-03 -1.0330058e-03 ...  1.7780085e-03
   4.6300455e-03  6.5461951e-03]
 ...
 [-1.6813785e-03 -2.0501695e-03 -9.5152325e-04 ...  1.6649482e-03
   4.4115409e-03  6.1889854e-03]
 [-4.1582165e-04 -5.7733117e-04 -2.5383366e-04 ...  4.6768418e-04
   1.1879146e-03  1.6576402e-03]
 [-4.1386951e-04 -5.2588817e-04 -2.3496113e-04 ...  4.1550750e-04
   1.0977287e-03  1.5577230e-03]]
layers.2.weight: [[ 1.2851015e-03  3.4595925e-01 -3.5061850e-03 -7.2830409e-04
  -2.1250620e-03  1.6864254e-01 -3.6657270e-04  2.0691450e-03
  -3.4611595e-01 -4.1953611e-01  4.8749992e-03 -2.8221691e-03
   2.3121712e-01  5.2635872e-04  1.2450005e-03 -2.5425811e-04
   0.0000000e+00 -2.5335955e-03 -2.0654905e-03  0.0000000e+00
  -4.5043034e-03 -1.0640019e-03 -9.9188618e-02  8.5220177e-04
  -2.6848183e-03 -5.4314786e-01  1.4016102e-03 -9.4515347e-04
  -9.7533909e-04  4.5959060e-03 -2.0415338e-03  1.3120123e-03
  -2.4613126e-03  1.1981392e-03 -3.8464010e-01 -1.3129740e-03
   1.5000396e-03  1.5492635e-03 -2.0859065e-03 -1.4424324e-03
  -3.9852797e-03 -2.1966717e-03  2.6847732e-03 -8.1020163e-04
  -3.6947213e-03 -1.6834917e-03  3.6360237e-01  2.3484044e-03
   2.4233228e-03  2.9985172e-01 -4.5820056e-03  0.0000000e+00
  -1.5103521e-03  4.0703375e-04  1.6201088e-03 -6.3529261e-03
   4.0857086e-01  2.3293402e-03  2.6253292e-03  4.8196903e-01
   5.2570910e-03  1.9956583e-01  2.9460078e-03 -2.0845572e-03
  -3.9597061e-01  5.1758983e-03 -1.5602816e-03  2.1977127e-03
   5.7857885e-04  1.8435094e-03  2.4481122e-03  2.7015654e-03
  -7.0600363e-04 -2.7103952e-04 -2.1073860e-03  1.9288665e-03
  -3.7292132e-01 -1.7611119e-03 -7.6497602e-04  3.2592115e-03
  -4.0236738e-01  1.5426702e-03 -3.3234534e-01 -4.3190820e-03
  -2.1988819e-03  2.9432685e-03 -2.2586165e-03  1.3918119e-02
  -1.6533093e-03  4.1406667e-03  7.7145938e-03 -1.1507692e-03
  -4.4707352e-01  2.2693058e-03  6.4899924e-04 -1.6678275e-03
  -4.2585140e-01 -1.2314495e-03 -1.9691302e-03  3.8900571e-03
  -1.7842802e-04 -1.2808006e-03  1.1796679e-04 -1.0916166e-03
  -7.8021281e-04 -1.1922458e-02  2.6645053e-03 -6.0600499e-03
  -2.7782628e-01  6.9700566e-04  4.6889461e-03 -3.4412171e-04
   3.3357344e-03 -3.6740392e-01 -7.3423353e-04 -2.9278081e-03
  -3.4922698e-01 -3.9933859e-03 -4.6675955e-03  3.0468567e-03
   3.4853303e-01 -3.1664845e-01  9.0824737e-04  2.2110138e-03
  -3.9332751e-03 -3.3236537e-03 -8.8581123e-04 -8.4007060e-04]]

Final Loss: 3.5577
Distance Metric: 26.4275
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 945

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.01114542  0.01154684 -0.00609913 ...  0.03437499  0.03409681
  -0.01803129]
 [ 0.01546664  0.0158827  -0.00847739 ...  0.07697167  0.07732036
  -0.04166703]
 [ 0.03448234  0.03463657 -0.01891334 ...  0.25000873  0.25362644
  -0.13798645]
 ...
 [-0.02437624 -0.02347392  0.01353905 ... -0.161904   -0.16498616
   0.09012071]
 [-0.05377133 -0.05237336  0.03079393 ...  1.3854923   1.4025887
  -0.7642328 ]
 [ 0.00488078  0.00539044 -0.00262555 ...  0.00452891  0.00384653
  -0.00162674]]
layers.1.weight: [[ 2.12679990e-03  3.07778316e-03  7.18299812e-03 ... -4.27768379e-03
  -5.47891781e-02  1.05247821e-03]
 [ 3.16589605e-03  4.49571898e-03  1.02136312e-02 ... -5.71012124e-03
  -7.46223629e-02  1.67899800e-03]
 [-3.69032053e-03 -5.19443862e-03 -1.16527379e-02 ...  6.30927691e-03
   8.32391381e-02 -2.01643258e-03]
 ...
 [ 1.90154684e-03  2.77023646e-03  6.52288785e-03 ... -3.95913003e-03
  -5.03332466e-02  9.18162579e-04]
 [-1.16669284e-02 -3.45545597e-02 -1.20265946e-01 ...  7.14334920e-02
  -8.39947402e-01  0.00000000e+00]
 [ 7.53888278e-04  1.17379951e-03  2.99482024e-03 ... -2.10593594e-03
  -2.51176730e-02  2.73268233e-04]]
layers.2.weight: [[-2.38239720e-01 -3.27837110e-01  3.68567348e-01  6.41674623e-02
   1.41141400e-01 -2.23387748e-01 -1.59087345e-01 -1.44074529e-01
  -3.03771019e-01 -7.39443749e-02  6.66810393e-01  2.49776497e-01
   1.53387174e-01  1.21520765e-01 -1.16469577e-01 -1.12830520e-01
   2.78608233e-01 -3.19266289e-01 -2.00523436e-01  2.52295882e-02
   3.80944042e-03  1.50591403e-01 -9.61695910e-02  1.32054284e-01
  -3.06657583e-01 -2.96587050e-01  4.74134356e-01  4.93178576e-01
  -5.21448195e-01  9.16721150e-02 -1.49162307e-01  5.55490196e-01
  -1.29231304e-01 -1.45650899e+00 -2.26163119e-02  2.26782769e-01
   2.33387053e-01  3.85681748e-01 -1.33060902e-01  1.24909550e-01
  -3.09868842e-01  1.60794690e-01  2.55438894e-01 -2.11383998e-01
  -4.72997785e-01 -4.01834756e-01  5.28989732e-01  1.53992093e+00
   2.44083256e-01 -3.25871319e-01 -2.42221355e-01 -5.77694833e-01
  -1.72101304e-01 -1.15809374e-01 -1.00683354e-01  8.87772441e-02
   4.18875903e-01 -2.42589235e-01  6.60815984e-02 -2.48331055e-01
  -1.44926598e-02 -4.33809191e-01 -1.57886460e-01 -4.39347923e-01
  -4.37656678e-02  2.26007015e-01  1.91771120e-01 -4.24522161e-02
  -1.16274431e-01  1.92621678e-01 -4.50473726e-01 -4.22804892e-01
  -3.37575614e-01  6.41204715e-02 -3.44070531e-02 -7.94208199e-02
  -4.75341499e-01  9.16677415e-02  5.33807158e-01 -6.90798044e-01
  -2.59890974e-01  1.52602373e-02 -3.67275119e-01  3.38544011e-01
  -3.98873717e-01 -2.08080828e-01 -1.43714219e-01 -1.46503195e-01
   3.88499141e-01  2.05175996e-01  3.84977043e-01 -2.53887206e-01
  -1.83936521e-01  3.10051054e-01 -1.86999530e-01 -6.20341492e+00
   2.09954247e-01 -2.98841652e-02  4.45073098e-02  5.13573945e-01
   8.47392827e-02  2.24942081e-02 -2.44350418e-01  8.46564546e-02
   1.55387238e-01  1.44633204e-01 -1.27349257e-01 -1.04499452e-01
   2.76060551e-01  2.19514981e-01 -3.05438191e-02 -3.13204169e-01
  -2.05657631e-01  2.68060178e-01  7.95336664e-02  1.55755267e-01
   6.19839787e-01 -8.58517662e-02 -4.94772494e-02  1.15663521e-01
   4.25441302e-02  1.29146054e-01 -6.08182065e-02 -1.40017360e-01
   2.23332763e-01 -2.18323618e-01 -2.09147501e+00 -1.07215837e-01]]

Final Loss: 0.2388
Distance Metric: 23.0484
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1603

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.         0.         0.        ...  0.        -0.0001209  0.       ]
 [ 0.         0.         0.        ...  0.         0.         0.       ]
 [ 0.         0.         0.        ...  0.         0.         0.       ]
 ...
 [ 0.         0.         0.        ...  0.         0.         0.       ]
 [ 0.         0.         0.        ...  0.         0.         0.       ]
 [ 0.         0.         0.        ...  0.         0.         0.       ]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.00054478 -0.00073165 -0.00037764  0.         -0.00012167 -0.00090391
  -0.0004564   0.00029924  0.00063315 -0.00016675  0.         -0.00048608
  -0.00011056  0.00090218 -0.0002994  -0.0005261  -0.00032217  0.0002727
   0.          0.          0.00068842 -0.0006705   0.00039096 -0.0002712
   0.00040007 -0.00045582 -0.00023834  0.         -0.00063569  0.
   0.00028898  0.00031863  0.00069738  0.00014518  0.00043913 -0.0009029
  -0.00024099 -0.00029277  0.00096742  0.0005033  -0.00074579 -0.00041762
  -0.00028153  0.         -0.00042859  0.00014955  0.00016317  0.
  -0.00021008  0.00022082 -0.00062808  0.          0.          0.
   0.          0.          0.          0.00069618 -0.00015321 -0.0003667
  -0.0001767  -0.00017526  0.00040016  0.00025366  0.00012287  0.00014938
  -0.00038006 -0.0004667   0.00024023  0.          0.          0.00032488
   0.00040414 -0.00032301 -0.00018372  0.          0.00019137  0.00025091
   0.00024805  0.00079018 -0.00020999  0.00027954  0.          0.00066974
  -0.00082261  0.00060123 -0.00019214  0.00021382  0.00020868 -0.00058687
  -0.00019873  0.0004616   0.00014068  0.0003561  -0.00034287  0.0002474
   0.00056691 -0.00017742  0.         -0.00036151  0.         -0.00041864
   0.          0.         -0.00015516 -0.00027763 -0.00051872 -0.00044568
   0.00036971  0.00073309 -0.00038246 -0.00027492  0.00018148 -0.00040959
   0.          0.00017398  0.00079315 -0.00018062  0.00011169 -0.00029987
   0.00015758 -0.00038001 -0.00053637  0.00011627  0.00052732 -0.00021997
   0.00023371  0.00032096]]

Final Loss: 0.5514
Distance Metric: 5.7193
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 875

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 1.7634027e-03 -1.4977857e-03  9.8607177e-04 ...  7.9663899e-03
  -6.2931767e-03  5.1071029e-03]
 [-3.5999930e-03  3.4065417e-03 -2.7570052e-03 ... -4.4741690e-01
   3.5589790e-01 -2.9339024e-01]
 [-2.0990109e-03  1.7753146e-03 -1.1388684e-03 ... -1.2347302e-02
   9.7504649e-03 -7.9595605e-03]
 ...
 [ 5.1275766e-03 -4.5205764e-03  3.5420638e-03 ... -2.2514824e-02
   1.7671885e-02 -1.5074330e-02]
 [ 8.4997565e-03 -7.3335734e-03  5.2976771e-03 ... -2.8058898e-03
   2.1449775e-03 -2.4206042e-03]
 [-2.2990703e-04  1.7542827e-04  0.0000000e+00 ... -8.0524366e-03
   6.3481843e-03 -5.2672517e-03]]
layers.1.weight: [[ 0.         -0.00011999  0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 ...
 [ 0.         -0.00020833  0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[ 1.0242166e-03 -7.4235187e-04  4.1276030e-04 -2.7973846e-01
   3.4750995e-04  6.5735081e-04 -2.3873788e-03 -5.3228135e-04
  -5.4291327e-04  0.0000000e+00 -1.7803400e-03  1.1033661e-03
   0.0000000e+00 -2.2184812e-04 -9.3276082e-03  1.7376400e-03
   1.1364440e-03 -1.4401661e-03 -3.6877126e-04 -1.1598647e-03
  -2.3775538e-04  2.5590353e-03  1.9989223e-03  1.7178435e-02
  -5.4247543e-04 -9.0694427e-04 -1.4227884e-03  0.0000000e+00
   3.2888810e-04 -8.6411252e-04  0.0000000e+00 -6.4020990e-03
  -9.9597394e-04 -1.0077755e-03 -6.8320299e-04  0.0000000e+00
  -4.1265208e-03  8.5164170e-04  2.0193460e-03  1.1348013e-03
   1.6097797e-03  0.0000000e+00 -1.1750999e-03  1.6223374e-04
  -1.5713398e-04  2.1938125e-03 -1.8846926e-04  1.2311175e-03
  -1.5862508e-03  1.0650787e-03  3.8530584e-04  6.5570385e-03
   6.8686873e-04 -1.1374839e-04 -2.6065795e-04 -1.2892253e-03
   1.4466116e-03 -1.3157646e-03  8.8140421e-04  0.0000000e+00
  -1.0189856e-03 -1.0446964e-03  2.7647731e-03 -1.3079400e-03
   1.0764214e-03 -6.1240309e-01 -1.5526809e-03 -8.5040356e-04
   0.0000000e+00 -8.9051470e-04 -4.4182083e-03 -2.4343310e-03
   0.0000000e+00  2.0774209e-03  2.4445585e-03  8.8085822e-04
   1.6036481e-03 -2.2198292e-03 -9.9945185e-04 -7.1574317e-04
  -1.3209796e-02  3.2144785e-04  1.3203516e-03 -1.0737691e-03
   1.6924361e-03  1.2318186e-03 -2.0043387e-04 -3.3718958e-01
  -1.4533788e-03 -1.7632230e-03 -7.6059328e-04  4.9397675e-04
  -1.7262893e-04  2.1403567e-03  1.5086969e-03  5.5836281e-04
   2.4169667e-04 -9.0656639e-04  0.0000000e+00 -1.6813894e-03
   1.0734146e-03 -8.8704639e-04 -1.8225122e-02  1.2128421e-03
   1.1252263e-03  1.0311044e-03 -1.4759314e-03  2.9496101e-04
   6.5465260e-04  0.0000000e+00  1.8050553e-02  2.2874698e-03
  -2.5726619e-04 -1.2079354e-02  1.6435398e-03  6.1512267e-04
   2.7647177e-03 -1.1772101e-02 -3.0728688e-03 -2.5360601e-04
   5.2059221e-01  2.5146061e-03 -9.7483787e-04  3.5240504e-04
   1.0702430e+00  1.7781772e-03 -2.8968771e-04  4.9235567e-04]]

Final Loss: 0.0003
Distance Metric: 6.0874
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 2815

================================================================================

