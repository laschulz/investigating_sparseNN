Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.02592414 -0.02824226  0.00846271 ...  0.02345611 -0.02566366
   0.00809758]
 [ 0.00753912 -0.00821536  0.00246178 ...  0.00682555 -0.00746687
   0.0023534 ]
 [ 0.01299045 -0.01415489  0.0042416  ...  0.01175989 -0.01286463
   0.00405546]
 ...
 [ 0.00292826 -0.00319077  0.00095621 ...  0.00265115 -0.0029002
   0.00091401]
 [-0.01275195  0.01389622 -0.0041636  ... -0.01154437  0.01262928
  -0.00398126]
 [ 0.00486734 -0.00530392  0.0015894  ...  0.00440689 -0.00482078
   0.0015193 ]]
layers.1.weight: [[-0.00370983 -0.00112968 -0.00189064 ... -0.00048723  0.00169837
  -0.00075733]
 [-0.0073121  -0.00214559 -0.00366951 ... -0.0008591   0.00351746
  -0.00139993]
 [ 0.00127013  0.00031465  0.00059649 ...  0.         -0.00073262
   0.00017677]
 ...
 [ 0.01651007  0.00469884  0.00818283 ...  0.00175781 -0.0082474
   0.00299427]
 [-0.007298   -0.0021416  -0.00366254 ... -0.00085767  0.00351019
  -0.00139741]
 [-0.00016976 -0.00010696 -0.00012548 ...  0.          0.
   0.        ]]
layers.2.weight: [[-0.09228116 -0.18516752  0.0342679  -0.09590042 -0.19082788 -0.090515
   0.21760368  0.08489224 -0.00229927 -0.01618506  0.28343257  0.05886367
  -0.1574124  -0.04635148 -0.03123489  0.35720995 -0.29436782  0.07406102
   0.16871376  0.01715049 -0.13806817  0.1295581  -0.12347923  0.0275322
  -0.24480473 -0.34382582 -0.08987797  0.12543254 -0.04113424 -0.12583931
  -0.21843964  0.00432373  0.21119969 -0.2977136  -0.2553603  -0.07380039
   0.15326425 -0.18224902  0.15611139 -0.37759066  0.12450801 -0.10701805
   0.09808511 -0.04513388  0.1078823   0.04985469 -0.15838929  0.12832433
   0.13765422  0.28654712  0.0985764   0.07214916  0.11641121  0.26427215
  -0.06927767  0.0740364   0.46947306  0.05133679 -0.08687396  0.16899024
   0.15581553 -0.02736014 -0.05801525 -0.1278884   0.27166474  0.15999374
  -0.11409791 -0.04569437 -0.10906176  0.1335843  -0.0151957  -0.05533034
  -0.18282895 -0.19233494 -0.06530125 -0.24106786 -0.1168091   0.09084484
   0.15504587 -0.279814    0.03079802  0.07447834  0.34987432  0.0845575
   0.02639645 -0.08678     0.22987226  0.11962847 -0.08461183  0.21068458
  -0.08555926  0.02364741  0.03318275  0.07443764 -0.1286045   0.15400109
  -0.01844614 -0.2752365  -0.17261423 -0.17666325 -0.07206532  0.39644626
  -0.10503888 -0.27818784  0.03111968  0.02309013  0.13448732  0.09189288
  -0.2892475   0.02755818  0.01499392 -0.3153578  -0.11825913 -0.15509644
   0.1291067  -0.16049346  0.278399   -0.25072277  0.20144594  0.20853026
   0.04114202 -0.03382687 -0.12356669  0.14216802  0.03622434  0.4275407
  -0.18480052 -0.00216649]]

Final Loss: 0.0003
Distance Metric: 10.6586
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 2151

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.00041691  0.00027572 -0.00013507  0.          0.         -0.00020768
   0.00052269  0.00011114  0.00022308 -0.00067544 -0.00017742  0.
   0.00038629 -0.00028953  0.00011137  0.         -0.00021664  0.00013864
   0.00033711 -0.00015618  0.          0.         -0.00016436  0.
  -0.00040888  0.00019053 -0.00056786  0.00017798  0.          0.00042788
  -0.00030104 -0.00026961 -0.00014724 -0.00028768  0.         -0.00022441
   0.00012802  0.00014606 -0.00029906  0.00044707 -0.00012759 -0.00010281
  -0.0001023  -0.00055211  0.00015047  0.00011074  0.          0.
   0.00032213 -0.00042155  0.          0.00019626 -0.00036701  0.
   0.          0.0005084  -0.00013525  0.         -0.00014796  0.00040187
  -0.00012083  0.00043764  0.          0.          0.         -0.0002209
   0.          0.          0.00024314  0.00031551 -0.00041203 -0.000377
  -0.00044292  0.          0.          0.00011275  0.00027005 -0.00019046
   0.         -0.00017142 -0.00016346 -0.00014106  0.00034934  0.00024862
   0.         -0.00024956  0.00012992 -0.00024504  0.00035793  0.
   0.         -0.00020447  0.          0.         -0.00035524  0.
   0.00025157  0.          0.          0.00036674  0.00014322 -0.00027841
   0.          0.         -0.00021654 -0.00011475 -0.00017745  0.00048499
  -0.00037797  0.00022357  0.          0.         -0.00025877  0.
   0.          0.00010753 -0.00023204  0.00021483  0.00024082  0.
   0.          0.00022622 -0.0002252   0.00047678  0.          0.
  -0.0003682   0.00018553]]

Final Loss: 0.2550
Distance Metric: 7.0283
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 922

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 4.1848086e-03  3.8784090e-03 -2.5041830e-03 ... -4.0137856e-03
   3.6835903e-03  3.7265220e-04]
 [-2.4478335e-03 -2.4674581e-03  1.7942149e-03 ...  2.6678981e-03
  -2.3032245e-03 -1.3033024e-04]
 [-1.7446683e-03 -1.6695530e-03  1.1554718e-03 ...  1.7665579e-03
  -1.6256694e-03  0.0000000e+00]
 ...
 [-1.0098483e-01 -7.9747282e-02  2.0237736e-02 ... -2.4376477e-01
  -5.6504983e-02  4.0772638e-01]
 [ 4.7076335e-03  4.6924520e-03 -3.2270288e-03 ... -4.8435689e-03
   4.3821121e-03  1.9875055e-04]
 [-1.6500453e-03 -1.7251112e-03  1.3086454e-03 ...  1.7690011e-03
  -1.5695676e-03  0.0000000e+00]]
layers.1.weight: [[ 0.00186358 -0.0008868   0.00098556 ...  0.02540116  0.00072704
   0.00110358]
 [-0.00151099  0.00137245 -0.00037856 ... -0.00116276  0.00133064
  -0.00075825]
 [ 0.00260291  0.00183838  0.0008638  ...  0.00295087  0.00106753
   0.00079201]
 ...
 [-0.00076671  0.00010153  0.00150885 ...  0.02343759  0.00132848
  -0.00346463]
 [-0.0024779  -0.00277295  0.00082475 ...  0.0049292  -0.0013943
  -0.00297033]
 [-0.00026925  0.00148494  0.00158257 ... -0.00674311 -0.00238024
   0.00030462]]
layers.2.weight: [[-0.00912315  0.00041581 -0.00104273  0.00794022 -0.0035712  -0.00097006
  -0.00267341  0.01177424  0.01017122  0.00614816 -0.00640223  0.00398403
   0.00215279  0.00686215 -0.0029274   0.00037968  0.00904349 -0.00212463
   0.00243354  0.00077417 -0.01288243 -0.0031829   0.00928719  0.01205255
   0.00159785  0.00119411 -0.00571197  0.00024183  0.01145599  0.00366445
   0.00023765  0.03687095  0.00467832  0.00330109 -0.00325823  0.00241216
  -0.00202754 -0.00509743 -0.00491953  0.00175518 -0.0011805  -0.01093043
  -0.00010631 -0.00027888  0.10544732 -0.02447029 -0.01384811 -0.00222856
  -0.00712472 -0.01715234 -0.00696609 -0.0055735  -0.00487978 -0.00214022
  -0.00107204 -0.00419089 -0.006646    0.00537927 -0.00127388 -0.00021486
  -0.0082635  -0.00018891  0.00419473  0.         -0.00298681 -0.00082032
  -0.00144959  0.0008357  -0.00204934  0.00173027  0.00152079 -0.00226661
   0.0144011  -0.00137726 -0.00476423 -0.00399525  0.         -0.00389166
   0.00138515  0.00404374  0.00010759  0.00374349  0.00144008  0.00289388
   0.00407025 -0.00420965  0.00044155 -0.00189751  0.00604787 -0.01235177
  -0.0001981   0.00293353 -0.00792409  0.00094638  0.00186737 -0.0008917
   0.00399314 -0.00239687 -0.00113983 -0.00272711 -0.00575271  0.00274621
   0.0008309  -0.00084643 -0.00823334  0.00153507  0.00257852  0.00083871
  -0.00180031  0.00484182 -0.00988779 -0.00589733 -0.00066958 -0.00224179
   0.0002073   0.01130563 -0.00124393 -0.00437982 -0.00823017  0.00043907
   0.00191491 -0.00093217 -0.0057608   0.00152013  0.00483355 -0.00832732
  -0.00179486  0.00239731]]

Final Loss: 0.2521
Distance Metric: 11.0830
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 482

================================================================================

nonoverlappingCNN_relu -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.05051357  0.05611494 -0.02501716 ... -0.19280104  0.20004325
  -0.05529626]
 [ 0.22873819 -0.2755416   0.07442109 ...  0.15160038 -0.1430431
   0.04831263]
 [-0.06594944  0.0673525  -0.03178971 ... -0.23622215  0.24785045
  -0.06431078]
 ...
 [-0.05861424  0.06241649 -0.02853178 ... -0.2154063   0.22482549
  -0.06010497]
 [ 0.37236464 -0.39812237  0.11129843 ... -0.5046969   0.5265028
  -0.16793194]
 [-0.02526744  0.03120317 -0.01466651 ... -0.12535991  0.12720981
  -0.03870065]]
layers.1.weight: [[-0.00164048 -0.01272376 -0.00129886 ... -0.00144749 -0.07087169
  -0.00275571]
 [ 0.00159052  0.01291447  0.00127487 ...  0.00141035  0.07456349
   0.00269337]
 [-0.00071954 -0.00404237 -0.00063184 ... -0.00066927 -0.02301507
  -0.00104323]
 ...
 [ 0.          0.00502827 -0.0001833  ... -0.00013263  0.03479396
   0.0004196 ]
 [-0.00193146 -0.01536693 -0.00147842 ... -0.00167758 -0.0825578
  -0.00331803]
 [-0.00148804 -0.0115242  -0.00118955 ... -0.00131884 -0.06515186
  -0.00248871]]
layers.2.weight: [[ 0.529486   -0.5635116   0.16462354 -0.5532193   0.41262737 -0.49534282
  -0.5973046   4.192202    0.04253001 -0.7910391   0.07923628 -0.74112254
  -0.40280503  3.5823262   0.72278965  0.676911    0.26680043  0.49297106
   0.388284    4.489297   -0.2117355  -0.5234401   0.06096024 -0.3072509
   0.118538    0.6017268  -0.4903676  -0.30862257 -0.745657   -0.64431536
  -0.82097685 -0.6804029  -0.5589199  -0.3621249  -0.6822908   0.13137808
  -0.33834225  0.5141509  -0.0501425   0.7012949   0.42844585  0.49150205
  -0.36303288 -0.62546307 -0.13676015  0.28149584 -0.25178012 -0.6783781
   0.01581822 -0.720417   -0.5556966  -5.4338956   0.518602    0.6728058
   0.33189103  0.5693888  -0.1037476   0.31559902 -0.70553374  0.6188846
  -3.5732574   0.23939766  0.07754683  0.48577008 -0.00569876 -0.809313
  -0.4152052  -0.01232892 -0.74680984  0.39954004  0.20538317  0.2732286
  -3.6003282  -0.6735248  -0.3275709   0.6616632  -0.6074978   0.5927308
  -0.37328535 -0.6823792   0.3436978   0.43056345 -0.48267266  0.3720523
  -0.60053     0.25946555 -0.6100234  -0.03602611 -0.37870833 -0.77921563
   0.25055116 -0.50882775 -0.37716874 -0.7563892  -0.62756634  0.48544475
   0.6089581  -0.32821804 -0.0997204   0.64749783  0.23578493  0.1489809
  -0.66559297 -0.5803682  -0.24829325 -0.5138213   0.13768458 -0.45933115
  -0.4806786   0.30617636 -0.6610737  -0.4332137  -0.85416305  1.5214779
   3.2252557  -0.23363309 -0.408779    3.5083048  -0.42806175 -0.27686933
  -0.7786618  -0.14936182  0.3437771  -0.78157985 -3.4641798  -0.26081818
   0.6243374   0.48417777]]

Final Loss: 3.3234
Distance Metric: 39.4698
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 3155

================================================================================

nonoverlappingCNN_relu -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[-0.00010249  0.          0.         -0.00015836  0.00018292  0.
   0.          0.         -0.000114    0.          0.         -0.00026336
   0.00024288  0.          0.         -0.00013116  0.          0.
   0.00031023 -0.00011033  0.          0.          0.00015469  0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.00015878 -0.00015562
  -0.00011945  0.00022299  0.          0.          0.00014395  0.
  -0.00013786  0.          0.          0.          0.00019446 -0.00010248
   0.          0.          0.         -0.00012499  0.00020698  0.
  -0.00010532  0.         -0.00014178  0.          0.          0.
  -0.00013506  0.          0.00013133  0.          0.         -0.00024656
   0.          0.00010672  0.00011693  0.          0.00022347  0.
   0.          0.          0.          0.          0.00010184  0.00013984
   0.          0.00019862  0.00027252  0.00012662  0.          0.
   0.          0.00010738  0.          0.          0.          0.
   0.          0.00012422  0.          0.          0.          0.00021404
   0.          0.         -0.00013066  0.00016861  0.          0.00044667
   0.00015412  0.00012299  0.00016998  0.00016457  0.          0.
  -0.00022544  0.00011739  0.          0.          0.00010324 -0.00022255
  -0.00011199  0.         -0.00014653  0.          0.         -0.00013839
   0.          0.          0.          0.          0.          0.
  -0.00018956  0.        ]]

Final Loss: 4.9466
Distance Metric: 7.0261
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 995

================================================================================

nonoverlappingCNN_relu -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 0.08613221 -0.07663439  0.03672766 ...  0.04071896 -0.05125936
   0.01248406]
 [ 0.14120659 -0.16928281  0.09173321 ...  0.03407316 -0.10535391
   0.03462442]
 [-0.06154481 -0.01953467 -0.01444256 ...  0.00295175  0.03092929
  -0.01419828]
 ...
 [-0.07418318  0.06341822 -0.03414302 ... -0.04119999  0.04478201
  -0.00675631]
 [ 1.4070485  -1.497217    0.3560665  ... -0.1562297   0.08312146
  -0.10337456]
 [ 0.3708075  -0.3732537   0.20103447 ... -0.8052742   0.58186287
  -0.27198762]]
layers.1.weight: [[ 6.7735784e-04  7.7241444e-04  3.5447587e-04 ... -6.4156624e-04
  -1.7116942e-03 -2.3843935e-03]
 [-2.3667380e-02 -3.0626750e-02 -5.0476853e-02 ...  2.7734451e-02
   5.5404282e-01  3.6642689e-01]
 [-1.7587657e-03 -2.1933925e-03 -1.0330058e-03 ...  1.7780085e-03
   4.6300455e-03  6.5461951e-03]
 ...
 [-1.6813785e-03 -2.0501695e-03 -9.5152325e-04 ...  1.6649482e-03
   4.4115409e-03  6.1889854e-03]
 [-4.1582165e-04 -5.7733117e-04 -2.5383366e-04 ...  4.6768418e-04
   1.1879146e-03  1.6576402e-03]
 [-4.1386951e-04 -5.2588817e-04 -2.3496113e-04 ...  4.1550750e-04
   1.0977287e-03  1.5577230e-03]]
layers.2.weight: [[ 1.2851015e-03  3.4595925e-01 -3.5061850e-03 -7.2830409e-04
  -2.1250620e-03  1.6864254e-01 -3.6657270e-04  2.0691450e-03
  -3.4611595e-01 -4.1953611e-01  4.8749992e-03 -2.8221691e-03
   2.3121712e-01  5.2635872e-04  1.2450005e-03 -2.5425811e-04
   0.0000000e+00 -2.5335955e-03 -2.0654905e-03  0.0000000e+00
  -4.5043034e-03 -1.0640019e-03 -9.9188618e-02  8.5220177e-04
  -2.6848183e-03 -5.4314786e-01  1.4016102e-03 -9.4515347e-04
  -9.7533909e-04  4.5959060e-03 -2.0415338e-03  1.3120123e-03
  -2.4613126e-03  1.1981392e-03 -3.8464010e-01 -1.3129740e-03
   1.5000396e-03  1.5492635e-03 -2.0859065e-03 -1.4424324e-03
  -3.9852797e-03 -2.1966717e-03  2.6847732e-03 -8.1020163e-04
  -3.6947213e-03 -1.6834917e-03  3.6360237e-01  2.3484044e-03
   2.4233228e-03  2.9985172e-01 -4.5820056e-03  0.0000000e+00
  -1.5103521e-03  4.0703375e-04  1.6201088e-03 -6.3529261e-03
   4.0857086e-01  2.3293402e-03  2.6253292e-03  4.8196903e-01
   5.2570910e-03  1.9956583e-01  2.9460078e-03 -2.0845572e-03
  -3.9597061e-01  5.1758983e-03 -1.5602816e-03  2.1977127e-03
   5.7857885e-04  1.8435094e-03  2.4481122e-03  2.7015654e-03
  -7.0600363e-04 -2.7103952e-04 -2.1073860e-03  1.9288665e-03
  -3.7292132e-01 -1.7611119e-03 -7.6497602e-04  3.2592115e-03
  -4.0236738e-01  1.5426702e-03 -3.3234534e-01 -4.3190820e-03
  -2.1988819e-03  2.9432685e-03 -2.2586165e-03  1.3918119e-02
  -1.6533093e-03  4.1406667e-03  7.7145938e-03 -1.1507692e-03
  -4.4707352e-01  2.2693058e-03  6.4899924e-04 -1.6678275e-03
  -4.2585140e-01 -1.2314495e-03 -1.9691302e-03  3.8900571e-03
  -1.7842802e-04 -1.2808006e-03  1.1796679e-04 -1.0916166e-03
  -7.8021281e-04 -1.1922458e-02  2.6645053e-03 -6.0600499e-03
  -2.7782628e-01  6.9700566e-04  4.6889461e-03 -3.4412171e-04
   3.3357344e-03 -3.6740392e-01 -7.3423353e-04 -2.9278081e-03
  -3.4922698e-01 -3.9933859e-03 -4.6675955e-03  3.0468567e-03
   3.4853303e-01 -3.1664845e-01  9.0824737e-04  2.2110138e-03
  -3.9332751e-03 -3.3236537e-03 -8.8581123e-04 -8.4007060e-04]]

Final Loss: 3.5577
Distance Metric: 26.4275
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 945

================================================================================

