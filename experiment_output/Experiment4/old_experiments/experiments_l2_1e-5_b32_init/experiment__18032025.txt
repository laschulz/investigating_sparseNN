Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

nonoverlappingCNN_relu -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[ 0.4810051  -0.64454204 -0.42083195 ... -0.3634321  -0.06290625
   0.09859321]
 [-0.11631724 -0.3606294   0.1159865  ... -0.21191816  0.17052358
   0.34569478]
 [-0.31483716  0.01680321 -0.17546973 ... -0.00416358 -0.38317564
  -0.15696813]
 ...
 [-0.14344975 -0.03559243 -0.6096548  ... -0.6085177  -0.37937582
  -0.0988949 ]
 [ 0.33463296  0.3925307  -0.5281123  ... -0.16331036  0.47776946
   0.44309044]
 [ 0.31091514  0.5043071   0.08890579 ... -0.56960565 -0.1065621
  -0.14720033]]
layers.1.weight: [[ 0.11370835  0.11288647 -0.14203873 ...  0.00042687 -0.04537009
   0.20496689]
 [ 0.23383558 -0.00624116  0.16969745 ...  0.04989573  0.2923397
   0.13224149]
 [ 0.07133437  0.06712355 -0.06169697 ...  0.18054318 -0.1319602
   0.06812667]
 ...
 [ 0.30678892 -0.07054871  0.0014911  ... -0.05910727  0.17619446
  -0.09335285]
 [-0.09077823 -0.04252871 -0.13188542 ... -0.07657048 -0.00731371
  -0.08606492]
 [-0.07752478  0.1437754   0.07572903 ...  0.26450038 -0.0845154
   0.13417134]]
layers.2.weight: [[-3.09397876e-01 -5.88387959e-02 -8.68468165e-01 -4.92151044e-02
  -4.44010109e-01  2.50389557e-02 -1.10267639e-01 -1.38490841e-01
  -4.59955961e-01 -2.00672075e-03 -1.93850681e-01 -8.86617303e-02
  -2.03915417e-01 -3.09308380e-01 -5.07824570e-02 -6.91433430e-01
   8.95178393e-02 -1.85123503e-01 -2.80705422e-01  1.15713708e-01
  -1.32056728e-01 -1.47124469e-01 -4.54398453e-01  1.69553645e-02
  -6.38013899e-01 -1.29598260e-01 -2.29486689e-01 -2.13850290e-01
  -1.41403347e-01 -1.50591591e-02 -3.56487855e-02 -2.19869956e-01
  -2.98139393e-01 -7.99419641e-01 -1.71550959e-01 -1.67738393e-01
  -2.61268318e-01 -4.21074033e-01 -1.11611620e-01  1.15569256e-01
   1.55926511e-01 -1.27128670e-02 -6.46366775e-02 -3.14744890e-01
  -9.28235769e-01 -1.81632265e-02  6.52219518e-04  9.70876440e-02
   2.09248349e-01  3.07508856e-01 -3.86763781e-01  2.00879022e-01
  -1.35917410e-01 -1.83138490e-01 -1.00421570e-01 -2.41489872e-01
   2.11937949e-01 -1.91577166e-01  5.38726822e-02  1.89255521e-01
  -4.30299819e-01 -2.45108102e-02 -1.26868322e-01  5.79711907e-02
  -1.62960872e-01 -2.21552514e-02  1.73455805e-01  4.97194827e-02
  -6.09235346e-01 -1.61257759e-01 -1.03172943e-01 -1.75929010e-01
  -8.34120214e-02 -1.43681705e-01  6.49669487e-03 -1.44343451e-01
  -4.54150029e-02 -1.79242328e-01 -3.72877121e-02 -4.10819314e-02
  -1.57056406e-01 -9.47906449e-02  4.69908267e-02  7.73229599e-02
   8.75184312e-02 -3.74422640e-01  1.88867241e-01 -1.26885846e-01
   1.15341302e-02 -1.26198471e-01 -5.49599640e-02  1.91356733e-01
  -2.78263927e-01 -9.64139476e-02 -5.88987805e-02 -1.01139389e-01
  -3.11129913e-02 -2.07013905e-01 -3.14781815e-01  5.34743890e-02
   1.02793023e-01 -1.46343008e-01 -5.21733224e-01  4.83183973e-02
  -6.93026125e-01 -1.33964911e-01 -3.31552982e-01  2.60679394e-01
  -1.33381888e-01  1.74161404e-01 -9.92362946e-02 -1.06127776e-01
  -3.46639931e-01 -4.02563214e-01  8.50355327e-02 -4.28028643e-01
  -3.12227130e-01 -2.60954827e-01 -2.79282033e-01 -3.63186955e-01
  -2.05614105e-01 -8.68407041e-02 -6.44699574e-01 -1.19621634e-01
  -1.63380578e-01 -1.65888667e-01 -5.83046786e-02 -6.05572015e-02]]

Final Loss: 4.8882
Distance Metric: 38.6048
L1 norm: 0
L2 norm: 1e-05
Batch size: 32
Clipping: 0

================================================================================

nonoverlappingCNN_tanh -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[-0.00958435 -0.06057701 -0.42177314 ...  0.40473402 -0.01114423
   0.24236484]
 [ 0.38070792  0.06705701 -0.01995327 ... -0.44331688  0.61969054
  -0.76928765]
 [-0.20705964 -0.14738092  0.48460436 ... -0.04876448 -0.4600201
   0.17372078]
 ...
 [ 0.34511834  0.36915067  0.10969807 ... -0.18071796 -0.11439902
  -0.04361908]
 [ 0.06747789 -0.46131092  0.46596393 ...  0.65170765 -0.47890934
  -0.65412325]
 [-0.16932094  0.10790247  0.05969256 ... -0.38993657  0.02491634
   0.46065205]]
layers.1.weight: [[-0.053685   -0.11527254  0.06442829 ...  0.04866288  0.0623528
  -0.06637689]
 [ 0.00395828  0.11692818 -0.10503869 ...  0.12932697 -0.01809233
   0.01072348]
 [ 0.06167839  0.04346115  0.01908924 ...  0.07550835 -0.05092528
   0.12051643]
 ...
 [-0.10487407  0.14409024  0.1558543  ... -0.12791823  0.11532153
  -0.06954074]
 [-0.14231315 -0.11183782 -0.21829513 ...  0.0813235   0.07519592
  -0.2099879 ]
 [-0.04809116 -0.1134242   0.15543291 ... -0.1265538   0.00788226
  -0.00489968]]
layers.2.weight: [[ 0.23136748  0.14435078  0.16623347  0.30126575  0.12552288  0.2976834
   0.22553779 -0.14727777  0.19818382  0.00488976  0.2342967   0.05828562
   0.16450469  0.1628505  -0.53728276 -0.11812305 -0.11137921  0.01060022
  -0.18735169 -0.07694801  0.04889776 -0.18864462  0.16897744 -0.18924467
  -0.25712106 -0.2124074   0.28036666 -0.14674675 -0.02244035  0.16836582
   0.10300782 -0.12877432  0.00757388  0.13528803  0.2409716   0.18086867
   0.2340662   0.02549134 -0.23142357 -0.18670693 -0.10244306 -0.21951368
  -0.23137487  0.5139771   0.15862682 -0.04485201  0.44399127  0.2720055
   0.17696032  0.13874711 -0.12454233  0.12224727  0.14204043 -0.07469657
   0.14348508  0.05166567 -0.20176603 -0.04937749 -0.12454985  0.18814819
  -0.00670064 -0.09422141  0.04561312 -0.1962794  -0.06947424  0.20293081
   0.01146982 -0.11056598  0.02340238  0.06145822 -0.16956982 -0.07596364
  -0.1489523  -0.02397961  0.21981676 -0.2809399  -0.1890727   0.08016457
   0.20614848 -0.0929342   0.0874439  -0.02671996 -0.07970905  0.26535913
   0.1687872   0.07476302  0.33180544 -0.26957846 -0.53499854 -0.0484974
   0.07771786  0.27472076 -0.17763706  0.01664237 -0.15068063  0.15081361
  -0.22135499 -0.14110702 -0.22517352 -0.1844748   0.04877022  0.12535425
  -0.00433143  0.19603215  0.08917328  0.03947515 -0.1983503   0.07097432
   0.3275503   0.32769358  0.01183446 -0.07110938 -0.23695008  0.09243249
  -0.13037145  0.05719076  0.11955235 -0.30928484 -0.25322583  0.03935308
  -0.03451621 -0.2154306  -0.25290954 -0.21080105 -0.16079453 -0.1998305
  -0.12902683 -0.08522329]]

Final Loss: 0.0000
Distance Metric: 32.2996
L1 norm: 0
L2 norm: 1e-05
Batch size: 32
Clipping: 0

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.29218796 -0.7325005   0.30140334 ...  0.45691276  0.14626133
   0.2000928 ]
 [ 0.31303135 -0.01688122  0.12912335 ... -0.31899026  0.07858024
   0.18831962]
 [-0.02429647 -0.1357878   0.5618243  ... -0.03364482 -0.5747168
  -0.22743817]
 ...
 [-0.06340385  0.22644748  0.05624003 ...  0.23322041 -0.27810362
   0.31036693]
 [ 0.07504483 -0.38612568  0.63805956 ...  0.2746886  -0.02621345
  -0.68219644]
 [ 0.45673236 -0.582456   -0.20700791 ... -0.26378712 -0.28781062
  -0.2685464 ]]
layers.1.weight: [[ 0.04393503 -0.05117499  0.09268685 ...  0.18440878  0.11259063
   0.01247471]
 [ 0.07307668  0.1853679   0.11729432 ... -0.08499116 -0.03796944
   0.10462496]
 [-0.03059569 -0.07670444 -0.17675513 ... -0.01719366 -0.08687205
   0.07042628]
 ...
 [ 0.171191   -0.10009233  0.07697763 ...  0.08417156  0.10473862
  -0.01528266]
 [-0.09812384  0.06303187  0.31409392 ... -0.10899909  0.06268033
   0.09907906]
 [ 0.01350809 -0.03785233  0.01969784 ... -0.11216258  0.07724766
  -0.15490691]]
layers.2.weight: [[-9.74706411e-02  9.70439687e-02 -1.48935303e-01 -1.31288484e-01
   4.39365432e-02 -5.24783581e-02 -1.00115977e-01 -9.92047414e-02
   2.96067744e-01 -5.47131570e-03  7.75372982e-02  7.84479454e-03
  -1.65257677e-01  6.58811331e-02 -2.14361265e-01  9.55078900e-02
  -8.04321393e-02  3.44614014e-02 -2.23562106e-01 -5.91361709e-02
  -8.96707773e-02  1.26783364e-02 -5.93978167e-02 -2.09237859e-02
   1.44217524e-03 -5.44260368e-02  3.95706184e-02  1.24410145e-01
   8.66352245e-02  1.36716291e-01  3.08173392e-02 -2.53584772e-01
   9.30581689e-02 -4.57188040e-02  4.80564870e-02 -2.16627959e-02
   1.53525293e-01 -2.13936672e-01  5.29336073e-02 -5.62003581e-03
  -6.91029206e-02  4.60610129e-02 -1.89106092e-01  7.74658993e-02
  -1.22629464e-01  1.15904622e-04 -1.66670144e-01 -4.93329167e-02
   3.42347860e-01 -1.98239107e-02  1.39847817e-02  2.05822978e-02
  -5.62897660e-02 -6.99059619e-03 -1.45850912e-01  6.06095605e-02
  -2.75642332e-02  1.14824176e-01  3.94951552e-02  1.41914979e-01
   8.71154740e-02 -4.60803583e-02  1.21834874e-01  1.15383737e-01
   2.32300796e-02  1.85218621e-02  1.14099413e-01 -2.59108663e-01
  -3.46854664e-02 -7.29243532e-02  1.74020827e-01  2.21551910e-01
  -4.56136279e-02 -3.94892722e-01  2.15046294e-02  1.79068461e-01
   1.68933012e-02 -2.26626590e-01  1.23194888e-01 -2.44681105e-01
   1.86372967e-03 -9.59008858e-02 -7.99755082e-02  6.89964816e-02
  -5.61566278e-02  2.11454228e-01  2.81035423e-01 -1.51654243e-01
  -1.40291363e-01  4.80841249e-02  3.03011257e-02 -4.59945053e-02
   1.00880876e-01 -2.44431421e-01 -7.83828646e-02  8.84076059e-02
  -1.35224029e-01  8.43093619e-02  5.63945770e-02  6.70899227e-02
  -7.56693035e-02  6.67211637e-02  3.83674130e-02 -1.67018145e-01
   4.87779751e-02  8.73567350e-03 -6.25427142e-02  2.56260466e-02
  -7.00782612e-02  7.05517903e-02  9.44586247e-02  9.86065529e-03
   3.86778601e-02 -5.11438660e-02 -1.37919426e-01  1.57128260e-01
   2.85560973e-02 -2.10395947e-01  2.03491136e-01  8.78010644e-04
  -9.50621143e-02  1.18228510e-01  4.40892726e-02 -6.34621382e-02
   2.92709358e-02  6.64309710e-02  1.14292782e-02  2.38553323e-02]]

Final Loss: 0.0003
Distance Metric: 34.4004
L1 norm: 0
L2 norm: 1e-05
Batch size: 32
Clipping: 0

================================================================================

