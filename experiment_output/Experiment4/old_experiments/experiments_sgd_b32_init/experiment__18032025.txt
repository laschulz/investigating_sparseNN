Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

nonoverlappingCNN_relu -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[ 0.04452137 -0.3781568   0.03645689 ...  0.12325039 -0.05325198
   0.47072044]
 [ 0.19079556 -0.22918054  0.04000828 ...  0.41890326 -0.23610082
   0.64952064]
 [-0.61008537  0.3886693   0.53140336 ... -0.31160915 -0.27066454
   0.3285094 ]
 ...
 [ 0.03069792  0.24539927 -0.90273935 ...  0.04439302  1.0230657
  -0.17311034]
 [-0.07572477 -0.9348014  -0.04614746 ... -0.23358044  0.8495073
   0.5935644 ]
 [-0.27579993 -0.1628377   0.323783   ...  0.3700313  -0.4509783
  -0.0325508 ]]
layers.1.weight: [[-2.0908581e-02  1.2355366e-02 -6.6259019e-02 ... -3.4719260e-04
  -3.7073684e-01 -1.4828537e-01]
 [ 5.0514624e-02 -4.6970144e-02 -1.2566413e-01 ...  2.0613253e-01
  -1.3698849e-01 -4.9614892e-03]
 [-2.1401193e-02  2.7830261e-01  1.0283686e-01 ... -9.3838722e-02
  -4.2104334e-02  1.8531622e-01]
 ...
 [-2.0584887e-01  9.9279210e-02  1.3251014e-02 ... -1.1484528e-01
   1.9624156e-01  1.3867035e-02]
 [ 8.8422216e-02 -1.1646783e-01  2.2703996e-02 ...  1.1126707e-01
  -6.2670838e-03 -3.0562204e-01]
 [ 8.3990932e-02  2.1797238e-01 -3.0912638e-02 ... -7.4291028e-02
  -8.7174676e-02 -1.7618543e-01]]
layers.2.weight: [[ 0.04849218  0.21920334 -0.1607076  -0.0923304   0.09679022  0.08830338
  -0.18707925  0.27731353 -0.05071579  0.25059927  0.04572994  0.10043409
   0.13283975  0.04315512  0.11005659  0.19165434 -0.26737902 -0.17705274
  -0.47606847 -0.11464254 -0.2050642  -0.07240875  0.01599052 -0.00473154
   0.26441327 -0.08487977 -0.2551498   0.21640725 -0.00201017 -0.00866879
   0.1258754   0.11740347 -0.14181429 -0.14670542 -0.17327401 -0.15330894
  -0.13015434  0.2981121   0.3363005  -0.0857094  -0.31748694  0.01264893
  -0.39427802  0.02238208  0.15901421 -0.42726615 -0.12015414  0.16404651
   0.2426111   0.02891737 -0.22932042 -0.28420228  0.18271935 -0.10613815
  -0.04556699  0.15418333 -0.32856232 -0.10300558  0.19273128  0.11778738
  -0.35268265 -0.05622837  0.11507536  0.24934061  0.18158732  0.04909929
   0.13004947 -0.15227167 -0.5960094   0.1951907  -0.06619388  0.0724026
   0.25832993  0.26235297  0.03102782  0.27761117 -0.01351111  0.2279781
   0.19573323 -0.03283449 -0.22572604 -0.24451947 -0.10374048 -0.08729906
   0.01438986 -0.20287614 -0.23756525 -0.06303465 -0.04161772  0.1260514
  -0.0527981  -0.21579756  0.08102442  0.03516263 -0.13498253  0.22191414
   0.0060285  -0.13435496 -0.0583805  -0.05548696 -0.08593217 -0.16738485
   0.1903001   0.131009    0.07713799  0.22194491  0.01345619  0.1618535
   0.16153821  0.1913412   0.08928127  0.02845288 -0.11012099 -0.07244796
   0.32781976  0.1604138   0.150272   -0.09255814 -0.13690165 -0.23235574
   0.05899268 -0.1829796  -0.12052161  0.23173423 -0.05081537 -0.19717431
  -0.31339544 -0.08322553]]

Final Loss: 0.0068
Distance Metric: 37.6772
L1 norm: 0
L2 norm: 0
Batch size: 32
Clipping: 0

================================================================================

nonoverlappingCNN_tanh -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 0.11561007  0.10061193  0.20501623 ...  0.03907292 -0.17317103
   0.19392048]
 [-0.33213234 -0.07768126 -0.32700825 ... -0.83218646 -0.37397292
  -0.27876267]
 [-0.04006969  0.4855391  -0.28896108 ...  0.27844334 -0.34524703
  -0.41033337]
 ...
 [ 0.27017668  0.31062925  0.10240583 ... -0.31439558  0.38914928
  -0.09189352]
 [ 0.80319196 -0.05413986  0.41789463 ...  0.55523866  0.3825293
  -0.02880796]
 [-0.00844194  0.1714548   0.24630071 ... -0.4063051  -0.14192058
   0.6628386 ]]
layers.1.weight: [[-0.15170957  0.04460866 -0.05761854 ... -0.15951611  0.28641897
   0.07225778]
 [-0.12798737 -0.07641612  0.06606552 ... -0.14711507 -0.14810859
   0.01582392]
 [ 0.13770224 -0.12608746 -0.0609279  ...  0.09474021  0.09578385
  -0.31514356]
 ...
 [ 0.04928511 -0.07692792 -0.11672197 ...  0.13464896 -0.09379568
  -0.02015453]
 [-0.02821212 -0.08934211 -0.01388043 ...  0.10669147 -0.11040839
  -0.22832929]
 [-0.05749702  0.00355302  0.0268763  ... -0.09277731 -0.05359505
   0.09052193]]
layers.2.weight: [[-0.15014361  0.14661647 -0.06391236  0.05654809  0.15530618  0.2228248
  -0.03289185  0.13574897  0.4962097   0.02953355  0.03179143 -0.07949008
   0.03104104 -0.12060696 -0.13547383 -0.11841614  0.15945046 -0.0668747
   0.13965952 -0.09585489 -0.09208447 -0.13083151  0.16188373 -0.01605568
   0.01662154 -0.13908707  0.01637015  0.25256458 -0.15169783 -0.2099637
  -0.2969681  -0.06782891 -0.14464466  0.18439658 -0.0490653   0.06952684
  -0.02645678  0.43570322 -0.13625376 -0.0525643   0.18275501  0.10278574
   0.05842371 -0.06302334  0.14664172 -0.53071797  0.14906044 -0.16825993
   0.1867534   0.01195864 -0.14867614  0.01339239 -0.14530267  0.02643105
   0.20978224  0.10120279  0.1407378  -0.06155076 -0.17276762 -0.05751408
  -0.13299339 -0.13044381  0.11126521 -0.01114489 -0.17957261 -0.18636669
  -0.160049   -0.2172766   0.03633039  0.10747969  0.13224603 -0.0764197
  -0.18930037  0.04886488 -0.02175588  0.0073128  -0.02254422  0.20830236
   0.0043662   0.31331933 -0.18532711  0.07857978  0.21555188 -0.13939512
  -0.10553031 -0.11873646 -0.11112656 -0.03536557  0.11647619  0.13708538
  -0.07711303  0.06305914 -0.0465443   0.31073827 -0.0400948  -0.12447395
  -0.07824752 -0.20284484 -0.06900413  0.22583315  0.16037597 -0.06234521
   0.16322763 -0.111566   -0.07829247 -0.27387324  0.1323237  -0.10041242
   0.21689057  0.05703351 -0.0316944  -0.13341925  0.10968719 -0.07035921
  -0.0557253   0.06839774  0.07681519  0.1733542  -0.13860802  0.05699376
   0.16954617  0.08568383  0.18200885 -0.15734589 -0.04453991 -0.0837453
  -0.16538629  0.14991751]]

Final Loss: 0.0000
Distance Metric: 37.2263
L1 norm: 0
L2 norm: 0
Batch size: 32
Clipping: 0

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.4020993  -0.14574386  0.041665   ... -0.2517627  -0.5438451
  -0.07032412]
 [ 0.5357952  -0.13573012 -0.26769713 ... -0.13179871  0.6085908
  -0.12095795]
 [ 0.5383942   0.72972965  0.18219997 ...  0.49860325  0.00562031
  -0.13859253]
 ...
 [-0.89707273  0.3751828   0.67790276 ...  0.18957919 -0.40462157
  -0.53897285]
 [ 0.31483483  0.24269265 -0.20126194 ... -0.5589672   0.5357738
  -1.131051  ]
 [ 0.9510073  -0.48741925  0.18220334 ...  0.8382846   0.2137246
   0.20747517]]
layers.1.weight: [[-0.08641365 -0.2225673   0.09424342 ... -0.02083287 -0.02882048
  -0.09905165]
 [-0.07116038 -0.08366036 -0.20049849 ...  0.17553455 -0.05076943
   0.10395558]
 [ 0.06691477 -0.01432107 -0.1089718  ... -0.15312716  0.15351678
   0.02317787]
 ...
 [-0.06650595 -0.23420016  0.04967055 ... -0.09389582 -0.03601292
   0.07514951]
 [-0.28682685 -0.06507193 -0.12027441 ... -0.36209372 -0.02356067
  -0.00294454]
 [ 0.06681712  0.00040388  0.08188742 ... -0.23452741  0.23839726
   0.03825828]]
layers.2.weight: [[ 0.06680615 -0.18383226 -0.09994151  0.28819138 -0.31258866 -0.0354647
  -0.24085948 -0.23864967  0.0507683   0.01807204  0.23910448 -0.08368561
   0.05333903  0.05745347  0.02756339  0.2527639  -0.01120211 -0.1670902
   0.09353143  0.06539708  0.4210163  -0.03772425 -0.07699788 -0.29967332
  -0.16077657 -0.26516882  0.09234223  0.20609319  0.0923227   0.14900729
   0.3009671  -0.04534775 -0.1735844   0.03695805  0.67045236  0.02630794
  -0.01781666 -0.05691516  0.10315493  0.07547884  0.06960011 -0.4244442
  -0.22864965  0.0410221  -0.0882224   0.2933728  -0.09650857 -0.23429021
   0.10189513 -0.14321348 -0.0818899   0.02804659 -0.0444126   0.33029413
   0.19967173  0.25057328  0.06627969 -0.0937769   0.1668359   0.22657937
   0.12015741  0.48295835 -0.21671355  0.09284288  0.11754382 -0.01233484
   0.1094005  -0.12240142  0.20916991  0.12408142 -0.16719869  0.1635253
  -0.05225791  0.03648908  0.0220273   0.13487215 -0.15415092 -0.04419567
   0.04627009 -0.12676978  0.06554215  0.34779438  0.16105367 -0.15210313
   0.0138347  -0.01755024 -0.04877576 -0.14246072 -0.08269817 -0.07019917
  -0.18063612  0.00368556  0.08547635 -0.05948724  0.01006691 -0.08293986
  -0.0943024  -0.08456936 -0.47137302  0.04197647 -0.038601   -0.09977479
   0.05186683  0.1823847  -0.22881144  0.03976322 -0.33449417  0.25883687
  -0.26164457 -0.14307438  0.3254423   0.28774598 -0.266912    0.2610561
  -0.30906114 -0.11754149 -0.29188368  0.02182961  0.02785015 -0.03457211
   0.16893381 -0.19094579 -0.01412269  0.08183295 -0.26409143 -0.22619365
   0.11933482  0.25499442]]

Final Loss: 0.0001
Distance Metric: 37.7608
L1 norm: 0
L2 norm: 0
Batch size: 32
Clipping: 0

================================================================================

