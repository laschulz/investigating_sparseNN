Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.01413996  0.01522301 -0.00457093 ... -0.01279109  0.01353688
  -0.00447274]
 [-0.00481015  0.00517944 -0.00155494 ... -0.00435251  0.00460566
  -0.00152154]
 [-0.00942523  0.01014837 -0.00304693 ... -0.00852764  0.00902437
  -0.00298155]
 ...
 [-0.01984415  0.02135994 -0.00641476 ... -0.01794431  0.01899444
  -0.00627727]
 [-0.01205515  0.01297917 -0.00389696 ... -0.01090593  0.01154122
  -0.00381338]
 [-0.01770525  0.01905878 -0.00572317 ... -0.0160128   0.01694801
  -0.00560067]]
layers.1.weight: [[-0.00195261 -0.00071558 -0.00132684 ... -0.00271256 -0.0016757
  -0.00242711]
 [ 0.00556415  0.00182892  0.00367465 ...  0.00785832  0.004728
   0.00699697]
 [ 0.00228291  0.00073495  0.00149989 ...  0.00323368  0.00193634
   0.00287661]
 ...
 [ 0.00499431  0.00164121  0.00329813 ...  0.00705399  0.00424362
   0.00628048]
 [-0.00413371 -0.00142304 -0.00276252 ... -0.00579873 -0.00352684
  -0.00517339]
 [-0.00504686 -0.00171419 -0.00336106 ... -0.00709418 -0.00430074
  -0.00632514]]
layers.2.weight: [[ 0.08852015 -0.26842585 -0.11083601  0.2701025   0.1980014   0.14663585
  -0.02730447  0.1370337   0.08265912 -0.08574612 -0.23631203 -0.29091096
   0.32861748  0.06069125  0.01228323 -0.20095882  0.05475768  0.1927309
   0.13618053  0.2053787   0.23734924  0.13688402 -0.17319141  0.09560392
   0.18528011 -0.04869001 -0.00505093 -0.0546389  -0.21117571 -0.22413835
  -0.05764417  0.37943843 -0.02383636 -0.12859477 -0.12814622 -0.25496817
  -0.13005525 -0.02759889 -0.06746883  0.06074456 -0.20741263  0.24169365
   0.02474624 -0.05736965 -0.01515508 -0.10795395 -0.19256976 -0.11884513
   0.01446596 -0.14125426 -0.29610756  0.17252685 -0.1457295   0.06362154
   0.14932875  0.34011146 -0.08706462  0.04687096  0.11585369 -0.13565162
  -0.29954216  0.22716041  0.23345351  0.06921551 -0.127622    0.16849554
   0.01608598  0.04054398 -0.02024162 -0.41700962  0.13741827 -0.04825279
  -0.04296232  0.04769406  0.02101322  0.39091212  0.12276318 -0.25594977
  -0.01976077 -0.21116601 -0.10770366  0.04441493 -0.17880222  0.25395194
   0.4573631  -0.00864485  0.13876276 -0.03790558 -0.09282783 -0.07370517
  -0.05866248  0.00238647 -0.28739437 -0.09994895  0.21225445  0.05503898
   0.08078264 -0.12920496  0.19639704 -0.30003354  0.20027867 -0.04024878
   0.18742554  0.21327473  0.08467549 -0.04075464  0.13493033 -0.11345383
  -0.13676746 -0.15501884  0.01075218  0.13585192 -0.28861445 -0.08904524
  -0.10207749  0.00364296 -0.17895871  0.02247025 -0.04832692  0.12835248
  -0.014027   -0.26380542  0.15329964  0.19380593 -0.15019614 -0.24076219
   0.19434802  0.23923795]]

Final Loss: 0.0003
Distance Metric: 10.6387
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 2399

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[-0.08454673  0.0300061  -0.07987567 ... -0.11977607  0.03853178
   0.12225678]
 [-0.05576926 -0.01389469 -0.00592883 ...  0.19722924 -0.12297305
   0.16611142]
 [ 0.00989977 -0.03254577 -0.0478252  ...  0.09323071 -0.05005533
  -0.00966442]
 ...
 [ 0.09291392  0.10881361  0.         ... -0.1225194  -0.09382739
   0.02166055]
 [-0.08714081  0.04452961  0.06532977 ...  0.03460288  0.09388831
   0.0967925 ]
 [-0.04140586 -0.08337992 -0.03156824 ... -0.05374043  0.11693735
  -0.00911733]]
layers.1.weight: [[-0.03820123 -0.03136477 -0.03103177 ... -0.02536942 -0.0221442
  -0.02342209]
 [ 0.0504561  -0.04500476  0.02753679 ... -0.09729056  0.0012472
  -0.00883613]
 [ 0.03460815  0.02017689 -0.0401802  ...  0.06866702  0.00725323
   0.01263672]
 ...
 [-0.03711769  0.05496661  0.0228982  ... -0.0153664   0.00613709
   0.02118023]
 [-0.07090306  0.04245788  0.04743076 ... -0.04873885  0.00130105
  -0.00154025]
 [-0.00824434 -0.01416289  0.04532756 ...  0.0240628   0.05013227
   0.06669167]]
layers.2.weight: [[-0.40316057  0.18122336 -0.24050526  0.1471896   0.2147332  -0.25591293
  -0.3257518  -0.2600109  -0.3328028  -0.30689114  0.22867191 -0.38870838
   0.2256092  -0.4154966  -0.21228844  0.25623438 -0.2659519   0.254946
  -0.31752336 -0.35477412  0.19204456  0.17232123  0.09231459 -0.27205518
  -0.3872967   0.2338246   0.21725576  0.1609945  -0.2692771   0.13092889
   0.21595332  0.12303022 -0.3185759   0.1675346   0.18958294  0.00482674
   0.17554854  0.17691052 -0.33551762 -0.4082249  -0.25420249 -0.37120983
   0.09500317  0.00309769  0.17493597  0.00408342 -0.29996464 -0.29823822
   0.3297317   0.22688097 -0.30026293  0.15662315  0.24476284  0.00357495
  -0.34160343  0.16228533  0.15145013 -0.34974468 -0.23671836  0.18257861
   0.00529796  0.18153203  0.24807392 -0.32682326 -0.35442585 -0.35545155
  -0.3641303   0.19251563 -0.34048125  0.1691931  -0.36147276  0.08733953
   0.27422357 -0.2644641   0.14666186  0.15237401 -0.2176236  -0.42034265
  -0.375164   -0.33695808 -0.42699876  0.14017633  0.10287531 -0.38338616
   0.19901198  0.19607422  0.19732137  0.1654179   0.22331765  0.22084932
  -0.25098294  0.20665641 -0.35135633 -0.42365795  0.1802453   0.20581806
  -0.21554928 -0.28424776 -0.18042712  0.2945294  -0.37715784  0.07517049
   0.3193481   0.01071235  0.15967947  0.27980706  0.16322486  0.18635894
   0.15274724  0.0039189  -0.3355874   0.1795844   0.17237933 -0.35013315
   0.00464052  0.18867683 -0.28851983  0.21284403 -0.3851403  -0.32553297
  -0.31947416  0.19567677  0.2283143   0.28967977  0.00182771  0.13885616
  -0.281796    0.19098389]]

Final Loss: 0.0097
Distance Metric: 17.5318
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 912

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 0.00850523 -0.0093509   0.00343386 ... -0.01164757 -0.00645584
  -0.00536095]
 [-0.00798658  0.00880508 -0.00323207 ...  0.01093742  0.00605355
   0.00504413]
 [-0.00132232  0.00148765 -0.00054492 ...  0.00181027  0.00099153
   0.00084736]
 ...
 [-0.00087122  0.00097843 -0.00035895 ...  0.00120313  0.0006489
   0.00056064]
 [ 0.00157141 -0.00176728  0.00064743 ... -0.00215203 -0.00117814
  -0.00100697]
 [-0.00179485  0.00201732 -0.00073928 ...  0.00246203  0.00134434
   0.00115078]]
layers.1.weight: [[ 0.          0.          0.         ... -0.00010317 -0.00021583
  -0.00017343]
 [ 0.         -0.00012757  0.         ...  0.          0.
  -0.00018226]
 [ 0.         -0.00013648  0.         ...  0.00011335  0.
   0.        ]
 ...
 [-0.00017192  0.00010311  0.         ...  0.          0.
   0.        ]
 [ 0.         -0.00015013  0.00016516 ...  0.00018378  0.
   0.        ]
 [ 0.          0.0001339   0.         ...  0.00010497 -0.00012892
   0.00016975]]
layers.2.weight: [[0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.15747139
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.        ]]

Final Loss: 0.2525
Distance Metric: 7.5741
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 586

================================================================================

nonoverlappingCNN_relu -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.04601508 -0.06021084  0.02118171 ...  0.05113173 -0.0580969
   0.01628458]
 [ 0.98139167 -1.0583047   0.2874576  ...  0.7066098  -0.7634174
   0.2220996 ]
 [ 0.03877089 -0.05027529  0.01676185 ...  0.03631775 -0.04152219
   0.01200007]
 ...
 [ 0.14001769 -0.10216036  0.03882581 ...  1.6353735  -1.7724646
   0.54088587]
 [-0.00939635  0.01351351 -0.0035399  ... -0.01216707  0.01581843
  -0.00379966]
 [ 0.0334562  -0.04316225  0.01417591 ...  0.02895897 -0.0332054
   0.00971994]]
layers.1.weight: [[-1.2595115e-02  2.0756221e-01 -9.6839648e-03 ... -1.1385896e-01
   2.6773796e-03 -8.0821449e-03]
 [-7.0282221e-03  1.0330788e-01 -5.6284252e-03 ... -5.7495873e-02
   4.9555162e-04 -4.8180274e-03]
 [ 2.8870227e-02 -4.9643362e-01  2.1938009e-02 ...  2.7126271e-01
  -7.7157849e-03  1.8085478e-02]
 ...
 [ 3.1361595e-02 -5.3868961e-01  2.3815852e-02 ...  2.9447976e-01
  -8.3684353e-03  1.9630125e-02]
 [ 2.2344979e-02 -3.8616344e-01  1.6989807e-02 ...  2.1077763e-01
  -6.0047652e-03  1.4000607e-02]
 [-2.0160163e-02  3.4120694e-01 -1.5387786e-02 ... -1.8670036e-01
   5.0326535e-03 -1.2707871e-02]]
layers.2.weight: [[ 0.5007055   0.24919349 -1.1992382   1.1913223   1.8637106   1.7586355
  -0.9400401  -1.0498161  -0.97094375 -0.11886945  1.0849483   0.9491092
   1.3838226  -1.8640091  -1.2626756   0.5749495   0.29917106  0.48805436
   0.0159242  -0.33443972 -1.3647923   0.8138274  -0.25549477  0.7649439
  -0.90062803 -1.4770961   0.73032993  0.23288211 -1.5276419  -0.18067014
  -0.85929775 -0.54923415  0.9167948   1.580651    0.638825   -1.6575599
   0.45617807  0.6696171   0.31791344  0.48678428  0.17506309  0.47178394
   0.60630655 -1.4380735   0.15225449 -0.51922387 -1.234599    0.62015164
   1.5283179  -0.27945524 -1.3541902   0.8957605   1.1966249   1.6162434
  -0.3227825  -0.41556957  0.23953205  0.9449837   0.39016053 -1.7018988
  -0.9474486   0.12321718  0.6049591   0.91160166 -0.16559191 -0.01354328
   1.240144   -0.77056915  1.0937928  -0.62263364 -0.8505016   1.5405385
   0.49032396  0.12688887  0.06911695  2.2139468   0.1058721   1.4056982
  -0.9919561  -0.5411543  -0.27269617  1.2183167  -1.6225293  -1.4437611
   0.7755884   0.17445341  0.6885397   0.8054678   0.62502724  1.1517341
  -1.2618042   1.6010182  -0.617141    0.7469538  -1.770073    0.27004606
  -0.05703781 -0.25737187 -0.34233615 -1.1350777  -1.634166   -0.06514431
  -1.0413957   0.40606368  0.1713359  -1.2994798  -0.58874387  0.61544913
  -0.27209425 -0.25079322 -1.3543882  -0.50891733 -0.6988523   0.53958154
  -0.7803964   1.3191273  -1.0660331   1.9855902  -0.3868802   0.60337645
   1.5612577   0.13552876  1.2776705  -1.0448203   0.30690193 -1.3014599
  -0.93261915  0.8232926 ]]

Final Loss: 3.3206
Distance Metric: 38.0677
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 769

================================================================================

nonoverlappingCNN_relu -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[-0.00921872  0.00982528 -0.00328194 ...  0.0003548  -0.00097029
   0.        ]
 [ 0.0794337  -0.08680525  0.02676872 ... -0.0002149   0.
   0.        ]
 [-0.00037549  0.000187    0.00063861 ... -0.00477116  0.0055485
  -0.00199044]
 ...
 [-0.07139219  0.07862135 -0.02398435 ... -0.00054938 -0.00036413
   0.        ]
 [-0.00803596  0.00879628 -0.00285524 ...  0.00045101 -0.00074016
   0.00012505]
 [-0.00056371  0.00045744  0.00083855 ...  0.02480963 -0.02726339
   0.0085698 ]]
layers.1.weight: [[ 0.          0.         -0.01013233 ...  0.          0.
   0.00057389]
 [ 0.          0.         -0.01785837 ...  0.00016216  0.
   0.00100733]
 [ 0.08081583 -0.02664876  0.0119832  ... -0.0231387   0.07125592
   0.00165182]
 ...
 [ 0.11661555 -0.03845552  0.01731504 ... -0.03337571  0.10284025
   0.0023905 ]
 [ 0.0001492   0.         -0.06850162 ...  0.0005414   0.00014958
   0.003894  ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[-3.11842449e-02 -5.50016761e-02  4.64754373e-01 -2.22146977e-02
  -2.56885756e-02  1.66001562e-02  1.83680505e-02 -4.17327369e-03
  -1.18901531e-04  3.89544308e-01 -7.92358220e-02  1.94817353e-02
  -4.90618236e-02  0.00000000e+00  2.44384408e-02 -3.07516232e-02
  -9.12938774e-01 -2.76297942e-04 -1.18302643e-01  0.00000000e+00
   0.00000000e+00  1.03523463e-01  0.00000000e+00  3.31717208e-02
   1.50837138e-01  6.73154294e-02  2.63568103e-01 -1.20217726e-03
   4.87557143e-01 -8.67019072e-02  0.00000000e+00 -1.56953894e-02
   4.28915508e-02  0.00000000e+00 -2.67761443e-02  1.87954269e-02
   1.57519188e-02  2.49949172e-01  2.85523236e-01  8.79537165e-01
   0.00000000e+00 -4.03692015e-04  0.00000000e+00  0.00000000e+00
   1.41572535e-01  4.61185798e-02  3.83331060e-01 -1.18152305e-01
   0.00000000e+00  1.74110621e-01  7.74334967e-01  5.56874946e-02
  -2.23340541e-01 -5.22276349e-02  5.84830232e-02 -9.57764499e-03
  -9.19318497e-02 -3.37997645e-01  2.61016027e-03 -8.20121670e-04
   1.60870239e-01  1.72916949e-02 -1.34940885e-04 -6.80068089e-03
  -5.36590219e-02 -1.65326018e-02 -4.40226635e-03 -1.78447779e-04
  -4.90197092e-02  6.75889850e-03  3.90297128e-03 -9.98618528e-02
  -1.53346264e-04 -2.45230447e-04 -2.38996133e-01  5.01876846e-02
   0.00000000e+00  3.11534759e-03 -2.62807846e-01  3.91851738e-02
   8.90990794e-02  4.15999740e-02 -1.25361010e-02  1.16453409e-01
  -5.47836488e-03  1.02451362e-01 -1.07425891e-04  7.78328851e-02
  -6.54255622e-04  4.94966596e-01  0.00000000e+00 -6.13567352e-01
   4.71877217e-01  1.88982079e-03 -3.43508720e-01  0.00000000e+00
  -2.91718228e-04 -2.62170643e-01 -2.87754610e-02  2.44046953e-02
   0.00000000e+00  1.75393194e-01 -8.77667964e-01  1.54726382e-04
   1.66309755e-02  2.14100823e-01  6.49095714e-01 -3.78572592e-03
   2.74600442e-02 -3.92232955e-01  0.00000000e+00  2.08978169e-03
   1.07502617e-01  3.27340662e-01  2.31637672e-01 -1.11533189e+00
  -8.12255032e-03 -4.67968220e-03  3.11578084e-02  0.00000000e+00
  -3.52481931e-01  1.87454045e-01  4.96055409e-02 -2.55197167e-01
   4.25119549e-02  6.70728207e-01 -2.10873201e-01  0.00000000e+00]]

Final Loss: 0.0003
Distance Metric: 12.2233
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 845

================================================================================

nonoverlappingCNN_relu -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 0.06865031 -0.0589952   0.0233536  ...  0.05518236 -0.05350892
   0.00846113]
 [ 0.05147882 -0.04719828  0.01670038 ...  0.05911642 -0.07073065
   0.01498519]
 [ 0.00252947  0.04684417  0.18784794 ...  0.18845563  0.03319566
   0.09366964]
 ...
 [ 0.06796381 -0.05850096  0.02227286 ...  0.06439859 -0.07098608
   0.015273  ]
 [-0.06190893  0.06500913 -0.02151985 ... -0.06959388  0.08069789
  -0.0249793 ]
 [-0.0493562   0.05400945 -0.01800148 ... -0.07021465  0.07603122
  -0.01956209]]
layers.1.weight: [[-0.00103021 -0.00101225 -0.00157788 ... -0.0018644   0.0015168
   0.00053247]
 [-0.00013669 -0.00019403 -0.00030315 ... -0.00027232  0.00019271
   0.00032824]
 [-0.00101167 -0.0009271  -0.00128926 ... -0.00111195  0.00073002
   0.00037381]
 ...
 [ 0.         -0.0001071  -0.0001344  ... -0.0003394  -0.00010395
   0.000274  ]
 [ 0.00132378  0.00137997  0.00189069 ...  0.00210591 -0.00165298
  -0.00058998]
 [-0.0008992  -0.00043139 -0.0011056  ... -0.00071804  0.00059871
   0.00036536]]
layers.2.weight: [[-0.02711734 -0.00591273 -0.02278955 -0.0162751  -0.02358335  0.01320986
  -0.02717041  0.00206535  0.00920987 -0.00597129 -0.03003922  0.0263111
  -0.01498917  0.00936078 -0.02486539 -0.01865647 -0.02696481 -0.02410809
   0.03009193  0.02833727 -0.02491586 -0.02446205 -0.02058969 -0.0270232
   0.03035927 -0.02077282  0.02656     0.02455515  0.02734649 -0.00192988
  -0.01786587  0.00247542 -0.01192468 -0.00759161  0.02489257 -0.0131636
  -0.00442671 -0.03064413 -0.02301074  0.02937417 -0.33883476 -0.01198746
   0.00520606  0.01959168 -0.28150025 -0.01458376 -0.5178313  -0.02520973
   0.02712226 -0.01115167  0.0293382   0.9962627  -0.02314819 -0.03477564
   0.02200848 -0.02553237  0.02217926 -0.03079614 -0.01181749  0.0182803
  -0.01941402 -0.29952645  0.02040448 -0.02138712 -0.01769254  0.01943104
   0.01844109  0.01971698 -0.00329931  0.03050559  0.01047901 -0.02743917
   0.0185794  -0.01001419  0.01888945 -0.02145599 -0.02404273 -0.02607665
   0.8871555  -0.02238141  0.02520081  0.02279042 -0.01929817 -0.02462863
   0.03366778 -0.02346441  0.03122285 -0.01847331  0.39669743  0.0134597
   0.02081014  0.02595165  0.02051311 -0.02510047  0.02266351 -0.02090646
   0.0151876   0.9946086  -0.02163382  0.01992964  0.01150893 -0.01325342
   0.02913678 -0.0195856  -0.02150345 -0.03027377 -0.0220009  -0.03276442
   0.02427551  0.00919773  0.02206414 -0.02018494  0.01876832  0.01990998
  -0.01287773 -0.02676129 -0.01971326 -0.02103415 -0.0120813  -0.02178626
   0.8843081  -0.02334638 -0.02331072  0.01539022  0.02196492 -0.00233662
   0.03093656 -0.01804703]]

Final Loss: 3.5549
Distance Metric: 22.8091
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 559

================================================================================

nonoverlappingCNN_tanh -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[-5.4137711e-03  4.8753573e-03 -1.8938391e-03 ... -1.3325470e-02
   1.4316481e-02 -3.5248878e-03]
 [-2.9550218e-03  2.1823987e-03 -6.2589842e-04 ... -6.8779872e-03
   7.5946767e-03 -1.1178817e-03]
 [-2.8042563e-03  2.0202028e-03 -5.4241222e-04 ... -6.2849731e-03
   7.0068762e-03 -8.8627561e-04]
 ...
 [-3.2975629e-03  2.5627713e-03 -7.9424022e-04 ... -7.4539892e-03
   8.2448805e-03 -1.3166656e-03]
 [-6.3029133e-02  6.2314380e-02 -2.2235235e-02 ...  3.7660301e+00
  -4.1083965e+00  1.2614763e+00]
 [-3.2330987e-03  2.4874301e-03 -7.6912338e-04 ... -7.5860480e-03
   8.3363811e-03 -1.3812169e-03]]
layers.1.weight: [[-8.8499143e-04 -5.6590617e-04 -5.4241973e-04 ... -6.0401851e-04
  -2.9234109e-02 -6.0157751e-04]
 [-8.6602295e-04 -5.6699966e-04 -5.4498634e-04 ... -6.0272514e-04
  -2.7403578e-02 -6.0045370e-04]
 [-8.2038186e-04 -5.7012995e-04 -5.5167370e-04 ... -6.0000992e-04
  -2.2960339e-02 -5.9811177e-04]
 ...
 [ 4.1289762e-02  3.4133587e-02  3.3652112e-02 ...  3.5060130e-02
  -5.5872244e-01  3.4939613e-02]
 [-8.8301254e-04 -5.6603021e-04 -5.4266641e-04 ... -6.0388941e-04
  -2.9044952e-02 -6.0146162e-04]
 [-8.9596264e-04 -5.6534470e-04 -5.4103247e-04 ... -6.0484960e-04
  -3.0284652e-02 -6.0229527e-04]]
layers.2.weight: [[ 0.05708785  0.0540526   0.04668707  0.05820508  0.0463378   0.04684229
   0.0446308   0.05081815  0.0584494   0.05718395  0.04677262  0.04548422
   0.0503793   0.04506814  0.05265406  0.04680453  0.0521514   0.05085503
   0.0466723   0.0566473   0.04597058  0.04591328  0.05883865  0.04496931
   0.05218785  0.05875629  0.04856568  0.04616243  0.05762395  0.05828249
   0.05726109  0.04670558  0.04070144  0.04789116  0.05176383  0.05677884
   0.04612551  0.05825592  0.05295279  0.04497517  0.04478077  0.04472353
   0.04687047  0.05433553  0.05959849  0.04652089  0.05298631  0.05609691
   0.04627807  0.05857783  0.04419602  0.0511888   0.05100401  0.04329097
   0.04655771  0.04624714  0.05900192  0.0550383  -0.02659777  0.04823494
   0.05565269  0.05660802  0.04664572  0.04576096 -4.7112517   0.05103115
   0.03882209  0.04565976  0.04127701  0.04422653  0.05996051  0.04543327
   0.04066275  0.04803063  0.05347358  0.05572573  0.046521    0.04679256
   0.04560214  0.05889743  0.04615872 -7.4747195   0.04610933  0.05208554
   0.05436631  0.04711189  0.05421654  0.05310783  0.0474438   0.04618339
   0.05628512  0.04710121  0.04846575  0.04402477  0.04967555  0.04584407
   0.05238277  0.05256276  0.05403011  0.05402774  0.04866123  0.04784113
   0.04943924  0.04881762  0.05044293  0.0571935   0.04640541  0.05996716
   0.05777365  0.05104925  0.05836735  0.0450611   0.04938525  0.03809508
  -0.21361114  0.04973644  0.05526654  0.05627694  0.03619727  0.04576686
   0.04675045  0.05770464  0.05285773  0.03618138  0.04699877 -4.341567
   0.05677449  0.05883037]]

Final Loss: 0.1898
Distance Metric: 34.0923
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1979

================================================================================

nonoverlappingCNN_tanh -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[-0.06559266  0.04071051 -0.01718204 ... -0.30072457  0.3198075
  -0.10495482]
 [ 0.34512132 -0.3869068   0.1192993  ...  0.02556131 -0.03269319
   0.00105274]
 [ 0.04200366  0.09585721  0.1429567  ...  0.05853369  0.06674281
  -0.03087648]
 ...
 [-0.02399019  0.07169549 -0.04059831 ...  0.14454347  0.16273358
   0.19488311]
 [-0.00110953 -0.0124658  -0.02314444 ...  0.17729293 -0.00516654
   0.0294127 ]
 [-0.14862423 -0.01738232 -0.03475798 ... -0.19062859 -0.0329385
  -0.05122196]]
layers.1.weight: [[-0.08435357  0.0297284   0.01263395 ... -0.10585656 -0.03443618
  -0.02824618]
 [ 0.0024068  -0.0241808   0.05898348 ... -0.01440133  0.02808754
   0.05589166]
 [-0.0401971   0.03385698  0.01489682 ...  0.03618075  0.04063955
   0.00952077]
 ...
 [ 0.08081451 -0.02631971  0.05667103 ... -0.03840802  0.00036617
   0.04453686]
 [ 0.03900683 -0.04708023  0.08446128 ...  0.07316049 -0.03996547
  -0.03882932]
 [-0.01531706  0.06059194  0.0486495  ... -0.01975754 -0.06205656
   0.05429521]]
layers.2.weight: [[-0.3735518   0.25152338 -0.26804605 -0.27095518  0.2298809  -0.37018654
   0.39004168  0.26915777 -0.36715418 -0.7641789   0.1946111  -0.31419832
  -1.0396763   0.17827609 -0.23516984  0.03669565 -0.59444374  0.17148808
  -0.19964708 -0.30987895 -0.44299364 -0.34752586 -0.332144   -0.45352003
  -0.29422832  0.35819057  0.18952149  0.2252176  -0.38847607 -0.3077991
  -0.15112056  0.47773868  0.11152502  0.1423114  -0.17424408  0.33001503
  -0.21329747  0.32999003 -0.35635918 -0.3938948  -0.3303097  -0.3321544
   0.18206082  0.2834701   0.3293611  -0.332468    0.28579623  0.27429593
   0.13024263  0.36748996  0.14230989  0.29382628 -0.25975886 -0.26206005
   0.35557833 -0.3321526  -0.2653849   0.27435714  0.34839934  0.20638345
   0.23650113  0.2846649  -0.35494053  0.18504529  0.16640133  0.3377555
   0.37124857 -0.3922935   0.19614826 -0.61517364 -0.28895736  0.13327557
   0.22036831 -0.23220484  0.32130903  0.45848432  0.3337012   0.40247527
  -0.30631718  0.27871758 -0.37549117  0.21961538  0.29227477  0.4463315
   0.21081722 -0.34311637 -0.4534554  -0.25689834 -0.34283873  0.3557137
  -0.38640597  0.28402272  0.2829283   0.13017768 -0.37098703  0.2981751
  -0.3137632  -0.4438042   0.26349252 -0.23347552  0.33258116  0.2963226
   0.30521053 -0.30598748 -0.3255649   0.1660799   0.4245583   0.12724222
   0.22245277  0.23522864 -0.377554    0.10305058 -0.33920375 -0.4898003
   0.24651055  0.29112566  0.2598706   0.3377277   0.3511503   0.44225252
   0.33601338 -0.27639058  0.05399576 -0.33477235 -0.34310147  0.16726005
  -0.18677372 -0.31616   ]]

Final Loss: 0.1925
Distance Metric: 19.9187
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 1126

================================================================================

nonoverlappingCNN_tanh -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 0.00452255 -0.00542265  0.00226574 ...  0.01538753 -0.01644368
   0.00628186]
 [ 0.0100225  -0.01126423  0.0028313  ... -0.00521247  0.0049431
  -0.00204425]
 [-0.01118341  0.01252962 -0.00380267 ... -0.00160285  0.00212221
  -0.00094177]
 ...
 [-0.00059916  0.00012942 -0.00019217 ...  0.01005373 -0.01073355
   0.00450566]
 [ 0.01180032 -0.01352584  0.00385341 ... -0.00259341  0.00304909
  -0.00086244]
 [ 0.01517632 -0.01539542  0.00481493 ... -0.00725831  0.00797938
  -0.00309243]]
layers.1.weight: [[ 0.00015658 -0.00024621  0.0001355  ...  0.00010799  0.
  -0.00039571]
 [ 0.          0.00013745  0.         ... -0.00041438 -0.00033627
   0.        ]
 [ 0.00108348  0.         -0.00043204 ...  0.00021252  0.00055492
   0.        ]
 ...
 [ 0.00017927  0.00043148  0.         ... -0.00015745  0.00030853
   0.        ]
 [ 0.00036607  0.         -0.00053451 ...  0.00059347  0.00054399
   0.00061087]
 [-0.00169248 -0.00046283  0.00124831 ... -0.00061583 -0.0007784
  -0.00080008]]
layers.2.weight: [[ 6.45827176e-03  1.76930558e-02 -3.36960480e-02 -4.23247404e-02
  -1.90836564e-02 -5.62351793e-02 -8.52388504e-04 -2.53165048e-02
  -7.37048611e-02 -4.04075496e-02  3.60205472e-02  4.54361290e-02
  -2.78462307e-03  4.54885662e-02 -3.93796451e-02  4.47267070e-02
   5.56729175e-02 -4.97130677e-03  3.81087251e-02  2.13321485e-03
  -3.34644900e-03  1.66049674e-02 -5.27430512e-03 -4.44686450e-02
  -2.74773277e-02  4.93382243e-03  2.48291809e-02  2.76516359e-02
   2.47807652e-02 -2.21277103e-02  1.77563336e-02  3.11596300e-02
   8.17305967e-03  6.10288307e-02 -1.65246818e-02  1.28421159e-02
  -2.31001880e-02  2.49013249e-02  5.19200377e-02  1.43914772e-02
  -2.13096365e-02 -9.82747134e-03  6.25275970e-02 -1.11130858e-02
   3.21120508e-02  1.38645209e-02 -5.98735511e-02 -2.61839330e-02
  -1.51218260e-02 -6.02251887e-02  2.26037335e-02 -6.34676740e-02
  -3.72160273e-03  4.78716716e-02  1.00335085e+00 -4.27299477e-02
  -2.84559168e-02 -2.87273806e-03  5.95237799e-02 -2.35716589e-02
  -1.36593878e-02 -2.09058579e-02 -6.29491508e-02  4.02673930e-02
  -8.63827690e-02 -4.74030897e-03 -1.01704466e+00  5.62902912e-03
  -4.70336527e-02  4.74505834e-02 -1.92462504e-02  2.03912128e-02
   2.61907969e-02 -9.56816971e-03 -4.07996587e-02  1.93561297e-02
   4.35399376e-02 -5.72199114e-02 -5.53588234e-02 -2.49218792e-02
  -4.74143513e-02  5.81426965e-03 -5.18713221e-02  3.59563492e-02
  -4.09049056e-02  7.17947818e-03  8.83905869e-03 -4.10813913e-02
   1.56912655e-02  6.07418828e-02  2.13246532e-02  3.82931940e-02
  -8.40888843e-02  2.67710369e-02  2.10943464e-02  4.39893827e-02
  -1.64863188e-02  6.73253322e-03 -1.41362455e-02  3.05245090e-02
   1.37314098e-02  3.00789829e-02 -5.11737466e-02 -2.44532321e-02
   1.20711531e-02 -4.89854254e-02  1.77029818e-02 -3.22347768e-02
   5.48226200e-02  3.35875787e-02  2.34820042e-02  1.93144958e-02
   2.43151337e-02  3.81772057e-03  6.49038926e-02  4.78746630e-02
   4.61510792e-02 -1.46597568e-02 -5.00281807e-03  5.92239276e-02
  -1.36999860e-02 -4.30190973e-02  2.81023933e-03  4.77634184e-02
   3.97465704e-03 -1.21041946e-02 -2.67040078e-02  5.76864332e-02]]

Final Loss: 0.0007
Distance Metric: 8.9202
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 1
stopped after epoch: 522

================================================================================

