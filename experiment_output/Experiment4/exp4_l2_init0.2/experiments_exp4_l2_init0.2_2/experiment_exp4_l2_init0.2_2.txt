Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.01430323  0.01576987 -0.00491074 ... -0.01350521  0.0143091
  -0.00450565]
 [ 0.00838521 -0.00924321  0.00287786 ...  0.00791575 -0.00838797
   0.00264055]
 [ 0.01200819 -0.0132385   0.00412229 ...  0.01133705 -0.01201321
   0.00378204]
 ...
 [ 0.02164557 -0.0238688   0.00743629 ...  0.02044293 -0.02165807
   0.00682119]
 [-0.01339302  0.01476609 -0.00459805 ... -0.01264542  0.01339828
  -0.00421873]
 [-0.00647802  0.00714142 -0.00222323 ... -0.00611555  0.00647994
  -0.00204009]]
layers.1.weight: [[-0.00090675  0.00039891  0.00060772 ...  0.001166   -0.00085417
  -0.00045568]
 [-0.00338309  0.00189176  0.00273529 ...  0.00499067 -0.00317071
  -0.00156075]
 [ 0.00216553 -0.00137037 -0.00193586 ... -0.00344767  0.00202324
   0.000944  ]
 ...
 [ 0.00310605 -0.00191662 -0.00272001 ... -0.00486732  0.00290388
   0.00137087]
 [ 0.         -0.00016315 -0.00019913 ... -0.00029533  0.
   0.        ]
 [ 0.002096   -0.00133023 -0.00187815 ... -0.00334296  0.00195802
   0.00091232]]
layers.2.weight: [[ 0.03775032  0.15264328 -0.10220949 -0.12744413  0.22947106 -0.19377723
  -0.18347093 -0.17512321 -0.10122326  0.2809102  -0.29944426  0.04249816
   0.11377706 -0.2414151   0.15123023  0.27484816 -0.16931905  0.06095182
  -0.21839394  0.21129604  0.2615918  -0.17880125  0.2569877  -0.05358343
   0.0827549  -0.01470965  0.25533378 -0.12236232 -0.02022243  0.32541558
   0.21162434  0.05938487  0.0436193  -0.19104147 -0.05626897  0.2375001
   0.3648493  -0.4336827   0.1169859  -0.05670135  0.21206467 -0.21196087
  -0.03874188 -0.18425737  0.18275096 -0.15785773  0.02975073 -0.3168258
  -0.11876501 -0.02433363 -0.22149386  0.1276639  -0.11249044  0.09579508
   0.0690232   0.02471131 -0.13793252 -0.13614532 -0.1772252   0.0802656
  -0.06468164 -0.3698633   0.03363781 -0.3435871   0.27139026  0.042094
  -0.15270597  0.0863029  -0.12476885 -0.0594121   0.1886503  -0.07529496
   0.2189417  -0.10817768 -0.10642666  0.19238587 -0.04403127 -0.21326073
   0.02719602 -0.02408141 -0.3469921   0.26531175  0.10108444 -0.02975842
   0.19497332  0.11160503 -0.06038887 -0.14982525 -0.2670939   0.05771879
   0.06576223  0.27271006  0.02359467 -0.15387885 -0.05489419  0.02789464
   0.1077555  -0.15009815 -0.15577959 -0.14287697 -0.05067757  0.12270886
  -0.14902759  0.3438145  -0.23823038  0.06904208  0.21837346  0.10942017
   0.03856843 -0.17514916 -0.06406     0.08515124  0.05658913  0.11063901
  -0.01962785  0.19526581  0.1473901  -0.10371599  0.2779642   0.17364725
   0.25223923  0.0182006  -0.08842144  0.04864771  0.03860657 -0.14528526
  -0.00648115 -0.09903372]]

Final Loss: 0.0003
Distance Metric: 10.7316
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 2197

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[ 0.02189724 -0.05131559  0.11473678 ...  0.087246    0.11130041
  -0.1499442 ]
 [ 0.05889518 -0.03035337  0.06487941 ...  0.22758946  0.02752533
  -0.02787138]
 [-0.07244043  0.1531811  -0.16152243 ...  0.0378562   0.02743179
   0.03335317]
 ...
 [ 0.05564985  0.08962039 -0.11352322 ...  0.17938073 -0.07457415
   0.07267761]
 [-0.11154434  0.01236591  0.01857395 ...  0.09863327  0.03717254
   0.02503529]
 [ 0.00788672  0.02300919  0.12699278 ... -0.00249511  0.04354088
  -0.21179497]]
layers.1.weight: [[ 2.19668116e-04  1.77863680e-04  1.13517970e-04 ...  0.00000000e+00
  -1.02942868e-04  0.00000000e+00]
 [ 6.32702336e-02 -4.23059240e-03 -1.32578477e-01 ... -1.06079904e-02
   3.69218178e-03 -9.92182568e-02]
 [-3.95455733e-02 -1.18097700e-01  2.53312979e-02 ...  1.34079810e-02
  -1.19952485e-02 -7.01059913e-03]
 ...
 [ 8.31905156e-02  2.21527591e-02 -3.21973339e-02 ...  1.61454249e-02
  -4.76167500e-02 -4.41015186e-03]
 [ 3.31099033e-02 -1.72252841e-02  5.05500697e-02 ...  3.41128036e-02
  -3.12828422e-02 -5.64540140e-02]
 [ 4.63243836e-04 -2.36433087e-04 -2.86865805e-04 ...  0.00000000e+00
   0.00000000e+00  3.67731816e-04]]
layers.2.weight: [[ 5.8958918e-04 -2.9743245e-01 -2.6607066e-01 -3.1132761e-01
  -2.6420280e-01 -2.3083696e-01 -4.2996359e-01  6.8641332e-04
  -3.8216624e-01 -3.6252150e-01  1.9144587e-01  2.1684176e-01
   1.7134230e-01  2.0024063e-01  2.5964767e-01 -3.0492228e-01
   6.8793492e-04  1.9518124e-01 -2.2579159e-01  1.7922067e-03
   2.1925482e-01 -3.3155066e-01 -2.2646725e-01 -4.2230740e-01
  -2.4790543e-01 -3.9336684e-01 -2.9007077e-01 -4.0792802e-01
  -4.2663226e-01  1.5514190e-01 -3.9466053e-01  3.0881318e-01
  -4.1546324e-01 -3.6929998e-01  1.5607290e-01  2.1584065e-01
   6.9740886e-04 -2.5194690e-01  2.8059924e-01 -2.5166428e-01
   1.9555248e-01 -2.5835994e-01 -2.5412399e-01  1.6383740e-01
   1.8924522e-01 -3.9188826e-01  2.1164371e-01  1.8043970e-01
  -2.5788605e-01 -4.3722373e-01 -2.9447913e-01  2.2793175e-01
   1.2098799e-01  1.7154688e-01 -3.8592258e-01  1.8015617e-01
  -3.1717351e-01 -2.3878111e-01  4.6651470e-03 -3.9312503e-01
   2.5152558e-01 -3.9163798e-01 -3.3371419e-01 -2.3932502e-01
   6.8916322e-04 -3.4073007e-01  1.1173787e-03  2.3698393e-01
  -2.6599658e-01 -3.9502868e-01  2.1401773e-01 -1.9575897e-01
  -3.7517670e-01  1.6407542e-01  2.3022327e-01 -3.1895214e-01
   1.3383324e-01 -4.1189119e-01 -3.3676058e-01  2.6130205e-01
  -3.0930239e-01 -3.7259164e-01 -2.2852361e-01  2.4084884e-01
   2.7161938e-01 -2.8937688e-01  1.7370430e-01  4.7507574e-04
   1.9792941e-01  2.4804911e-01 -2.5643322e-01  1.6482568e-01
  -3.9247280e-01  1.3579172e-01 -3.1764582e-01 -3.5999736e-01
  -2.7659199e-01 -3.6372766e-01  2.2652844e-01 -4.8789495e-01
   1.4549117e-01  1.7578082e-01 -3.1539473e-01  2.1280524e-01
   2.0919685e-01  2.6083240e-01  2.0596899e-01  2.3070757e-01
   1.0922340e-03  2.0669053e-01 -3.2693177e-01  2.3109944e-01
  -2.5015461e-01  1.9168669e-01 -3.0094776e-01  1.2017460e-01
   1.8463995e-01 -2.2674672e-01 -2.5149840e-01  2.2233956e-01
  -3.8943323e-01  1.5709220e-01 -3.0326843e-01 -2.9821613e-01
  -3.2080957e-01 -3.4659585e-01  2.6119572e-01  1.9023346e-03]]

Final Loss: 0.0096
Distance Metric: 18.1436
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 2019

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[-0.01190002  0.00971945 -0.00021142 ...  0.00298622  0.0114432
  -0.00635203]
 [ 0.02113851 -0.01715096 -0.00017542 ... -0.00578228 -0.0197048
   0.01150521]
 [ 0.01043939 -0.00853028  0.00021333 ... -0.00259341 -0.01006805
   0.00556123]
 ...
 [-0.01074487  0.00878032 -0.0002142  ...  0.0026732   0.01035511
  -0.00572544]
 [ 0.00044587 -0.00034765 -0.00092912 ...  0.00084663 -0.00339506
   0.00130264]
 [ 0.02054868 -0.01668311 -0.00012662 ... -0.00558588 -0.01920925
   0.011167  ]]
layers.1.weight: [[ 0.0001812   0.00029785  0.         ...  0.0001818   0.
   0.        ]
 [ 0.         -0.00017429  0.00026033 ... -0.00025208  0.
  -0.00013706]
 [ 0.00010041  0.         -0.0003275  ...  0.          0.
  -0.00010236]
 ...
 [ 0.00025323 -0.00028634  0.         ...  0.          0.
   0.        ]
 [ 0.          0.0001156  -0.00018507 ...  0.          0.
  -0.00010991]
 [-0.00023472  0.0002119   0.         ...  0.         -0.0001446
   0.00019066]]
layers.2.weight: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.         -0.18800463  0.          0.
   0.          0.        ]]

Final Loss: 0.2525
Distance Metric: 7.5710
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 548

================================================================================

nonoverlappingCNN_relu -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.0504396   0.05934276 -0.02105707 ... -0.03382477  0.03861904
  -0.0149203 ]
 [-0.07259069  0.08123527 -0.03137594 ... -0.04608148  0.05509066
  -0.02092372]
 [ 0.0342857  -0.04002902  0.01375271 ...  0.01926694 -0.02129365
   0.00730299]
 ...
 [ 0.02088098 -0.02435884  0.00828036 ...  0.01079335 -0.01173033
   0.00376213]
 [ 0.04122713 -0.04794183  0.01658285 ...  0.02364718 -0.02633739
   0.009171  ]
 [-0.06722088  0.07653674 -0.02842792 ... -0.04342957  0.05100529
  -0.01950579]]
layers.1.weight: [[-0.01801451 -0.02546145  0.00877263 ...  0.00453822  0.01099272
  -0.02364286]
 [ 0.03237275  0.04581587 -0.01581876 ... -0.00821179 -0.01982236
   0.0425356 ]
 [-0.01644378 -0.02324007  0.00802157 ...  0.00415656  0.01004534
  -0.02158737]
 ...
 [-0.01036425 -0.01464778  0.00506611 ...  0.0026298   0.00634115
  -0.01360522]
 [ 0.02056734  0.02917352 -0.01031719 ... -0.00543988 -0.0128789
   0.02707006]
 [-0.02107079 -0.02977051  0.0102255  ...  0.00528628  0.01281894
  -0.02764412]]
layers.2.weight: [[-0.8562888   1.5310098  -0.78243315 -1.436123    0.09122567 -0.93675786
  -0.25948495 -0.87282515 -0.14649321  1.2012573  -0.23827334  0.6772184
   1.1903759   0.9419558  -1.2287825   0.30948168  0.43050814  1.0130403
   1.3357306  -2.3669364   0.2053845  -0.34056216  0.28702015  0.80570567
  -1.0387948  -0.3167441  -1.102061    0.8252552   1.4924546   2.083594
   0.20161365 -1.3907628  -0.19963008  0.01675022  0.96702003  0.9504748
  -0.3336125  -1.0219122  -1.0496172  -1.331547   -0.05094641  1.5173144
   0.1075579   0.95403993 -0.9992829  -0.6910245  -0.39590448  0.17560308
   0.0776736   1.9382408  -0.8908557   1.0539939  -0.08894972  1.2956158
   0.7591119  -0.28802198  1.9708365  -0.66472286  0.19874477  0.58780503
   1.4703686  -0.60186183 -2.2644267  -0.05071157 -1.2317686   1.3915868
   0.03333192  0.57511336  0.81508964  0.30526108  0.92971224 -0.3808859
   0.20806108 -1.3889182  -1.5680258  -0.55231    -0.12799713 -0.49887565
  -1.5148922   0.8328971   1.2437055  -0.98177433 -0.09220724  0.67270786
   0.06356855  0.7768158   0.779924    0.61137825 -0.03257823  0.59364355
   1.5436497  -0.369825   -0.03979648  0.24612412  1.0575726   1.1773417
  -1.2117366   1.4271209  -2.0371852   1.047904    0.60092384 -0.67530143
  -1.7433112  -0.1868182   0.18085785 -1.7518363  -0.6683871   1.3678281
  -0.7246099  -0.33070207  1.337484    1.0460932   1.7381876  -0.59452724
  -0.74608237  0.32075003 -1.1821096  -1.3720988  -0.5347312   0.21765395
  -1.6679317   0.64676464 -1.2251766   1.1266735   0.8341841  -0.49470815
   0.98131156 -0.99961984]]

Final Loss: 3.2674
Distance Metric: 38.8530
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 977

================================================================================

nonoverlappingCNN_relu -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[-0.00345749  0.00380071 -0.00116312 ... -0.00037188  0.0004152
   0.00021569]
 [ 0.          0.         -0.00010521 ... -0.00047117  0.00058652
  -0.00020294]
 [-0.0003352   0.00082492 -0.00031105 ...  0.02418431 -0.02630647
   0.00769685]
 ...
 [-0.00106291  0.00126087  0.00051761 ... -0.0106369   0.01357763
  -0.00345963]
 [ 0.00074627  0.          0.00064584 ...  0.0113716  -0.01302067
   0.00376103]
 [-0.00803633  0.01011205 -0.00263593 ...  0.         -0.00092521
   0.00042799]]
layers.1.weight: [[ 0.04147085  0.00071317  0.0008051  ...  0.00255245 -0.00039179
   0.10274544]
 [ 0.00034451  0.          0.         ...  0.          0.
   0.00084333]
 [ 0.          0.00044633 -0.00034543 ...  0.0015922  -0.00013257
  -0.00017412]
 ...
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.0394857   0.00067781  0.00078399 ...  0.00242908 -0.00037464
   0.09782763]
 [ 0.01838804  0.0003165   0.00038187 ...  0.00114396 -0.00016863
   0.04553299]]
layers.2.weight: [[ 4.52845991e-01  4.19410691e-03 -1.65719483e-02  0.00000000e+00
   2.38530943e-03  9.21595935e-03  1.68122813e-01 -2.55554100e-03
   3.32790986e-03 -2.71965936e-02  1.99786603e-01  1.67181849e-01
   2.28719622e-01 -5.14417179e-02  1.60477847e-01  2.11416423e-01
   1.52545326e-04  0.00000000e+00  0.00000000e+00  6.61261022e-01
   5.29984534e-01 -1.67396497e-02 -1.38997389e-02 -3.17095757e-01
   1.41100027e-04 -1.14120841e-01  3.85349570e-03 -1.04000065e-02
   9.08917689e-04  3.37759346e-01 -7.30970860e-01  6.42716527e-01
  -2.08038911e-01  0.00000000e+00 -2.35925820e-02  1.64135825e-02
   1.93248093e-02  1.35890354e-04 -1.39256477e-01  1.28632054e-01
  -1.46758684e-03  0.00000000e+00 -6.26562117e-03  1.04580605e-02
   1.67757366e-02 -1.58701926e-01  4.69952583e-01 -1.26297725e-02
  -1.56424850e-01  6.56775683e-02  8.14838428e-03 -3.11306924e-01
   9.24141482e-02  9.69735347e-03 -1.90595514e-03 -1.00577176e-02
  -4.03635018e-02  3.07909817e-01 -2.23935507e-02  6.93944457e-04
   1.62960023e-01  1.56399444e-01 -7.14665174e-01 -4.93761972e-02
   1.23231024e-01 -7.38061452e-03 -6.92865610e-01  8.01138341e-01
   5.02184391e-01 -1.03225721e-04 -1.26395309e+00  1.80286039e-02
  -1.41500726e-01  8.44425932e-02  3.25413197e-01 -3.60830314e-03
   1.11867186e-04  0.00000000e+00 -8.66687763e-03 -1.50657352e-02
   3.16100985e-01  1.26838665e-02  2.56179124e-01  9.14080441e-03
  -5.30235190e-03  3.13200444e-01 -2.95724243e-01  1.06147694e-04
   1.05173931e-01  1.64494649e-01 -5.45290206e-03  1.42923087e-01
  -2.68049687e-02 -1.66522991e-02  5.08168563e-02 -2.05739010e-02
  -6.76518753e-02 -5.69270924e-02  9.56605375e-03  1.16817184e-01
   2.47124806e-02 -3.25983041e-03  1.18247373e-02  0.00000000e+00
   1.60548724e-02 -3.23152199e-04  5.73496222e-01  1.28138531e-02
  -5.49454056e-03 -1.02982864e-01 -3.03045125e-03 -1.70198306e-01
   2.91270502e-02  0.00000000e+00  3.20294142e-01 -1.36269751e-04
   0.00000000e+00  0.00000000e+00  7.80181140e-02 -4.04855609e-01
   1.96777028e-03 -6.21375680e-01 -2.32393178e-03  1.52672585e-02
   1.65227987e-02  0.00000000e+00  4.31341290e-01  2.00784698e-01]]

Final Loss: 0.0003
Distance Metric: 12.1393
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 828

================================================================================

nonoverlappingCNN_relu -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[-0.03972909  0.04986069 -0.01872009 ... -0.05088656  0.05783478
  -0.00342283]
 [ 0.07518128 -0.0936605   0.03633159 ...  0.04941874 -0.04630097
   0.01369364]
 [ 0.23761065 -0.3752275   0.07070297 ...  0.40634388 -0.4710747
   0.12784101]
 ...
 [ 0.11128521 -0.15704662  0.03115806 ...  0.18150885 -0.03001227
   0.07971355]
 [-0.04625976  0.09214412 -0.04740866 ... -0.2982896   0.485646
  -0.19541954]
 [-0.06605504  0.07574598 -0.03001759 ... -0.02211117  0.01294191
  -0.0062521 ]]
layers.1.weight: [[-0.0021581   0.00153846  0.01538523 ...  0.01257056 -0.01066241
   0.00107477]
 [ 0.00174539 -0.00143696 -0.01328933 ... -0.01075997  0.00916999
  -0.001059  ]
 [-0.00198084  0.00157292  0.01411288 ...  0.01143126 -0.00981004
   0.00088108]
 ...
 [-0.00030301  0.          0.00154641 ...  0.00113883 -0.00106759
   0.0001743 ]
 [ 0.00142732 -0.00114232 -0.01120637 ... -0.00898953  0.00779073
  -0.00069673]
 [-0.00076417  0.00043865  0.00479036 ...  0.00375729 -0.00328966
   0.0002189 ]]
layers.2.weight: [[ 1.72625557e-02 -1.47877214e-02  1.57698784e-02  1.32876188e-02
   7.76384678e-03 -8.40816298e-04 -3.06500262e-03 -2.94491369e-03
   1.40435649e-02 -1.70229245e-02 -1.29423458e-02 -2.10075319e-04
   4.25019907e-03  1.57568827e-02  1.78016233e-03  1.35381472e+00
   9.60132480e-03  1.16483979e-02  3.70990252e-03  1.93220302e-02
   2.47116084e-03  1.71658415e-02 -1.23139434e-02 -6.11872226e-03
  -1.17681921e-02 -2.76293606e-04 -4.39683301e-03 -9.03456938e-03
   6.47170888e-03 -1.38327936e-02 -3.31691980e-01 -1.55023206e-02
  -1.11272866e-02 -9.33256745e-03  5.67902625e-03 -4.63605206e-03
   6.58858614e-03  1.70396734e-02  1.22857830e-02 -7.21009029e-03
  -6.49304152e-01 -1.42150996e-02 -4.31197230e-03 -1.86770745e-02
  -4.93379263e-03  1.14726266e-02  1.36466306e-02  1.33067146e-02
  -1.24686677e-02 -1.74301006e-02 -1.23880161e-02 -1.62710324e-02
  -1.65262930e-02 -1.37251373e-02 -1.92404613e-02  1.48657355e-02
   1.33046657e-02  1.85584184e-02 -4.60800808e-03  1.10063925e-02
   1.01634832e-02 -1.65161062e-02 -1.01300282e-02 -2.21064477e-03
  -2.01347508e-02 -5.53977722e-03  1.36985315e-03  2.85632000e-03
   1.26537737e-02 -2.00940780e-02 -5.00220992e-03 -6.31846488e-01
   6.09573960e-01  1.97193343e-02 -2.12205630e-02 -2.52784416e-03
  -1.81875937e-02  8.30361433e-03  1.46219740e-04 -8.22035887e-04
   1.71533786e-02 -2.09344290e-02 -5.33929467e-03  1.81916319e-02
  -1.09037245e-02 -1.39059927e-02 -1.76024400e-02  1.13473842e-02
   3.92217771e-04  1.82509441e-02  1.75049584e-02 -7.15358416e-03
   1.05839260e-02  1.77485328e-02 -7.80195417e-03  2.04164814e-02
   4.35903668e-03 -1.82480793e-02  2.10022386e-02  6.98048202e-03
  -2.16085315e-02 -1.77194308e-02  1.06697930e-02 -1.95961967e-02
  -1.28683066e-02 -6.56256638e-03  4.54899877e-01  5.23477420e-03
  -6.72082929e-03 -5.13734281e-01  1.20815579e-02  8.15100409e-03
  -9.46352165e-03 -1.25399278e-02  1.50444759e-02  1.38876995e-03
  -1.60765499e-02  1.37896168e+00 -5.54046966e-03  1.72780994e-02
   8.99870938e-04 -6.32491056e-03 -5.65102184e-03  7.30318204e-03
   5.29991870e-04  1.66819547e-03 -1.24130556e-02  5.21755870e-03]]

Final Loss: 3.5035
Distance Metric: 22.9601
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 618

================================================================================

nonoverlappingCNN_tanh -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00508593 -0.00407584  0.00186555 ...  0.01742383 -0.02056982
   0.00694965]
 [-0.13766608  0.1684236  -0.04697281 ... -0.18857196  0.20485261
  -0.05674854]
 [ 0.00468448 -0.00372318  0.00165922 ...  0.01771804 -0.02093454
   0.00712238]
 ...
 [ 0.00522268 -0.00383736  0.00209316 ...  0.01554216 -0.01844343
   0.00617341]
 [ 0.00607508 -0.0024669   0.00346525 ...  0.00453046 -0.00598905
   0.00161734]
 [ 0.00493802 -0.0034654   0.00200003 ...  0.0151483  -0.01802447
   0.0060527 ]]
layers.1.weight: [[ 0.00042254 -0.01234058  0.0004066  ...  0.0003854   0.00016979
   0.00035968]
 [ 0.00037812 -0.01181682  0.00036292 ...  0.00034266  0.00013687
   0.00031814]
 [ 0.00041372 -0.0122358   0.00039794 ...  0.00037691  0.00016326
   0.00035143]
 ...
 [ 0.00039436 -0.01200773  0.00037889 ...  0.00035828  0.00014891
   0.00033333]
 [ 0.0006224  -0.01473055  0.00060311 ...  0.00057757  0.00031719
   0.00054645]
 [ 0.00035308 -0.01152183  0.00033829 ...  0.00031857  0.00011831
   0.00029471]]
layers.2.weight: [[ 0.05030101  0.04867083  0.0499778   0.0602929   0.0511541   0.05965335
   0.04556173  0.04965597  0.05614503  0.04827805  0.04814955  0.05698354
   0.04686635  0.0543716   0.05932754  0.057201    0.05632507  0.06229044
  -7.5367236   0.05965412  0.06002118 -2.0360963   0.05576384  0.04764746
   0.06474739  0.05839267  0.05382793  0.01163674  0.04870667  0.0557343
   0.04856553  0.05457857  0.04816247  0.04856917  0.06113042  0.04882546
   0.06385545  0.05800374  0.04792857 -3.9792235  -0.06431252  0.05044843
   0.06129971  0.05765559  0.04901892  0.05468919  0.05002369  0.04875498
   0.05128301  0.05170589  0.06122139  0.04865101  0.05790958  0.05189666
   0.05963121  0.05911008  0.04853832  0.04866022  0.05468108  0.05988184
   0.05486888  0.04850917  0.04861961  0.04867277  0.03556697  0.04782004
   0.05427018  0.06214631  0.03906904  0.05051683  0.05678335  0.05552353
   0.04896126  0.047973    0.0487394   0.05431919  0.04456729  0.05143297
   0.05562267  0.05005141  0.04752846  0.04933596  0.04849749  0.04991319
   0.05830552  0.04802443 -4.314863    0.0340264   0.04908246  0.05767079
   0.0487083   0.05921422  0.0577926   0.04750548  0.04814983  0.05787361
   0.04826597  0.05269775  0.04808951  0.05412387  0.06117738  0.06276351
   0.05905005  0.05008399  0.06207009  0.0605848   0.05510259  0.0485862
   0.05725783  0.05314706  0.04523267  0.05860692  0.04821285  0.04988176
   0.05512875  0.04801062  0.0475605   0.06109601  0.05989542  0.05605277
   0.04847613  0.05913111  0.04945888  0.05179786  0.0553073   0.04926638
   0.0577307   0.04775354]]

Final Loss: 0.1892
Distance Metric: 33.8161
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 1713

================================================================================

nonoverlappingCNN_tanh -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[-0.00255059  0.03561823  0.06091908 ... -0.12143829  0.01403668
   0.29337338]
 [-0.04038331 -0.0335677   0.0676581  ...  0.29115134 -0.2635566
   0.15677439]
 [ 0.04884097  0.03013242  0.03077982 ...  0.11401623  0.07690319
   0.14954159]
 ...
 [ 0.21959098 -0.17094181  0.02565703 ...  0.15645687 -0.03997143
  -0.06021192]
 [ 0.11072977 -0.0310754   0.03244991 ...  0.06011878  0.16098809
  -0.04987665]
 [ 0.09672423 -0.1615178  -0.04991527 ...  0.07572068 -0.18684958
  -0.14884835]]
layers.1.weight: [[ 0.01202689 -0.00232168  0.03781249 ...  0.03616554  0.05679943
   0.08346599]
 [ 0.04079362  0.0212706   0.13288139 ...  0.02865631  0.00218516
   0.0080178 ]
 [ 0.09070665 -0.10549711 -0.02202159 ...  0.05302327 -0.07037099
  -0.04561393]
 ...
 [ 0.10454977 -0.12620512 -0.09435376 ... -0.08370541  0.10268874
  -0.04832844]
 [ 0.0165172   0.00465355 -0.0517732  ...  0.00698688 -0.0152822
  -0.01147505]
 [-0.03147418 -0.02629465  0.02921246 ...  0.05838117  0.04933048
  -0.05639886]]
layers.2.weight: [[-5.02241552e-01 -3.71875703e-01  2.46602416e-01 -1.02054775e+00
  -3.05789948e-01 -4.41236585e-01  2.09442019e-01 -2.98367739e-01
   3.11025649e-01  2.78099567e-01  2.51359880e-01 -4.17230934e-01
  -4.68725145e-01  2.76552320e-01 -5.59229374e-01 -1.28180698e-01
  -2.80811310e-01  3.33962232e-01 -2.86742985e-01  3.61716241e-01
  -4.30933207e-01  5.07788956e-01  2.53610164e-01 -2.09387422e-01
   4.49330313e-03  2.59153813e-01  2.29852527e-01 -4.27862972e-01
   1.79815277e-01  1.10057876e-01 -6.00954115e-01  1.43263683e-01
  -2.13167533e-01  3.71572644e-01 -3.10346872e-01  3.00737053e-01
   4.09429241e-03 -2.44202957e-01  1.78029418e-01 -3.38075310e-01
  -3.42651993e-01 -4.62975860e-01 -4.29902881e-01  4.06358808e-01
   1.50495976e-01 -4.19763774e-01 -4.97217178e-01  2.00936556e-01
   9.77152362e-02  1.25394076e-01 -2.31609553e-01  3.10818374e-01
  -3.98974597e-01  8.84302035e-02  2.58568287e-01 -4.56940502e-01
  -3.32029074e-01  1.29592836e-01  1.80928916e-01 -3.78596485e-01
   1.57916531e-01  6.34564692e-03  1.44984692e-01 -4.43683058e-01
   5.84540129e-01 -5.77417195e-01 -4.00222123e-01  1.04626298e-01
  -2.79723376e-01  4.95258212e-01  1.15488872e-01  5.65769093e-04
   2.08855197e-01 -3.01945299e-01 -3.79389286e-01 -4.76568878e-01
   3.84962946e-01 -3.23261261e-01  5.16632460e-02  1.78558767e-01
   2.28237823e-01  2.76233912e-01 -3.31999511e-01  1.14260823e-01
   4.32713002e-01  3.16555560e-01  4.38995361e-01  3.50282043e-01
  -4.39473510e-01  3.67368191e-01  3.87384266e-01  1.17669962e-01
  -2.40046635e-01 -3.78256798e-01  3.22607011e-01  1.03472687e-01
   5.31794786e-01  2.80775577e-01 -1.98074728e-01  1.36208728e-01
   3.29580486e-01 -5.57541609e-01 -7.50504673e-01  2.35382140e-01
   4.66701835e-01  1.09340854e-01 -2.69647926e-01 -4.22010869e-01
  -2.72757769e-01  3.62666726e-01 -5.03605843e-01  3.13821912e-01
   4.00125653e-01  2.91944534e-01 -3.16905856e-01  7.10408837e-02
   1.04341260e-03 -4.61396009e-01  2.99810350e-01  2.10614040e-01
   3.95305604e-01 -2.16337949e-01  2.81111687e-01 -4.22523469e-01
   4.02011499e-02  3.64832371e-01 -4.13486838e-01 -2.94774771e-01]]

Final Loss: 0.1919
Distance Metric: 20.6190
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 2352

================================================================================

nonoverlappingCNN_tanh -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 0.00370123 -0.00367627  0.00083131 ...  0.0171999  -0.01827266
   0.00608583]
 [-0.00279988  0.00302683 -0.00084872 ... -0.00599018  0.00640663
  -0.00216616]
 [-0.01255427  0.01322287 -0.00408168 ...  0.00392208 -0.00418671
   0.00272596]
 ...
 [-0.01061852  0.01193198 -0.00352687 ... -0.00017957  0.00023599
  -0.00084217]
 [ 0.00227397 -0.00223855  0.00082636 ... -0.0019527   0.00214163
  -0.00153865]
 [-0.00014461 -0.00020615  0.         ... -0.00101591  0.00138175
   0.0003984 ]]
layers.1.weight: [[-0.00086341  0.00058303  0.00064551 ...  0.00038676 -0.00023615
   0.        ]
 [ 0.00012639  0.          0.         ... -0.00029361  0.00011511
  -0.00017743]
 [ 0.00012751 -0.00027902 -0.00025268 ...  0.          0.
   0.        ]
 ...
 [ 0.00020657  0.         -0.00012673 ...  0.          0.00021375
   0.        ]
 [ 0.00025103 -0.00011575  0.         ...  0.          0.
  -0.00020265]
 [-0.00023388  0.00020725  0.         ...  0.         -0.00019853
   0.        ]]
layers.2.weight: [[ 4.3136407e-02 -9.9852327e-03 -1.1592347e-02  8.1791379e-02
   3.7482616e-02 -4.6937648e-02 -6.4649791e-02  7.8412905e-02
  -5.6997869e-02  3.4883324e-02 -2.0509211e-02 -7.7222519e-02
   2.6716938e-04  2.9096551e-02 -1.2446038e-02 -4.3680698e-02
  -1.7481137e-02  4.5507032e-02  2.5314748e-02 -2.1692186e-03
  -4.7465861e-03 -3.2925535e-02 -8.4359320e-03 -1.0686114e-02
  -3.9343953e-02 -1.5978033e-02  1.1572890e-02  4.8591238e-02
  -1.4631434e-02  1.1808717e-01  5.4252569e-02 -3.3980981e-02
  -3.0186910e-03  5.0308976e-02 -1.2598072e-02  3.3226021e-02
  -1.1660048e-02 -3.3985060e-02 -5.6900275e-03  2.1896470e-02
  -4.3939874e-02 -2.3376439e-02 -1.1709079e-04 -2.6136905e-02
   2.9845051e-03  4.4238348e-02  4.7247399e-02  2.9513559e-03
   9.9799365e-01  3.0409986e-02  3.8402299e-03  1.8660655e-02
  -9.6487645e-03  6.6064157e-02 -3.3912450e-02  6.1220592e-03
   4.7316095e-03 -1.1275500e-03  1.0668773e-02  7.9827113e-03
  -2.2962742e-02 -1.6950253e-02  4.8883040e-02  4.4227116e-02
  -2.0897167e-02 -2.0059694e-03 -2.1563364e-02 -2.1146873e-02
  -4.4504497e-02 -3.3902485e-02  7.6023266e-02  6.7106816e-03
  -5.7592325e-02  2.9553864e-03 -2.2248749e-02  1.4160589e-02
   6.1580036e-02  5.8346868e-02  2.5005464e-02 -4.8463006e-02
   4.2147603e-02 -7.7096778e-03 -1.9732647e-02  1.0162799e+00
   7.9992242e-02  3.6432687e-03 -2.7548641e-03  4.6327412e-02
  -1.8752765e-02  2.3180577e-03 -8.2156979e-02 -3.7471391e-02
  -3.1773515e-02 -1.7382208e-03  2.0765755e-02  4.1812174e-02
   1.9734832e-02 -3.5977268e-03  1.0884016e-02 -4.6083350e-02
  -2.6961140e-02  6.0028542e-02  4.3730970e-02 -1.9390188e-02
  -4.3392261e-03  4.3100622e-02  9.1773933e-03  6.6078648e-02
   1.9614834e-03 -3.1691723e-02  1.1397635e-02 -5.1304502e-03
   2.2597071e-02 -1.7375004e-02  6.5584928e-02 -1.7431172e-02
  -2.3942370e-02 -2.0645551e-02  7.8854095e-03 -1.3684783e-02
  -1.6438851e-02  3.8569791e-03 -2.6217243e-02  2.7891526e-02
   9.8989904e-03 -1.6828449e-02 -1.2886574e-02  4.8697521e-03]]

Final Loss: 0.0007
Distance Metric: 8.9099
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 2
stopped after epoch: 594

================================================================================

