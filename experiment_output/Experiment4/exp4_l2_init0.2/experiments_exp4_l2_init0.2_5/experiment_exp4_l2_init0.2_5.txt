Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.01110718 -0.01246677  0.0037862  ...  0.01037514 -0.01129412
   0.00355122]
 [-0.00497896  0.00558777 -0.00169673 ... -0.00465061  0.00506287
  -0.00159232]
 [-0.0036216   0.00406417 -0.00123401 ... -0.00338265  0.00368244
  -0.00115821]
 ...
 [ 0.00926824 -0.01040292  0.00315929 ...  0.00865788 -0.00942454
   0.00296342]
 [ 0.00770452 -0.00864745  0.00262623 ...  0.00719707 -0.00783432
   0.00246344]
 [ 0.01321232 -0.01482976  0.00450372 ...  0.01234122 -0.01343437
   0.00422384]]
layers.1.weight: [[ 0.00011071 -0.00016237 -0.00013933 ...  0.          0.
   0.00014655]
 [ 0.00168823 -0.00084414 -0.0006305  ...  0.00139827  0.00115175
   0.00202064]
 [ 0.00084548 -0.0004777  -0.00036608 ...  0.00069394  0.00056513
   0.00101917]
 ...
 [-0.00515691  0.00236972  0.0017347  ... -0.00429496 -0.00356237
  -0.00614497]
 [ 0.00277791 -0.00133558 -0.00098857 ...  0.00230676  0.00190643
   0.00331784]
 [-0.0004541   0.          0.         ... -0.00039264 -0.0003404
  -0.00052454]]
layers.2.weight: [[ 0.01102756  0.10218115  0.05337474 -0.08893549  0.17698021 -0.11901046
  -0.23419839  0.05898328  0.19999507 -0.1260597   0.10519477  0.04725723
   0.15312238  0.24963307  0.09325327  0.15413044 -0.06137443 -0.02655694
  -0.15459378  0.15862104  0.15177481 -0.26138726 -0.02591123  0.09029083
  -0.13309637 -0.08243553 -0.11298684 -0.01090355  0.27216756 -0.2603476
  -0.00219812 -0.3316337  -0.37304556 -0.1792993  -0.39429358 -0.05420278
   0.04271926  0.14525302  0.22716224 -0.13303296  0.36395153 -0.1857809
   0.16715896  0.03056017 -0.2829236  -0.1524231   0.24569046 -0.02087808
   0.18751906  0.01254466  0.19829574  0.16788134 -0.03586528  0.12627324
   0.04128488 -0.06007478  0.1416136  -0.20168117 -0.01940181 -0.06519648
   0.18370672  0.07819369  0.16709068 -0.02025905  0.18802461 -0.1535552
   0.2664484   0.08538488  0.0302792   0.05142857 -0.24098842 -0.02436863
  -0.01532808 -0.06882061  0.165828   -0.09378608  0.04072805 -0.0252366
  -0.11944767  0.03920924 -0.00774325 -0.00391659  0.13837615  0.44436628
  -0.18473226  0.06329654  0.12783355  0.04448064 -0.07018989 -0.0170502
   0.12331552  0.39884493  0.08515193 -0.05958392 -0.12791578  0.09029973
   0.1617627  -0.37261513 -0.14950022 -0.06103331 -0.00640765 -0.2465252
  -0.11408012 -0.00063859 -0.24484745 -0.15224753  0.13745812 -0.14882448
   0.2807289   0.10118075 -0.19494298  0.20889151 -0.16199724 -0.09052401
  -0.21152434 -0.12510498 -0.17447576  0.19492161 -0.11894362 -0.01647966
  -0.21631981  0.21138577  0.42298964 -0.05127163 -0.28168923 -0.30505416
   0.16612834 -0.02162083]]

Final Loss: 0.0003
Distance Metric: 10.7471
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 2489

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[ 0.02755408  0.02802164  0.01921853 ...  0.05340977 -0.03211256
  -0.0092467 ]
 [ 0.1307577  -0.13018455  0.16766448 ...  0.04873285  0.06804505
   0.02267589]
 [-0.0026721   0.09105002  0.00365945 ...  0.00176342  0.11189653
   0.06495568]
 ...
 [-0.02896624 -0.04012559 -0.14899716 ... -0.00341494 -0.20104894
   0.0550964 ]
 [-0.05181824  0.16229461  0.00441169 ...  0.10242488 -0.0003373
   0.1982998 ]
 [-0.08179304  0.07678249 -0.08589444 ... -0.03993792 -0.07012458
  -0.0768856 ]]
layers.1.weight: [[-0.04158082  0.02998566  0.00272595 ... -0.03065998 -0.00782012
   0.04245978]
 [-0.06677678 -0.0211835   0.01383471 ... -0.08019315  0.03272602
   0.02954807]
 [ 0.02518193  0.08039328  0.03612155 ... -0.01226453  0.03442588
   0.00688842]
 ...
 [-0.0328101   0.01252095  0.04996267 ... -0.01920373  0.06635943
   0.02284581]
 [ 0.01493459 -0.07391436 -0.11249861 ...  0.02908566  0.05656828
  -0.00617848]
 [-0.00218915  0.00300281  0.00264153 ...  0.01021377  0.00951131
   0.00730398]]
layers.2.weight: [[-0.22232091 -0.4250848   0.23844983 -0.30984804  0.17460673  0.2290005
  -0.40524712  0.17234674  0.28014717 -0.32394198 -0.27021784  0.20084934
  -0.21768308 -0.35673     0.25544953  0.05050403 -0.39880836 -0.3401898
  -0.376278    0.00530456  0.05395375 -0.37910843  0.20450887  0.2025056
   0.18598016 -0.2930194   0.1887397   0.36517856 -0.291995    0.24910483
   0.1892661  -0.29101738 -0.40649107  0.28820094 -0.34671062  0.18641849
  -0.39616543  0.34457892 -0.20905529  0.1918035  -0.3827875  -0.24660307
  -0.3271215  -0.25037926  0.04449108 -0.3341302   0.12766367  0.11464878
   0.2530872  -0.25697842  0.02939769  0.02469408 -0.2968379  -0.2701583
   0.19837143 -0.3608584  -0.2811256  -0.36109188  0.1450303   0.02806965
   0.23585498  0.01953275 -0.3739672  -0.3116357  -0.23770283  0.26995215
   0.21990591 -0.40751186  0.21468268  0.25140327  0.19461058  0.18422522
   0.06929581  0.3493737  -0.35676825  0.03414724  0.17957726  0.20101407
   0.16576046  0.09178009 -0.20327738  0.01835491 -0.37655097  0.146136
  -0.3766709  -0.1911493   0.02179296  0.11696582  0.06924815  0.18945438
  -0.25955647 -0.3452239  -0.23267592 -0.35762826  0.16989529  0.02246204
  -0.30972868 -0.33014765 -0.36169645  0.11631637 -0.35146472 -0.35354853
   0.27050048  0.21594742 -0.25113413  0.15430883 -0.36103734 -0.34808028
   0.20153415 -0.23444    -0.30504486  0.3034959   0.23190722 -0.342519
  -0.31320956  0.24389818  0.1675495   0.19036742 -0.3817107  -0.29356873
  -0.22912386 -0.19877139  0.16481058 -0.39327902 -0.26915556  0.15885912
   0.35566047  0.03456878]]

Final Loss: 0.0096
Distance Metric: 17.7199
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1105

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 0.0021159  -0.00085323  0.00025368 ... -0.00011719  0.00063201
   0.0009952 ]
 [-0.03750409  0.01638706 -0.00486203 ...  0.00082726 -0.01193611
  -0.02024606]
 [-0.02741277  0.0115142  -0.00339602 ...  0.00106974 -0.00848479
  -0.01384105]
 ...
 [ 0.03634658 -0.01580145  0.00467627 ... -0.00088813  0.01152138
   0.01945527]
 [ 0.01283802 -0.00521991  0.00154351 ... -0.00066952  0.00386003
   0.00613661]
 [-0.03551359  0.01538177 -0.00454648 ...  0.00092609 -0.01122872
  -0.01889508]]
layers.1.weight: [[ 0.          0.         -0.00038813 ...  0.00035645  0.00010446
   0.        ]
 [-0.00015788  0.         -0.00014636 ... -0.00021733  0.00011739
   0.        ]
 [-0.00030374  0.00036346 -0.00022165 ...  0.00040346 -0.00018857
   0.        ]
 ...
 [ 0.          0.          0.00024207 ...  0.          0.
  -0.00012706]
 [-0.00022444 -0.00041454 -0.00018356 ... -0.00024285  0.00030599
   0.        ]
 [-0.00027571  0.00032925 -0.00028774 ... -0.00014559 -0.00031707
   0.00012825]]
layers.2.weight: [[-3.82375729e-04  1.51681335e-04 -2.90527823e-04  2.60774686e-04
   0.00000000e+00 -1.90133185e-04  0.00000000e+00  1.50652486e-04
  -1.95642584e-04  0.00000000e+00 -1.64148296e-04  1.98630363e-01
   2.12599072e-04 -1.59478252e-04 -3.27168498e-04  0.00000000e+00
   2.83068570e-04 -2.21661248e-04 -3.81741906e-04 -1.48531326e-04
  -2.55456835e-04  0.00000000e+00  0.00000000e+00 -1.51366854e-04
   4.01614641e-04  0.00000000e+00 -1.03412312e-04  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.28868138e-04
   0.00000000e+00 -2.80271255e-04  2.39424233e-04  0.00000000e+00
   1.03153274e-04  0.00000000e+00  0.00000000e+00  2.13545893e-04
  -1.28028536e-04  1.97532499e-04  0.00000000e+00  2.02318144e-04
   0.00000000e+00  0.00000000e+00  0.00000000e+00 -3.23407497e-04
   2.33898332e-04  0.00000000e+00 -1.30550601e-04  0.00000000e+00
   2.24690826e-04  2.99252424e-04  3.07171285e-04  0.00000000e+00
  -1.13646543e-04  0.00000000e+00 -1.53890724e-04 -4.15463670e-04
   1.59460236e-04 -1.55549264e-04  0.00000000e+00  0.00000000e+00
  -3.25848232e-04  2.19765279e-04  3.28600552e-04  3.34088283e-04
  -2.49335746e-04  0.00000000e+00 -1.44090402e-04  0.00000000e+00
  -1.50683089e-04  1.59567135e-04  0.00000000e+00 -2.31144732e-04
   0.00000000e+00  4.11948393e-04  1.20526027e-04  0.00000000e+00
  -1.26678424e-04  1.47053608e-04  1.88965743e-04 -1.77070760e-04
   0.00000000e+00  1.25685809e-04 -2.07692792e-04 -1.41520693e-04
   1.17819240e-04  1.27358566e-04  3.32740339e-04 -1.79415656e-04
   0.00000000e+00  1.97996225e-04 -2.20025831e-04 -2.48311786e-04
   0.00000000e+00  0.00000000e+00  2.60746281e-04 -1.23916834e-04
  -2.93781777e-04 -4.57545451e-04 -1.02545790e-04  0.00000000e+00
  -1.88912265e-04 -3.52291827e-04  0.00000000e+00 -2.93354911e-04
   1.78761868e-04 -2.66884104e-04  2.23585230e-04  0.00000000e+00
  -2.58200278e-04  0.00000000e+00 -3.26118636e-04  0.00000000e+00
   0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.15322376e-04
   2.35934291e-04  0.00000000e+00  1.01556478e-04  0.00000000e+00
   0.00000000e+00  3.55021242e-04  0.00000000e+00  1.18753553e-04]]

Final Loss: 0.2524
Distance Metric: 7.3678
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 508

================================================================================

nonoverlappingCNN_relu -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.06290989  0.08133939 -0.02507131 ... -0.05371055  0.05537286
  -0.02497934]
 [ 0.06585647 -0.07841212  0.02449094 ...  0.03940408 -0.04107694
   0.01676393]
 [-0.03177965  0.03612623 -0.01027394 ... -0.01837302  0.02249324
  -0.00585909]
 ...
 [ 0.06410154 -0.07495561  0.02324691 ...  0.03525795 -0.03749602
   0.01416129]
 [-0.99255824  1.1431354  -0.3719323  ...  1.3424706  -1.4830611
   0.42598525]
 [-0.03560453  0.04025781 -0.01158277 ... -0.02052791  0.02472772
  -0.00673768]]
layers.1.weight: [[ 0.00514101 -0.00702915  0.00164241 ... -0.00660975 -0.06062506
   0.00184483]
 [ 0.04150955 -0.03538967  0.01946738 ... -0.03362047 -0.3476345
   0.02144038]
 [-0.02467454  0.02075999 -0.01178142 ...  0.01955347  0.21629159
  -0.0128797 ]
 ...
 [-0.02814518  0.02349783 -0.01336129 ...  0.02234697  0.24449573
  -0.01476647]
 [ 0.02816572 -0.02417373  0.01314873 ... -0.02289397 -0.24484001
   0.01456382]
 [-0.03879667  0.0327667  -0.01831131 ...  0.03103368  0.3292387
  -0.02010914]]
layers.2.weight: [[ 0.24330151  1.4632648  -0.9034265  -0.7418062  -0.20773661  1.0532656
   0.9462647  -0.4125714  -0.48136434 -0.92864     1.0851313   0.9637765
  -0.2275381  -1.225258   -1.954159   -0.15758047  1.273069   -0.04684179
   1.8581543   0.09395128  0.06378645 -0.44953048 -0.04496184 -1.4398896
   0.32635027  0.9280075  -0.25885633 -0.37383062  0.89231306  0.43704042
  -0.4882187   0.19335729  1.3563435   0.24122865 -0.22841823 -1.5867901
   0.4752193  -0.1952067   0.7734994  -1.4384357   1.4397852  -1.8008522
  -1.0842723  -0.90148675  1.6557603   0.63422036 -0.4594726   0.29841912
  -0.5177949  -0.81141263 -0.19637093  0.8665795   0.5826682   1.2047626
  -0.9676843  -0.5520857  -0.45746025  0.19632821  1.5924373  -0.9615321
  -1.2606047  -0.24493885 -0.05214082  1.350308    0.01041957  0.29454964
  -0.6528621   0.7091394  -0.01644602 -0.96092355 -0.9092805  -1.0959724
  -1.5442538  -1.8405095   0.67768973 -0.4385536   0.4376906  -0.47640726
  -0.44467628 -0.0284998  -0.5342076   1.6103928   1.4035714   0.7746362
   0.42229235  0.548959    0.83813983  0.1693104  -0.6763772   0.36147508
   1.0696516  -1.6310823   0.78316915  1.3451174   1.2116636   0.37346932
  -0.6761256  -1.320802   -1.5406985   0.9124164   0.52654237  0.0328378
  -0.01301968  1.3773575  -0.28861767  0.50772727  1.2455969  -0.367568
  -0.3519275   1.5741681  -0.78161126  0.36604708 -0.6797795   0.817808
   0.1476542   0.26040608 -0.36439627  0.76528114  0.7066413  -0.99039936
   0.62810904 -0.04781389 -0.16184436  1.360836    0.5804037  -1.023688
   1.0206797  -1.3889363 ]]

Final Loss: 3.3005
Distance Metric: 37.0608
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 595

================================================================================

nonoverlappingCNN_relu -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[ 0.0000000e+00 -3.3442193e-04  0.0000000e+00 ...  5.6571681e-02
  -6.1677493e-02  1.8107180e-02]
 [-1.0939770e-02  1.1875747e-02 -3.4338748e-03 ...  1.2204332e-03
  -1.2392438e-03  2.4742959e-04]
 [-7.2930119e-04  6.9390761e-04 -2.3040753e-03 ...  5.2734560e-01
  -5.7850933e-01  1.7854452e-01]
 ...
 [-3.0128970e-03  4.0441123e-03 -1.2104079e-03 ... -1.5925814e-03
   1.8702061e-03 -1.1242202e-03]
 [ 0.0000000e+00  0.0000000e+00 -1.1344924e-04 ... -3.8252029e-04
   3.2033501e-04  0.0000000e+00]
 [ 2.0261439e-03 -2.2133354e-03  6.9206796e-04 ...  0.0000000e+00
   0.0000000e+00  0.0000000e+00]]
layers.1.weight: [[ 0.          0.00576298 -0.00197491 ...  0.00174603  0.
  -0.00011568]
 [-0.00163201 -0.00024025 -0.01476655 ...  0.          0.
   0.        ]
 [-0.00019121  0.         -0.00206652 ...  0.          0.
   0.        ]
 ...
 [ 0.         -0.00010662  0.         ...  0.         -0.0001515
   0.        ]
 [-0.00018127 -0.0001849  -0.00221409 ...  0.          0.
   0.        ]
 [ 0.00846776  0.00027709  0.0786366  ... -0.00045177  0.
   0.        ]]
layers.2.weight: [[ 4.96530794e-02 -3.81902680e-02 -4.92241234e-03 -1.77466288e-01
   8.14646366e-04  1.94860157e-02  3.88758898e-01  4.52587716e-02
  -1.57713860e-01  3.40523832e-02  2.57633150e-01  1.49890676e-01
  -1.71108589e-01 -1.67212635e-02 -1.59152609e-03 -1.51346445e-01
  -1.21146083e-01 -1.07170525e-03 -1.22887146e+00 -2.95248926e-02
  -3.34859878e-01  4.05862927e-03 -3.91096662e-04  3.00364703e-01
   1.61104172e-03 -1.43426424e-03 -2.77308226e-02 -8.17692399e-01
  -5.78482449e-01  3.97713751e-01 -1.61113933e-01  2.37441793e-01
  -2.18416173e-02 -1.98814808e-03 -6.70824479e-03  2.53297910e-02
   1.78292170e-01 -3.04422677e-01  1.59808338e-01  3.09039563e-01
   1.97831281e-02 -3.78671288e-02 -2.42210701e-02  1.31989658e-01
   9.52662453e-02 -7.22449273e-02 -3.06391940e-02 -4.22856286e-02
   5.11542931e-02  4.83282775e-01 -7.35300705e-02 -6.26905868e-03
   1.20373571e-03  6.93697572e-01  3.16016138e-01  1.73853561e-01
  -2.93548226e-01 -1.24996516e-03 -1.28842844e-02  6.69142008e-01
   0.00000000e+00  5.50328255e-01 -1.52632982e-01  3.05466235e-01
  -5.13502657e-01 -2.69314554e-02 -1.74826905e-02  6.47445850e-04
  -1.70884212e-03 -1.63146276e-02  6.48067221e-02 -1.87454571e-03
   3.66457254e-01  1.58779963e-04 -8.25992972e-03  4.82008010e-02
   5.30951992e-02  6.20217063e-02 -3.63695621e-01  9.72451642e-02
   2.85758916e-02  4.08834405e-02  3.33310932e-01 -1.83654442e-01
  -2.05321293e-02  5.06171770e-02 -7.93766510e-03  4.59296741e-02
  -1.93961591e-01 -4.52783257e-01  1.03078075e-02 -1.10661604e-01
   4.05754370e-04 -1.29481906e-03 -6.15855027e-03  1.16423436e-03
  -2.68182635e-01 -9.98532698e-02  9.56360344e-03  1.99906640e-02
  -2.41964143e-02  2.48467028e-02 -5.67615271e-01  6.17866293e-02
   6.59791306e-02 -7.92265031e-03 -1.03161521e-01  3.10960829e-01
   7.26634026e-01  3.65748942e-01 -1.31334573e-01  3.87065828e-01
   0.00000000e+00  1.00433387e-01 -2.08742060e-02  1.37099311e-01
  -1.69264816e-03  1.14699587e-01 -6.34224117e-02  2.58451253e-01
   4.07307118e-01  5.02488017e-01  5.89798689e-02 -3.97253186e-02
   7.16410130e-02  5.93081291e-04 -4.89569129e-03 -2.05560178e-01]]

Final Loss: 0.0003
Distance Metric: 12.6820
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 623

================================================================================

nonoverlappingCNN_relu -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 0.06170231 -0.06887519  0.01939386 ...  0.05634339 -0.06887744
   0.01757843]
 [ 0.05285531 -0.0636785   0.0280669  ...  0.05023486 -0.07568639
   0.01657259]
 [ 0.09236785 -0.09013952  0.02423239 ...  0.05295157 -0.0575803
   0.01031653]
 ...
 [-0.11854725  0.11070611 -0.03178434 ... -0.04138376  0.04733868
  -0.00236924]
 [-0.06040546  0.07029279 -0.03049985 ... -0.05332165  0.07785244
  -0.01693501]
 [-0.05743691  0.06805059 -0.02992706 ... -0.05223561  0.0772626
  -0.01692212]]
layers.1.weight: [[ 0.00088215  0.00055526  0.00065478 ... -0.00107869 -0.00043685
  -0.00046029]
 [ 0.00093177  0.00058028  0.00073722 ... -0.00113137 -0.00049356
  -0.00051066]
 [ 0.00088003  0.00055382  0.00065222 ... -0.00107699 -0.00043435
  -0.00045891]
 ...
 [-0.03550822 -0.03216412 -0.04722163 ...  0.05065448  0.03684017
   0.03533776]
 [ 0.00090493  0.00056698  0.00069428 ... -0.00110358 -0.00046338
  -0.00048495]
 [-0.00087786 -0.00055268 -0.00064831 ...  0.00107438  0.00043182
   0.00045608]]
layers.2.weight: [[-0.03383633 -0.03326158 -0.03385654  0.0338048   0.03379321  0.03379889
   0.0338861  -0.03320453 -0.03383575  0.03347073 -0.03383277 -0.03362067
  -0.03383446  0.03387157 -0.03114852 -0.03350341  0.03375388  0.03377197
   0.03377386  0.03380875 -0.03389777  0.7948069   0.03389188  0.03386363
  -0.03386454 -0.0337752  -0.03380485 -0.03377014  0.03371818 -0.03387332
   0.03367032 -0.03382719 -0.03375896 -0.03384512 -0.03382101  0.03376936
   0.03377342 -0.03372169 -0.03377701 -0.0338599   0.03364619  0.03259117
  -0.03334963 -0.0337779   0.7027322   0.43595672  0.03382015 -0.03386386
  -0.03344012  0.03382025 -0.03383294  0.03386673 -0.03390264  0.03368336
  -0.03373246 -0.03366203  0.03354716  0.03379835 -0.03379507  0.03370174
   0.03391006 -0.03381642  0.03363844 -0.03356343  0.56762266 -0.03383427
   0.03371546 -0.03379732  0.03341625 -0.03376415 -0.03385672 -0.03346971
  -0.03366794 -0.17019755 -0.03203797 -0.77605796  0.03379692  0.03378011
  -0.03373878 -0.03390215 -0.03369862  0.03382374 -0.03359561 -0.03370198
  -0.03383112 -0.51095957  0.03360225 -0.03381537  0.03380167  0.03392136
   0.03379076  0.03379823 -0.03342215  0.0338234   0.03362349 -0.03231668
   0.03381365 -0.03384343  0.0335646   0.0337466  -0.03373538 -0.03378185
  -0.03383528  0.03373174 -0.03288077  0.03383116  0.03383969  0.03375387
   0.03362831  0.03385779  0.03380697 -0.03390268  0.03380624 -0.03357673
  -0.03373356 -0.03366008  0.03378308 -0.03376716 -0.03348679 -0.03387715
   0.03371945  0.03385613  0.03372462 -0.03376437 -0.03383055  0.82384527
  -0.03356879  0.03388132]]

Final Loss: 3.5317
Distance Metric: 23.0105
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1214

================================================================================

nonoverlappingCNN_tanh -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.          0.0003172   0.         ... -0.00318606  0.00334362
  -0.00053762]
 [ 0.00306757 -0.0031401   0.00196153 ... -0.02756008  0.02673337
  -0.00883384]
 [ 0.00140565 -0.00105427  0.00049002 ... -0.00596396  0.00632374
  -0.00141617]
 ...
 [ 0.00259534 -0.00252489  0.00143406 ... -0.0182001   0.0179147
  -0.00560764]
 [ 0.          0.00041671  0.         ... -0.0087264   0.00851516
  -0.00246127]
 [ 0.00185651 -0.00167408  0.00123527 ... -0.02196642  0.02129701
  -0.00695319]]
layers.1.weight: [[ 0.00017255  0.00012461  0.00011938 ...  0.00012088  0.00018093
   0.00014461]
 [-0.00017384 -0.00019574 -0.00020057 ... -0.00019895 -0.00016808
  -0.00018516]
 [-0.00010588 -0.00013283 -0.00013721 ... -0.00013603  0.
  -0.00012062]
 ...
 [-0.00013444 -0.00015937 -0.00016366 ... -0.00016271 -0.00012843
  -0.00014777]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [-0.00011638 -0.00014254 -0.00014709 ... -0.00014577 -0.00011045
  -0.00013069]]
layers.2.weight: [[ 0.07695124  0.04816493  0.05307086  0.04872115  0.07606673 -2.9523566
   0.04960302  0.06428452  0.05295234  0.05626868  0.05172596  0.07463952
   0.05117447  0.06592178  0.05269283  0.05039063 -7.4923124   0.05281634
   0.05207168  0.06553385  0.04535179  0.07257618  0.05667402  0.05330122
   0.07378044  0.06898268  0.06497549  0.0660883   0.06511272  0.05154667
   0.0395257  -3.3808289   0.07116206 -3.23734     0.05288213  0.05170906
   0.06035981  0.06905562  0.05486009  0.05826087  0.07179772  0.05226423
   0.05769547  0.05029493  0.07133079  0.05604273  0.05248121  0.0499184
   0.0490315   0.07414225  0.06076138  0.07146468  0.05144684  0.06072357
   0.05115381  0.0694336   0.07972624  0.05441877  0.05165657  0.05274256
   0.06557807  0.07309882  0.05709563  0.06933267  0.07386921  0.0680818
   0.05492026  0.07337046  0.05135467  0.06651562  0.05023978  0.06701286
   0.04032315  0.0498402   0.06900676  0.07004065  0.05251753  0.04823827
   0.05119592  0.05131442  0.07082722 -2.7466204   0.06899439  0.05055364
   0.05238199  0.05712542  0.05739556  0.0616839   0.05125007  0.03253664
   0.07167297  0.07418986  0.06476271  0.05293055  0.05202626  0.05762494
   0.05130329  0.05204763  0.07363097  0.06743314  0.050633    0.07170733
   0.06356131  0.04397806  0.05185807  0.05768681  0.07132255  0.05712101
   0.05696583  0.04877785  0.05215969  0.05974347 -0.02855113  0.0636627
   0.07131861  0.06591147  0.05292558  0.03596009  0.07431507  0.06136508
   0.07583448  0.04897882  0.07325131  0.06494609  0.05055198  0.05099522
   0.06601591  0.05230789]]

Final Loss: 0.1858
Distance Metric: 33.5207
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1299

================================================================================

nonoverlappingCNN_tanh -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[ 0.22816741  0.12878679 -0.02378103 ... -0.09343758  0.20809193
  -0.11266699]
 [-0.2789279   0.29175147 -0.09271479 ... -0.05255722  0.02944457
   0.03008699]
 [ 0.01917183  0.05127814  0.18777639 ...  0.18747951  0.08866782
  -0.06652115]
 ...
 [ 0.04072173 -0.01146022 -0.00624141 ...  0.6709191  -0.75817436
   0.23067924]
 [ 0.22484416  0.17465433 -0.03310404 ...  0.11163559 -0.19125213
  -0.08893952]
 [-0.15902723  0.25426456 -0.11153752 ...  0.214535   -0.18254827
   0.09213088]]
layers.1.weight: [[-0.03450939 -0.07927577 -0.00590288 ... -0.1053931  -0.04693757
   0.05003316]
 [-0.00466294  0.00331136 -0.00844656 ... -0.00648964  0.00978489
  -0.01000074]
 [ 0.02154908 -0.02169182  0.01270205 ... -0.02090012 -0.06718123
  -0.05644053]
 ...
 [ 0.03571422 -0.08269241 -0.00946794 ...  0.04884421  0.03522071
  -0.09280129]
 [-0.10354588 -0.00681957  0.06588849 ...  0.03645394  0.0466055
  -0.01626964]
 [ 0.02567937 -0.05193841 -0.0420158  ...  0.14894801 -0.05281072
   0.00649446]]
layers.2.weight: [[ 0.32301056  0.05200027 -0.2541114  -0.5360944  -0.4204327  -0.3870713
  -0.41735744 -0.39255422  0.04242491 -0.39281884  0.28201777 -0.23224041
  -0.22106512 -0.19357163 -0.35045844 -0.3086893  -0.54373693  0.24070421
  -0.27871123 -0.34411785 -0.39400262  0.24289733  0.19742933  0.1612085
  -0.15881182  0.2906696  -0.20149912  0.0223434   0.37782136  0.18329497
   0.18407601  0.19701241  0.09589784 -0.15401436  0.15769108 -0.20852928
   0.02349691  0.32900885 -0.18092708  0.0676781  -0.2618388   0.17802349
   0.0132394  -0.42269427 -0.28111535  0.04656034  0.3553583   0.34019417
  -0.48200646 -0.24833669 -0.9061275   0.29346588 -0.52654     0.02972572
   0.00699543 -0.29811895  0.31696263 -0.61162     0.37922755 -0.5650463
  -0.3473916  -0.8727935   0.03410171 -0.25525528  0.38231695 -0.34663823
  -0.21371706 -0.21731488  0.07584079 -0.42528284 -0.3061997  -0.40114748
  -0.39963713 -0.6104654  -0.27300236  0.20081715  0.4480803   0.28376046
  -0.17737712  0.07183953  0.41839555 -0.39838266  0.20796545 -0.34188902
  -0.18700773 -0.2766296   0.17886615 -0.20075974  0.39216906 -0.3252398
  -0.35095334  0.2555912  -0.31987023 -0.3147691  -0.37868667 -0.38372654
  -0.30209053 -0.3317264   0.1684015   0.31930885 -0.51604366  0.21723825
  -0.22705594 -0.6095146  -0.37968507 -0.18407466  0.00938509  0.34434482
   0.04559059 -0.3557464  -0.49342808  0.35664064 -0.37329268 -0.33426195
  -0.32406592 -0.28378978  0.36621118  0.3148452  -0.21207191  0.20114028
  -0.3306934   0.29891497  0.2349963  -0.22074383 -0.40661386 -0.35109016
   0.35696024  0.27083895]]

Final Loss: 0.1886
Distance Metric: 20.0246
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 1462

================================================================================

nonoverlappingCNN_tanh -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[-0.00961135  0.01026962 -0.00335801 ... -0.00337565  0.00251888
  -0.0016809 ]
 [ 0.00507213 -0.00581907  0.00174009 ...  0.          0.00054534
   0.        ]
 [-0.00552062  0.00663834 -0.00230584 ...  0.00299704 -0.00388139
   0.0009484 ]
 ...
 [ 0.00425715 -0.00825662  0.001481   ...  0.02127677 -0.02178809
   0.00611269]
 [ 0.00170904 -0.0008167   0.00073733 ...  0.01676904 -0.0185128
   0.00569196]
 [ 0.03918367 -0.04398177  0.01370918 ... -0.00260869  0.00405533
  -0.00022436]]
layers.1.weight: [[-0.00011522  0.0001117   0.00030588 ...  0.00745462  0.00050106
   0.00011332]
 [-0.00099559 -0.0001209   0.         ...  0.02528431  0.00093009
   0.00163829]
 [ 0.00050832  0.00044195  0.00010268 ...  0.00051277 -0.00048446
   0.0001618 ]
 ...
 [-0.00045618  0.00019482 -0.0002847  ... -0.00098137  0.
   0.        ]
 [ 0.00020559 -0.00066541 -0.00015439 ... -0.02891572 -0.0011438
  -0.00182611]
 [ 0.          0.00034254 -0.00022238 ...  0.00083671 -0.00017117
  -0.0003199 ]]
layers.2.weight: [[-1.40345236e-02 -4.58346643e-02 -1.03345432e-03 -3.32882963e-02
   6.02386519e-02 -4.10323329e-02  3.09751201e-02 -7.95936957e-02
  -1.71153769e-02 -8.32122751e-03 -3.08984555e-02 -6.78069366e-04
  -2.24603545e-02 -6.67549111e-03  4.12830524e-03 -4.10535000e-02
  -5.30258194e-02  0.00000000e+00  2.74626985e-02 -2.27736365e-02
  -1.55884717e-02  2.52523981e-02 -3.77253890e-02  9.96511698e-01
   5.25231473e-02 -2.85481531e-02 -2.35325843e-02  6.62364112e-03
  -5.01862839e-02 -2.22474691e-02 -3.56704667e-02  4.74364683e-02
  -2.01739538e-02  5.09201773e-02 -2.71557365e-02  1.29994787e-02
   4.73510027e-02  1.11310892e-02  3.37503441e-02  1.62248090e-02
   4.22628187e-02  6.24038987e-02 -5.69613371e-03  1.47988331e-02
   2.39491127e-02 -2.78785080e-02 -1.57672111e-02 -1.07998876e-02
   5.64546585e-02 -4.80743125e-02 -3.85656357e-02 -2.97179949e-02
   2.62666829e-02  8.39699153e-03  1.01602483e+00 -5.07395482e-03
   1.47216336e-03 -1.29176173e-02 -2.48478726e-02 -3.87797668e-03
   5.41866235e-02 -4.54919413e-02 -2.42477246e-02  4.07635346e-02
  -2.17038412e-02 -9.78005957e-03 -5.57684898e-02 -5.30478694e-02
  -4.16767364e-03 -5.65086603e-02  7.04679778e-03  1.53957149e-02
   2.57192533e-02  6.54362291e-02 -3.32884565e-02 -4.69878828e-03
  -1.48794744e-02  1.35000553e-02 -5.69030829e-02 -1.32102342e-02
   8.77086725e-03 -6.18800074e-02 -6.29696399e-02  1.35693895e-02
  -2.55385805e-02  2.84117144e-02 -7.15599768e-03 -1.23537090e-02
  -5.21293096e-02 -1.23611279e-02  3.23212780e-02  2.08606645e-02
  -4.48049754e-02  1.71292722e-02 -4.95072044e-02 -1.48589397e-02
  -4.25780527e-02 -8.11739415e-02  3.10967863e-02 -1.46321068e-03
   3.05913761e-02  7.32931048e-02 -7.27062076e-02 -1.85765103e-02
   5.12119755e-03  3.84270586e-02 -6.70406818e-02  1.24479737e-02
   1.06272884e-02  4.82976474e-02 -1.90507453e-02 -3.75184156e-02
  -2.14103702e-02  5.25027364e-02  4.46606725e-02  9.85322148e-03
   3.97829898e-02 -2.33779401e-02 -2.01500338e-02  5.98605834e-02
  -1.97414006e-03 -1.43225370e-02  4.31216322e-02 -6.72905659e-03
   5.53756766e-02  2.55214446e-03  5.28725423e-02 -1.70837389e-03]]

Final Loss: 0.0007
Distance Metric: 8.8968
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 5
stopped after epoch: 491

================================================================================

