Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.02187155 -0.02342012  0.0064251  ...  0.02033528 -0.02223253
   0.00701518]
 [-0.00219341  0.00234967 -0.00064409 ... -0.00204173  0.00223005
  -0.00070392]
 [-0.00069747  0.00074736 -0.00020493 ... -0.00064936  0.00070891
  -0.00022407]
 ...
 [ 0.02611537 -0.02796008  0.00767463 ...  0.0242695  -0.02654163
   0.00837642]
 [-0.00737161  0.00789616 -0.00216433 ... -0.00686082  0.00749506
  -0.00236498]
 [ 0.00816316 -0.00874303  0.00239643 ...  0.00759683 -0.00830048
   0.00261817]]
layers.1.weight: [[-0.00384984  0.00031187  0.         ... -0.00459127  0.00120486
  -0.00147383]
 [ 0.00390457 -0.0004632  -0.00019259 ...  0.00468282 -0.0014004
   0.00141093]
 [-0.00265355  0.00018427  0.         ... -0.00315906  0.0007932
  -0.0010334 ]
 ...
 [-0.00241404  0.00015899  0.         ... -0.0028724   0.0007111
  -0.00094504]
 [-0.0065204   0.00060026  0.00015909 ... -0.0077888   0.00212817
  -0.00245513]
 [ 0.00158815 -0.00024347 -0.00012998 ...  0.00191447 -0.00063648
   0.00054244]]
layers.2.weight: [[-0.11283414  0.11837671 -0.07691871 -0.0153276  -0.1601597   0.08117177
   0.09438101  0.06529702  0.08133391  0.22193997 -0.14486241  0.03673654
  -0.1198083   0.09557986 -0.15193449 -0.17596515  0.1649944   0.07113995
   0.18355773  0.21144903 -0.01053958  0.16412021 -0.25885096 -0.135257
   0.24234366 -0.17469226 -0.12160162  0.07824127 -0.11340572 -0.42351037
  -0.0900493  -0.12727554 -0.22016002  0.21674061 -0.15050422 -0.20867267
  -0.15359554 -0.35812023  0.2037239  -0.13973889 -0.02357933  0.13094172
  -0.10109444 -0.19617274 -0.09805872 -0.22327484 -0.02985919 -0.03824632
   0.01243482  0.22497454  0.10666635  0.36009437 -0.10238031 -0.22688106
   0.0180201  -0.11667784  0.00802397  0.28792873  0.01072346 -0.33745468
   0.21356606  0.07211272  0.13483663  0.14446594  0.04552978  0.27729693
   0.09311926  0.07514732 -0.24651407  0.01947441  0.05614495 -0.11196346
   0.30981454 -0.1402061   0.09571636  0.19561996 -0.14405218  0.00256301
  -0.02736109 -0.30557594  0.28287145  0.02216876 -0.03879648  0.16887824
  -0.20009671 -0.21352704  0.334755   -0.03280931  0.19825137 -0.09935562
  -0.14305489  0.15521568 -0.17417155 -0.19890289  0.09361405  0.1486913
  -0.23233244 -0.00324796 -0.172114   -0.1319975  -0.02999334 -0.08100101
   0.04611097  0.12477075 -0.17616697 -0.01981931 -0.132932   -0.05077846
   0.2359863  -0.07532869  0.16834453  0.14246425  0.14345475 -0.06160138
  -0.01603659 -0.26561984  0.20725808 -0.13630633  0.21224816 -0.13421363
   0.1636934   0.07582716  0.2062951   0.35606113  0.45324457 -0.06974058
  -0.1933216   0.04959128]]

Final Loss: 0.0003
Distance Metric: 10.7039
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 2215

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[-0.03154439 -0.01521184  0.15888408 ... -0.00040855 -0.07618254
   0.06198466]
 [ 0.18681665  0.01331352 -0.10878313 ...  0.05983122  0.10146222
  -0.01982641]
 [-0.05026497  0.05008911 -0.17556556 ...  0.05061448  0.16953151
  -0.08357361]
 ...
 [-0.08336058  0.09815054 -0.03551692 ... -0.04729093  0.00047396
  -0.02724044]
 [ 0.01535419  0.06349858 -0.0090466  ...  0.05433743  0.06720446
   0.21162571]
 [-0.02909528 -0.09956095  0.10184006 ...  0.02311307 -0.10760151
   0.07216594]]
layers.1.weight: [[-0.01049661  0.01625079 -0.08998056 ... -0.02176772  0.01219838
   0.04373848]
 [ 0.04032597  0.01905523 -0.13569765 ... -0.040342   -0.06074575
  -0.00873575]
 [-0.04392508  0.03498745 -0.01012001 ...  0.03485042 -0.00522005
   0.11996737]
 ...
 [ 0.03772325 -0.04367152 -0.02914296 ...  0.07385474 -0.02955542
  -0.05233423]
 [-0.04278347 -0.06481794 -0.01734454 ...  0.1103199   0.00811067
  -0.03573344]
 [-0.05193374  0.01689106  0.00930285 ...  0.04674774  0.08999086
   0.00545223]]
layers.2.weight: [[-0.28563088  0.37109122 -0.21812412  0.2290965   0.2427607  -0.27290282
  -0.33541214  0.18723655  0.11074658 -0.37390327  0.18495128  0.20171818
  -0.30049756 -0.37817958  0.19519712  0.20314983  0.14582784 -0.23657714
   0.03807422 -0.3718025  -0.38360655 -0.39850613 -0.35013527 -0.2104877
  -0.38694236 -0.3759009   0.1738638   0.02656407 -0.36240947  0.25668642
   0.04201557  0.29638264  0.17166246 -0.35940245  0.2150544   0.25458807
  -0.23661797 -0.30866495  0.01951902 -0.44484428  0.1970181  -0.23332214
   0.19299643 -0.2670346  -0.28088674 -0.33237433  0.26662606 -0.3996685
  -0.33669952 -0.23301199  0.19860362  0.01304961 -0.34416986  0.22531772
   0.3544413  -0.28860262  0.11284769  0.02121902  0.26134732  0.05512834
  -0.43379056 -0.32027528 -0.29817113  0.25025696 -0.20711865  0.15471688
   0.19191746 -0.36180672 -0.15608299  0.3985073  -0.382301    0.02363314
  -0.4050335  -0.40189904  0.16758186 -0.3476487   0.1832903  -0.39218545
  -0.3264518   0.0330529   0.16899095 -0.2388756  -0.32943228  0.17016353
   0.18404843 -0.3010065  -0.18446194 -0.21158986  0.2531268   0.20542972
   0.19338068  0.23474704  0.24003816  0.19823636 -0.29805845  0.23002271
  -0.2387607   0.19727123 -0.38835952  0.17593087 -0.26867786  0.24768518
  -0.2604672   0.22805905  0.18766493  0.17507277  0.02647345 -0.31995
  -0.23340757  0.22526623 -0.33700982  0.26062167  0.19970599 -0.20437214
  -0.41707018  0.17032976 -0.36655712 -0.42186671  0.31815884 -0.3859939
   0.2146158   0.25612393 -0.28328323  0.21593934  0.18466471 -0.20386203
   0.21894976  0.18004   ]]

Final Loss: 0.0096
Distance Metric: 17.9502
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1384

================================================================================

nonoverlappingCNN_sigmoid -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[-0.00827547  0.00775084  0.00611127 ... -0.00297357  0.
  -0.00183575]
 [-0.01205459  0.011134    0.00879278 ... -0.00430372  0.
  -0.0026392 ]
 [-0.02004005  0.01763315  0.01411224 ... -0.00704182 -0.0002885
  -0.00423085]
 ...
 [ 0.0156756  -0.01421828 -0.01126349 ...  0.00555638  0.
   0.00337781]
 [-0.0031087   0.00294203  0.00231919 ... -0.00112308  0.
  -0.00069728]
 [-0.01251481  0.01153432  0.00911334 ... -0.00446418  0.
  -0.00273531]]
layers.1.weight: [[ 0.          0.          0.         ...  0.          0.
   0.00012791]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.         -0.00015064 -0.00014749 ...  0.00015722  0.
   0.        ]
 ...
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[-1.58469731e-04  0.00000000e+00  2.04041484e-04 -2.60208355e-04
  -2.46693846e-04 -3.58912948e-04  3.40828206e-04 -1.71720108e-04
   1.26714076e-04 -2.35722182e-04 -2.39510118e-04  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -1.40178410e-04 -1.05244784e-04
   1.93036467e-04 -3.82549944e-04  0.00000000e+00 -7.11804256e-04
   0.00000000e+00 -4.23549529e-04  0.00000000e+00  1.14294395e-04
  -1.60884447e-04 -1.64188634e-04 -1.67352031e-04 -2.74348073e-04
   1.33435693e-04  2.46253039e-04 -3.04283982e-04  2.96546437e-04
   1.48354651e-04 -2.80952227e-04 -1.56882670e-04 -1.16038682e-04
   1.32595887e-03  0.00000000e+00 -1.66586557e-04  1.20410885e-04
   0.00000000e+00  0.00000000e+00  0.00000000e+00  3.40272818e-04
   0.00000000e+00  6.11689524e-04 -2.48339697e-04  0.00000000e+00
  -2.20361471e-04  0.00000000e+00 -1.59190691e-04  1.69301115e-04
   0.00000000e+00  7.63126591e-04 -3.61326413e-04  4.72356769e-04
   1.29118649e-04  7.88379344e-04 -1.55716363e-04 -2.69899872e-04
   0.00000000e+00 -1.57409435e-04  2.66131858e-04  3.27064656e-04
  -8.99749110e-04  0.00000000e+00  0.00000000e+00  2.38163804e-04
   1.28736079e-04  0.00000000e+00 -1.19046861e-04  0.00000000e+00
  -1.86191857e-04  4.03800193e-04  1.82952776e-04  0.00000000e+00
   5.70992590e-04  0.00000000e+00  1.72510818e-01 -1.89225504e-03
   0.00000000e+00 -3.84724292e-04 -1.66794474e-04 -7.55915069e-04
   0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.72996956e-04
  -2.30063015e-04  3.17186437e-04 -6.53458934e-04  1.66178404e-04
   0.00000000e+00  2.10559389e-04  2.66612769e-04 -4.08684224e-04
   3.82080238e-04  0.00000000e+00  1.80330258e-04 -3.79015575e-04
  -1.68381695e-04 -4.51553584e-04  5.13100880e-04 -7.35499721e-04
   9.81663354e-04  1.00848956e-04  1.77613023e-04  0.00000000e+00
  -4.53097309e-04  0.00000000e+00  0.00000000e+00 -3.81760328e-04
   1.87400394e-04  0.00000000e+00  0.00000000e+00  1.31830369e-04
   4.23301914e-04  0.00000000e+00  0.00000000e+00 -3.16888210e-04
   1.26139130e-04 -2.94478552e-04 -9.31659422e-04 -3.06089409e-04
  -3.03154608e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00]]

Final Loss: 0.2525
Distance Metric: 7.4644
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 689

================================================================================

nonoverlappingCNN_relu -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.06750523  0.07293868 -0.02777619 ... -0.03465508  0.04449961
  -0.01009678]
 [-0.36618793  0.39584202 -0.16381526 ...  1.1424549  -1.1568494
   0.33697775]
 [ 0.05741327 -0.06136079  0.02185961 ...  0.02462896 -0.0300513
   0.00614645]
 ...
 [-0.00896849  0.00873311 -0.0033023  ... -0.00628875  0.00710539
  -0.00208632]
 [ 0.0578528  -0.06191418  0.02208012 ...  0.02480442 -0.03038706
   0.00619851]
 [ 0.06262481 -0.06752061  0.0244078  ...  0.02755408 -0.03437611
   0.00696194]]
layers.1.weight: [[ 0.01445221 -0.10469668 -0.01125834 ...  0.00203721 -0.01137318
  -0.01251671]
 [-0.01428264  0.09947547  0.01021027 ... -0.00247728  0.01026408
   0.01133726]
 [-0.03625108  0.25154683  0.02641823 ... -0.00603838  0.02664276
   0.02939146]
 ...
 [ 0.01581065 -0.11389513 -0.01217145 ...  0.00229694 -0.0123137
  -0.01355087]
 [ 0.00045102 -0.01832403 -0.00343066 ... -0.00139541 -0.00342843
  -0.00362242]
 [ 0.03251599 -0.22659397 -0.02389025 ...  0.0052759  -0.02413628
  -0.02663106]]
layers.2.weight: [[ 0.6128141  -0.5906316  -1.4915036   0.8786772  -1.2380266  -1.1807499
   1.1666914   0.7640626   0.4334443  -1.2574726   0.481773   -1.439928
   1.771065   -0.6629619   0.6923753  -0.37533692  1.179493   -1.2219332
   0.21077159 -0.11791295  0.43024996  1.1413207   1.5446501   0.09839167
  -0.806584    0.8192728  -0.36418194  0.17137407  1.0468833   0.2454367
   0.6569938   0.4265947  -0.78006774  0.6167126   0.6671722  -1.4032302
   0.8646051   1.3574946   1.0427425   0.69639206  0.1771822  -0.9334553
   0.37628257  0.6320536  -1.457104    0.29020807  0.50171506 -0.126621
   0.92951274 -1.5213215  -0.96359587  1.0947107  -1.4617345   0.6866431
   0.15243508  0.9165098   1.2312267   0.9749924  -0.9261874   1.7711841
  -0.81011647 -0.22503048 -0.8575386  -0.10492221  0.962868    0.62843907
   1.2839952  -1.5118265  -1.7572144   0.27370173  1.0672815  -0.930776
  -0.7724402  -1.5826975   0.21552953  1.1169745   0.20050184 -0.86858773
   0.7894025   0.16538522 -1.1708508   0.602581    0.9376832  -0.04022536
   1.2810541  -0.3267954  -1.6874274  -1.9398547  -1.5726091  -0.5857613
   0.9335698  -0.51978916  0.4346933   0.82271826  0.02412473  1.2985498
   1.0042962   0.4752532  -1.3195258  -1.2519629  -1.138585   -1.3496736
   0.14236894 -1.1642017   0.4100033   0.13804018  1.2768203  -0.22280714
  -0.98684037  0.70670474 -1.6242884   0.62723225  0.25949544 -0.17490724
   0.01086257 -0.78996664  0.7805742  -1.4571992  -0.14974472  0.46964145
  -1.3951744   1.7093803  -0.8975369   1.2010791   1.5399375   0.66752625
   0.09775225  1.3360482 ]]

Final Loss: 3.2542
Distance Metric: 37.9566
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 765

================================================================================

nonoverlappingCNN_relu -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[-4.5614163e-03  5.4665171e-03 -1.6875602e-03 ...  7.3964056e-04
   0.0000000e+00  4.0031059e-04]
 [ 0.0000000e+00  0.0000000e+00 -2.0204732e-04 ...  4.0660508e-02
  -4.4295792e-02  1.3720956e-02]
 [ 1.5942798e-03 -9.0307090e-04  0.0000000e+00 ... -8.0049727e-03
   9.3450788e-03 -2.5418091e-03]
 ...
 [-8.8268956e-03  9.3341609e-03 -2.7645535e-03 ... -2.8537231e-04
   6.7165669e-04 -1.4518302e-04]
 [ 2.4721961e-01 -2.7076724e-01  8.3393462e-02 ...  0.0000000e+00
   0.0000000e+00 -6.1960530e-04]
 [-2.0116973e-03  2.2370049e-03 -7.0136279e-04 ...  0.0000000e+00
  -1.1148877e-04  1.0714032e-04]]
layers.1.weight: [[ 0.00013875  0.00649531  0.         ...  0.          0.00021545
   0.        ]
 [ 0.01343132 -0.00036175 -0.0019523  ...  0.02456178 -0.03379689
   0.00570445]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 ...
 [-0.00030475 -0.00376864  0.00145931 ... -0.00037246  0.0021092
   0.        ]
 [-0.0005456  -0.00672467  0.00259481 ... -0.00064817  0.00379021
  -0.00023085]
 [ 0.         -0.00029274  0.00010934 ...  0.          0.00024769
   0.        ]]
layers.2.weight: [[-2.21279636e-01  1.89154983e-01  1.24693164e-04 -1.29942283e-01
  -2.52889134e-02 -1.24251544e-01  5.87476969e-01  0.00000000e+00
   7.63375387e-02  2.24896386e-01  1.90766789e-02  3.32967341e-01
   1.99282523e-02 -1.64228771e-02  1.35990903e-01 -5.45176446e-01
  -1.71598000e-03 -9.54363570e-02  3.99628729e-02 -1.49307474e-02
   3.50483894e-01 -3.35818306e-02  5.19353710e-02  0.00000000e+00
   0.00000000e+00  0.00000000e+00 -5.06147044e-03 -4.54590172e-02
  -6.43547952e-01  6.13363609e-02  3.38044353e-02  2.71786213e-01
   1.63781002e-01  0.00000000e+00 -2.71474719e-02  2.15301156e-01
  -6.85480051e-03 -2.70662140e-02  1.13175157e-02 -3.82545847e-03
  -1.07416773e+00  2.62395304e-04 -3.49170715e-02  4.87702876e-01
  -6.29128739e-02 -2.85173893e-01  0.00000000e+00  6.99568614e-02
   1.30326515e-02  8.50814134e-02  3.00301731e-01 -3.77550209e-03
   0.00000000e+00  1.18308589e-01 -1.05975580e-03  0.00000000e+00
   2.73545295e-01  4.46124047e-01  7.13915974e-02 -1.76329486e-04
  -2.97227837e-02  1.50570571e-01 -7.64385760e-02  7.41331160e-01
   1.30323663e-01 -1.75969512e-03 -1.85126439e-04 -3.97939146e-01
  -1.08009025e-01  0.00000000e+00  4.18696553e-02  9.30584967e-03
  -8.38757958e-03 -5.37799656e-01 -2.77089495e-02  1.88440652e-04
  -1.85884368e-02  4.11642909e-01 -5.93721159e-02 -1.35631002e-02
   1.18932448e-01  0.00000000e+00 -1.91487949e-02 -1.36956364e-01
  -1.89323217e-01  0.00000000e+00  0.00000000e+00 -8.45956355e-02
  -1.15774357e+00  0.00000000e+00  3.70273799e-01 -1.13419490e-02
   1.30613102e-04 -7.27195898e-03 -4.94942889e-02 -1.77393854e-02
  -1.27538582e-02  5.18327057e-02 -2.29576528e-02  0.00000000e+00
   3.09351474e-01  1.93411320e-01 -1.41714858e-02 -1.05928883e-01
  -7.25570843e-02 -2.47829825e-01 -2.38952175e-01 -2.10915171e-02
   5.02464734e-02  4.90065925e-02 -4.27346528e-02 -4.36620936e-02
   0.00000000e+00  7.26605773e-01 -1.88469887e-01  0.00000000e+00
   4.14551556e-01  9.37413722e-02  6.59900844e-01 -1.21412202e-04
   6.62860215e-01  1.72177068e-04 -3.76668274e-02  1.56822488e-01
   1.79453855e-04 -1.27357647e-01 -2.26377979e-01 -9.51784197e-03]]

Final Loss: 0.0003
Distance Metric: 12.7411
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 798

================================================================================

nonoverlappingCNN_relu -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[-0.11621707  0.05368941 -0.02689437 ... -0.04337208 -0.04401435
  -0.00393163]
 [ 0.07042319  0.194249   -0.11750305 ... -0.22513647  0.03281476
  -0.08490524]
 [ 0.6139768  -0.5239818  -0.04782342 ... -0.27516085  0.4972682
  -0.22690496]
 ...
 [ 0.04692962 -0.02661226  0.01825419 ...  0.04336856 -0.0647515
   0.01624235]
 [-0.10689531  0.09673198 -0.07802705 ...  0.32188174 -0.42901644
   0.06011828]
 [-0.07342878  0.0682183  -0.02650468 ... -0.03824593  0.04421477
  -0.01821335]]
layers.1.weight: [[-3.3359704e-04 -4.4122217e-03 -2.2665104e-03 ... -4.9133768e-04
   1.4079579e-03  6.7402964e-04]
 [-8.6049601e-02 -1.5279995e-01  3.8621034e-02 ... -3.2378841e-02
  -4.6042451e-01  2.4897324e-02]
 [ 1.2884117e-03  1.6898697e-02  8.6904746e-03 ...  1.8331006e-03
  -5.4047033e-03 -2.5106755e-03]
 ...
 [ 4.4948919e-04  5.9688399e-03  3.0666240e-03 ...  6.6041213e-04
  -1.9041704e-03 -9.0381433e-04]
 [-1.4994115e-03 -1.9566284e-02 -1.0068004e-02 ... -2.1053879e-03
   6.2641385e-03  2.8854380e-03]
 [-1.2316381e-03 -1.6124755e-02 -8.2922205e-03 ... -1.7536139e-03
   5.1561780e-03  2.4019510e-03]]
layers.2.weight: [[-3.89719871e-03  5.21803916e-01  1.48270158e-02 -1.09775970e-02
   1.46477399e-02 -5.44542121e-03  1.16474051e-02 -3.99587722e-03
   1.20150578e+00  1.69592891e-02  2.15373691e-02  8.88261653e-04
  -1.59699447e-03 -1.27389915e-02 -6.60557393e-03 -7.46787060e-03
  -6.43757870e-03 -1.64498389e-03 -1.57998409e-02  1.17761607e-03
   9.27701592e-03 -1.46930302e-02 -1.03882756e-02  2.15107831e-03
  -5.21194702e-03 -1.20397378e-02  1.00440430e-02  1.31517071e-02
   1.12258857e-02  1.50787299e-02 -6.44253939e-03  7.84379151e-03
  -1.10322312e-02  5.57728810e-03  5.82462829e-03  1.65298302e-02
  -9.74736456e-03 -1.02803446e-02  1.32061001e-02 -7.59463152e-03
  -1.42695624e-02 -9.12989303e-03  2.18896056e-03  1.17593687e-02
  -6.87811757e-03  4.75684881e-01  3.15620517e-03 -1.30696753e-02
  -1.65619254e-02 -8.58865026e-03  1.75708923e-02 -6.87507447e-03
  -1.13529228e-02 -1.22083677e-03 -1.57546287e-03  7.16385152e-03
  -2.76199053e-03  7.84981064e-03  2.16056313e-02  6.09262148e-03
   1.03095854e-02  4.12214547e-03 -3.92383397e-01 -1.38944816e-02
  -1.23490188e-02  6.55300496e-03 -6.92355260e-03  4.60472107e-01
  -4.09472082e-03  4.54139113e-01 -7.42822234e-03 -1.05230622e-02
  -3.99954151e-03 -5.81394374e-01  1.04921469e-02  1.20087247e-02
  -1.05826948e-02 -1.75616015e-02  6.42854441e-03 -6.49501628e-04
   6.01266146e-01  1.23992981e-02  1.01422006e-02 -8.71030241e-03
  -1.38975121e-02  1.36827296e-02 -2.50027347e-02 -7.43724545e-03
  -1.24499737e-03  9.13794059e-03 -1.27168417e+00 -1.74747091e-02
  -1.00820037e-02 -1.50390193e-02  1.13443723e-02  1.82896527e-03
   6.72436599e-03  1.86083969e-02 -8.21712427e-03 -1.10114133e-02
  -5.84661782e-01  1.12902634e-02  1.38620399e-02  1.07030533e-02
   1.33116124e-02 -1.40031325e-02  8.26936960e-03 -6.41773967e-03
   1.83345983e-03  3.49409180e-03  6.97983243e-03  4.00788896e-03
  -1.53591223e-02  2.06079837e-02 -9.61726718e-03 -2.09896173e-02
   4.85807657e-03  3.22174537e-03 -9.50269122e-03 -1.11737391e-02
  -2.15942655e-02  7.89871812e-03 -1.24770170e-02  6.07742975e-03
  -2.72390689e-03  5.26681682e-03 -1.71351545e-02 -1.41552938e-02]]

Final Loss: 3.4907
Distance Metric: 23.0244
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1036

================================================================================

nonoverlappingCNN_tanh -> fcnn_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.00610899 -0.00696264  0.00015466 ... -0.00437125  0.0054603
  -0.00211451]
 [ 0.00623804 -0.00593373  0.00059435 ...  0.00372414 -0.00241054
   0.00026545]
 [ 0.00609846 -0.00989936 -0.00085069 ... -0.02315376  0.02367496
  -0.007643  ]
 ...
 [ 0.02083999 -0.01922778  0.00591787 ...  0.07867815 -0.07717381
   0.02211336]
 [ 0.00574033 -0.00791194 -0.0004178  ... -0.01446848  0.01530511
  -0.00508187]
 [ 0.00582522 -0.00745444 -0.00020496 ... -0.01062439  0.01156499
  -0.00395162]]
layers.1.weight: [[ 0.          0.          0.         ...  0.00310189 -0.00012068
   0.        ]
 [-0.02865812 -0.02406271 -0.04121171 ... -0.04872214 -0.03348054
  -0.03135214]
 [ 0.06543665  0.0631016   0.07092835 ...  0.04524202  0.06828532
   0.06718401]
 ...
 [ 0.          0.          0.         ...  0.0032166  -0.000108
   0.        ]
 [ 0.          0.          0.         ...  0.00303977 -0.00012759
   0.        ]
 [ 0.          0.0001056   0.         ...  0.00339309  0.
   0.        ]]
layers.2.weight: [[ 0.05795254 -2.9492621  -7.528214    0.05727845  0.06000843  0.05784219
   0.05904814  0.05745611  0.05998332  0.05834628  0.06004637  0.05796133
   0.05944923  0.05727343  0.06020531  0.0587508   0.06003539  0.05742706
   0.05759423  0.05889124  0.05922815  0.06044655  0.05753322  0.0608315
   0.05872485  0.05605781  0.05884412  0.06054629  0.05998112  0.06055288
   0.05758274  0.05765015  0.05685847  0.05686677  0.06004731  0.057482
   0.05918492  0.05328192  0.05892929  0.05761053  0.05802666  0.05807372
   0.05910895  0.05755697  0.0576899   0.0611386   0.06037891  0.05790383
   0.05479052  0.05905475  0.0583645   0.05953438 -2.4890735   0.05958619
   0.05753576  0.05884746  0.05894719  0.05978219  0.06007885  0.06077534
   0.06038456  0.05722779  0.05848692  0.06051082  0.05968091  0.05773917
   0.05684987  0.05700312  0.05898208  0.05277611  0.05775764  0.05760086
   0.05753718  0.05786786  0.05757087  0.06052374  0.06089253  0.05786578
  -3.3790717   0.05762481  0.06027834  0.05977042  0.0589702   0.05836872
   0.02685163  0.05891741  0.06020449  0.05803757  0.05880498  0.05770073
   0.05750801  0.05986433 -3.420128    0.0575808   0.06071841  0.05694907
   0.05969741  0.05765355  0.06080332  0.06019808  0.05960685  0.05727039
   0.0576658   0.05908695  0.058132    0.05817865  0.06052548  0.05792286
   0.060044    0.057173    0.05744472  0.05904078  0.05751481  0.06081098
   0.05962074  0.0575108   0.05859796  0.0605258   0.05795145  0.06031864
   0.05882035  0.06059931  0.05839605  0.05988057  0.05843059  0.05909134
   0.0573346   0.06084945]]

Final Loss: 0.1896
Distance Metric: 33.8370
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1756

================================================================================

nonoverlappingCNN_tanh -> fcnn_relu

Student Model Parameters:
layers.0.weight: [[ 0.05407489 -0.02143222 -0.00760239 ...  0.00892741  0.06839909
  -0.09765883]
 [-0.06884493 -0.06948027  0.03166706 ...  0.05374898 -0.07693411
   0.18343641]
 [ 0.14031824  0.14118281 -0.06691088 ... -0.06276372 -0.14718664
  -0.11577296]
 ...
 [ 0.04974583 -0.13934845 -0.04858193 ...  0.03078171  0.12650304
   0.04861636]
 [ 0.11000656 -0.02546128  0.06751946 ... -0.17778733 -0.10428563
   0.08166894]
 [ 0.44708878 -0.46259576  0.15172057 ... -0.00304497 -0.01268991
  -0.006744  ]]
layers.1.weight: [[ 0.00289675  0.01401566 -0.00195842 ...  0.00278001  0.00974888
  -0.00469975]
 [-0.08905196  0.02900291 -0.01701459 ... -0.03875465 -0.05537462
  -0.11342192]
 [ 0.00670555 -0.01960824 -0.00672129 ...  0.02896903  0.00959878
  -0.07239094]
 ...
 [-0.02840501 -0.03788109 -0.03689583 ...  0.04512468 -0.10538746
  -0.01939612]
 [ 0.01372107 -0.05244825 -0.0067223  ...  0.10798974 -0.10942321
   0.00473992]
 [ 0.04246001 -0.08677983 -0.03582769 ... -0.05776127 -0.01108507
   0.04135668]]
layers.2.weight: [[ 3.07226162e-02  2.37910971e-01  2.20910817e-01 -3.73676538e-01
  -5.22831619e-01  2.60229260e-01  1.32657203e-03  2.90055841e-01
  -3.98171455e-01 -5.00118554e-01 -3.19676340e-01  7.77439848e-02
   4.48302358e-01 -3.19242507e-01  1.84641522e-03  2.85993159e-01
  -2.54522502e-01  3.24052930e-01  1.55675322e-01  2.26449058e-03
   3.65329564e-01  4.97146160e-04 -2.85761833e-01  2.56227732e-01
   1.87344670e-01  2.12265149e-01 -3.81849468e-01 -5.16653121e-01
  -2.20117807e-01  5.97503662e-01  1.62906751e-01 -8.45786273e-01
   1.78774431e-01  3.20549682e-02  2.96876669e-01 -4.23431903e-01
  -4.08800870e-01  1.71866715e-01 -2.18077838e-01  1.31570175e-01
  -2.81643301e-01 -4.45766717e-01  1.53398916e-01  1.13844834e-02
   6.04814626e-02  1.77284911e-01  2.39974007e-01  3.42074811e-01
  -4.29406136e-01  3.29534560e-01  1.38148919e-01  2.27435350e-01
   3.70759368e-01 -4.03864890e-01 -3.58966917e-01  3.75249237e-01
   1.00356497e-01  2.61299998e-01 -2.43338868e-01  3.46300185e-01
   1.51105106e-01 -3.47539991e-01  3.11751783e-01 -2.62495786e-01
   3.51236820e-01 -9.57202256e-01  3.21252853e-01 -2.38045245e-01
   1.11524254e-01  2.60562629e-01  7.65430555e-02 -4.03470129e-01
   2.74539024e-01 -2.90390849e-01 -3.97080958e-01 -3.02359223e-01
  -4.62251157e-01  2.17211261e-01  2.26619065e-01 -3.96185189e-01
   7.76859894e-02  3.31090301e-01  1.63584501e-01 -3.72749865e-01
   9.02383551e-02  2.72554934e-01 -3.30684572e-01 -3.17462802e-01
   2.30528772e-01  4.18830365e-01  2.76812911e-01  3.14845353e-01
  -3.41092259e-01  2.60114700e-01 -3.49290669e-01 -3.64468664e-01
  -3.08928579e-01  2.61310339e-01 -3.66063744e-01 -3.75877887e-01
   1.96024105e-01 -3.56722564e-01  2.31159572e-02 -5.83503067e-01
   2.64968216e-01  3.37659031e-01  1.97238609e-01  1.73986092e-01
  -3.90853256e-01 -3.50613743e-01  2.23703220e-01 -2.55991191e-01
   2.86951333e-01  1.55060977e-01 -3.03335190e-01  3.85939121e-01
  -3.00420284e-01 -6.79431558e-01  1.79467157e-01  1.30305467e-02
  -5.41014671e-01  3.42932134e-03  2.92867601e-01 -2.75628865e-01
  -3.72090548e-01 -1.64810672e-01 -2.47410893e-01 -2.95157850e-01]]

Final Loss: 0.1924
Distance Metric: 19.7817
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1585

================================================================================

nonoverlappingCNN_tanh -> fcnn_tanh

Student Model Parameters:
layers.0.weight: [[ 0.00381525 -0.00398071  0.00119365 ...  0.00876345 -0.00908337
   0.00298552]
 [-0.0020743   0.00241854 -0.00086553 ...  0.00109522 -0.00173549
   0.        ]
 [ 0.00386475 -0.00428867  0.00094829 ...  0.00468295 -0.00510602
   0.00146122]
 ...
 [ 0.00065879 -0.00047416 -0.0002019  ...  0.00701445 -0.00817488
   0.00191518]
 [ 0.00492321 -0.00533795  0.0016571  ...  0.00724805 -0.00713345
   0.00266589]
 [-0.00472509  0.00573116 -0.00107107 ...  0.00429612 -0.00440506
   0.00149343]]
layers.1.weight: [[ 0.0001708   0.          0.         ...  0.          0.00019308
   0.00018433]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.00010387  0.          0.         ...  0.          0.00011777
   0.00011093]
 ...
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [-0.00021541  0.          0.         ...  0.         -0.00024267
  -0.00023452]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.2.weight: [[ 4.81775030e-02  1.24931037e-02  2.91336644e-02 -1.16386199e-02
  -7.17253014e-02 -6.57811819e-04  2.00691186e-02  3.89150828e-02
   4.34662625e-02  9.87759233e-03 -1.01289675e-02 -1.81345902e-02
   4.10884097e-02 -2.19546519e-02 -4.14926857e-02  2.81599350e-02
   3.45798135e-02  1.68641750e-02  6.82946853e-03  6.19299524e-02
   6.19561737e-03  3.43940109e-02  7.09629506e-02  1.54045606e-02
  -3.26259360e-02 -6.53725564e-02 -2.96926461e-02 -1.52875939e-02
   2.81546786e-02  3.31562012e-02 -5.62883429e-02 -6.51467824e-03
   5.05271368e-02  1.65607389e-02  1.81939756e-03 -3.77761126e-02
  -1.72204375e-02  3.58346216e-02  1.13089662e-02  6.49223775e-02
   8.64024758e-01 -3.84964459e-02 -1.34108576e-03 -2.66566668e-02
   6.18636236e-02  6.55344082e-03 -1.70223154e-02 -7.67029643e-01
  -1.01822424e+00  3.91531177e-02  8.68185535e-02  5.33453748e-03
   1.57340604e-03  1.64903942e-02 -2.41153780e-02  2.38684118e-02
   4.67845500e-02  5.10561503e-02 -1.95707707e-03 -5.14934547e-02
  -5.13170660e-02  3.21570598e-02 -2.32407581e-02 -1.70648377e-02
   3.73134241e-02 -6.94067657e-01  3.85340415e-02 -3.26118656e-02
   9.84845497e-03 -3.21802148e-03  3.83767523e-02  3.77362482e-02
   2.39377040e-02 -3.54696289e-02 -7.20989853e-02  1.28809186e-02
  -4.21242602e-02  2.18052417e-02  2.48097815e-02  9.83011536e-03
  -4.58574444e-02 -1.16731953e-02 -1.64077692e-02  6.92252293e-02
   6.12971885e-03  1.80966761e-02 -1.64197553e-02  3.57113369e-02
  -4.86151502e-02  1.01003125e-02 -4.51303050e-02  1.19130705e-02
  -1.23314681e-02 -3.51062603e-02  5.24472371e-02  2.87725367e-02
   6.91686347e-02  1.15494104e-02 -1.28899496e-02 -5.23909591e-02
  -9.05947667e-03 -1.61151041e-03  7.33467788e-02  4.28697579e-02
  -6.23072498e-02  2.88083162e-02 -6.41987473e-02 -4.64694202e-02
   7.53951492e-03 -1.16103608e-02  6.06151856e-02 -1.08424556e-02
  -4.73832572e-03 -1.90999582e-02 -1.45101408e-02  4.82064672e-02
  -2.90739834e-02 -5.36273085e-02 -1.38687985e-02  4.35702652e-02
  -5.34911081e-03  6.24030316e-03 -1.23741000e-03 -2.95473449e-02
   1.31435171e-02  1.84637140e-02 -6.10143133e-02 -1.71821527e-02]]

Final Loss: 0.0006
Distance Metric: 10.0181
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 3
stopped after epoch: 1218

================================================================================

