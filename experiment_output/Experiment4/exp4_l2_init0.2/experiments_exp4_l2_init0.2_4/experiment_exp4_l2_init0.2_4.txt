Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.03646512  0.03986567 -0.01242944 ... -0.03318575  0.03884267
  -0.01116675]
 [-1.0449344   1.1412939  -0.3469223  ...  0.04152339 -0.03614754
   0.01107285]
 [-0.0059955   0.00656538 -0.00205652 ... -0.00547073  0.0064024
  -0.00184756]
 ...
 [-0.0039011   0.00427201 -0.00133815 ... -0.00356111  0.00416731
  -0.00120266]
 [-0.04286752  0.0468327  -0.01457206 ... -0.03896531  0.0456094
  -0.01309128]
 [-0.01095646  0.01199669 -0.00375696 ... -0.00999395  0.01169611
  -0.00337423]]
layers.1.weight: [[ 0.01450236 -0.12719142  0.00235147 ...  0.00152631  0.01712123
   0.00430822]
 [-0.01197564  0.10425862 -0.00199579 ... -0.00131809 -0.01412637
  -0.00360304]
 [-0.01080462  0.09399149 -0.00180243 ... -0.00119118 -0.0127449
  -0.00325222]
 ...
 [ 0.00140982 -0.01313438  0.00015775 ...  0.          0.00167986
   0.00035936]
 [ 0.00124496 -0.01169015  0.00013133 ...  0.          0.0014851
   0.00031064]
 [ 0.0086286  -0.07604912  0.00134889 ...  0.0008546   0.01019786
   0.00252122]]
layers.2.weight: [[-2.55492359e-01  2.09881842e-01  1.89241007e-01  5.66236563e-02
   1.79260492e-01 -6.06932752e-02  9.24156979e-04  1.74825713e-01
   8.30487907e-02  2.73194253e-01 -1.33554906e-01 -1.04967378e-01
   1.01430062e-02 -1.82956412e-01 -1.53009994e-02  1.34243503e-01
   1.23526782e-01  4.29277793e-02 -6.52138293e-02 -1.21954672e-01
   8.12404230e-03 -1.11983463e-01 -9.39936563e-02 -1.24070339e-01
  -6.37524799e-02 -2.87221134e-01 -9.26976372e-03  5.32282479e-02
   2.50973552e-01  1.29588544e-01 -5.37357777e-02  4.91488278e-01
  -8.46436322e-02 -3.45614672e-01  1.69046357e-01 -1.54333115e-01
   1.30181134e-01 -3.20461346e-04  1.46775171e-01  2.26690814e-01
   2.34591272e-02  1.60330847e-01 -1.68457255e-01  3.35754067e-01
   2.60541327e-02 -1.33258328e-01  1.12495460e-01 -1.66573182e-01
  -1.62954554e-01  3.44262481e-01  2.05295056e-01  1.45880284e-03
  -2.27990493e-01 -3.09576523e-02 -7.37040043e-02  1.23029485e-01
   5.33222780e-02  1.63046092e-01 -1.04636997e-01 -1.28301382e-02
   1.98643669e-01 -1.89937577e-01  1.90411210e-01 -7.17179710e-03
  -1.82636708e-01 -1.59802571e-01 -8.88732895e-02 -2.08520234e-01
   8.15772265e-02 -8.85829255e-02 -6.01283982e-02  3.97113889e-01
  -1.29571036e-01  1.55115545e-01 -1.58855319e-01 -2.14334071e-01
  -1.65795282e-01  3.62966657e-02 -2.25486174e-01 -3.07505876e-02
   1.65358499e-01 -1.21930614e-01 -1.71826586e-01  3.03592831e-01
   1.45206183e-01 -2.50493556e-01 -5.34637012e-02  7.84446578e-03
   2.03750715e-01 -2.97081061e-02 -2.08167940e-01 -5.23271449e-02
   3.17472905e-01  1.94678605e-01 -9.27833766e-02 -2.60122150e-01
  -7.36431032e-02  6.76127821e-02 -2.29024738e-02 -1.45560309e-01
  -1.57633651e-04 -6.43993670e-04 -3.65710407e-02  2.31084764e-01
  -2.74216142e-02 -2.43505001e-01  3.53200436e-01  4.18088622e-02
   2.13732406e-01 -5.11824667e-01 -9.54689384e-02 -1.17404662e-01
   2.72506714e-01  9.73019227e-02  7.60205137e-03  9.17212367e-02
  -8.92460868e-02 -1.09336479e-02 -7.85583332e-02 -2.69433439e-01
   1.38886958e-01  7.33015537e-02 -1.67885460e-02 -3.34317893e-01
   1.52124763e-01 -2.60815546e-02 -2.31779944e-02 -1.52630359e-01]]

Final Loss: 0.0003
Distance Metric: 10.4813
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 2183

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.06251788  0.04712873 -0.03685807 ... -0.09433472 -0.10223486
  -0.10858316]
 [ 0.12054957  0.09197991  0.07179151 ...  0.11083387 -0.09393291
  -0.04731882]
 [-0.00773043 -0.08311094 -0.12388875 ... -0.13067847 -0.03902227
  -0.18272394]
 ...
 [-0.07015284  0.02152053  0.08248564 ...  0.03527473  0.03800726
   0.1317043 ]
 [-0.1867314   0.17053927 -0.04084323 ... -0.06404797  0.00469626
  -0.01850159]
 [-0.03136204  0.09431354  0.02672704 ...  0.06882393  0.02139997
  -0.14923248]]
layers.1.weight: [[-0.00602814  0.08028925  0.0760673  ... -0.0700358  -0.06456056
   0.01915227]
 [-0.03612279  0.03342388 -0.08903623 ... -0.02032856  0.02290692
   0.00965322]
 [-0.03570667  0.00916533 -0.00944638 ... -0.01950285 -0.00731358
   0.04934192]
 ...
 [-0.02420381 -0.03566508  0.04745585 ...  0.01185097 -0.01845654
  -0.01004988]
 [-0.00094195 -0.12054376  0.06401207 ... -0.00233974  0.02133996
   0.04348743]
 [-0.01106326  0.02697101 -0.00850946 ... -0.01818478 -0.02285285
   0.02855339]]
layers.2.weight: [[ 0.14314203 -0.2837517   0.12823315  0.17027846 -0.38295287  0.25377932
   0.21794058 -0.39182654 -0.38834786 -0.26446667 -0.29450673  0.20799553
   0.08609992  0.18025903  0.1360691   0.00063633 -0.3558101  -0.30814278
   0.14299673  0.22102737 -0.39255407  0.19623213 -0.37489563  0.27666524
  -0.18667568  0.00045391 -0.35226893 -0.21441239  0.2089214   0.00157798
   0.19858058  0.16010746 -0.38339293  0.19378346  0.15937586 -0.37242782
   0.17304677 -0.3883791   0.00202802 -0.3665727  -0.2286061   0.22072531
  -0.24694763  0.23225094  0.0785327  -0.39576635 -0.34036046  0.19065571
  -0.37041616  0.2582278   0.16318093 -0.22891304  0.22695729  0.00318728
  -0.2815975  -0.2713481   0.17050701 -0.3917241  -0.27033716  0.27223426
  -0.3838759   0.21904518  0.18944958  0.16328442 -0.2222448  -0.318863
   0.17201234  0.26562858 -0.36531147 -0.35763     0.20827004 -0.39581764
   0.31655926  0.00129145  0.20705281 -0.22512098 -0.3914483   0.1899261
  -0.16548106  0.20361255 -0.43486717 -0.41319653 -0.36367163 -0.36916578
   0.28855515 -0.3869882  -0.31666014  0.14945166  0.17526788 -0.31352803
  -0.25614297 -0.34201178 -0.3256456   0.14951408 -0.32329583  0.16317435
  -0.3768413   0.13401148  0.13953406  0.11943303  0.20928666 -0.34618482
   0.17678519 -0.39181525  0.2756677   0.00147117 -0.2589087   0.15162233
  -0.38233584  0.10215272 -0.25371754  0.00168317  0.18949032 -0.25336063
  -0.24935628 -0.22814536  0.19025885  0.29116347  0.202118    0.00176133
   0.1806546  -0.4121061  -0.33460525  0.23681934  0.14753704  0.2560542
  -0.41026846  0.14677198]]

Final Loss: 0.0096
Distance Metric: 17.7656
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1049

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.00752814  0.02478232 -0.00742654 ... -0.01661456  0.01685084
  -0.04540718]
 [-0.00233352 -0.00740509  0.00254939 ...  0.00474656 -0.00505592
   0.01340191]
 [-0.00507668 -0.01635791  0.0053665  ...  0.01068186 -0.01114479
   0.02974202]
 ...
 [ 0.00205739  0.00646967 -0.0022302  ... -0.00413647  0.00441842
  -0.01172721]
 [ 0.00415224  0.01326829 -0.00444871 ... -0.00859932  0.00905253
  -0.0240847 ]
 [ 0.00935141  0.03165117 -0.00844998 ... -0.02160337  0.02151787
  -0.05833549]]
layers.1.weight: [[ 0.00018318 -0.00051023  0.         ... -0.0003912  -0.00072307
  -0.00057722]
 [ 0.00082928 -0.00022652 -0.00059204 ... -0.00081848  0.00076863
  -0.00013348]
 [-0.00056256 -0.00016796  0.00083108 ...  0.00051228 -0.00016484
   0.00057403]
 ...
 [-0.00077736  0.          0.         ...  0.00059445 -0.0008385
   0.00083133]
 [ 0.         -0.00062956 -0.00092325 ...  0.00115004  0.000954
   0.        ]
 [-0.00026203  0.00045537  0.00014773 ...  0.          0.00037179
  -0.00082287]]
layers.2.weight: [[ 0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.         -0.21469626  0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.        ]]

Final Loss: 0.2525
Distance Metric: 7.2672
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 406

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.05620533 -0.06797733  0.02399197 ...  0.0292801  -0.02614043
   0.00736596]
 [-0.05965736  0.07268389 -0.02691829 ... -0.03799737  0.03469951
  -0.00925497]
 [ 0.05596351 -0.06769957  0.02389366 ...  0.02913777 -0.02598944
   0.00731787]
 ...
 [-0.05787041  0.07063261 -0.0261487  ... -0.03695098  0.0335567
  -0.00886989]
 [ 0.05193279 -0.06302128  0.02223718 ...  0.02678291 -0.02353009
   0.00652876]
 [ 0.05529961 -0.06693665  0.02362162 ...  0.02874703 -0.0255765
   0.0071849 ]]
layers.1.weight: [[-0.01117031  0.01313288 -0.01111705 ...  0.01273202 -0.010235
  -0.01097173]
 [-0.00314954  0.         -0.0031427  ...  0.         -0.00302866
  -0.00312393]
 [ 0.03012086 -0.03883355  0.02997137 ... -0.03769676  0.02746906
   0.02955824]
 ...
 [ 0.00741582 -0.01013957  0.00737755 ... -0.00985035  0.00674073
   0.00727241]
 [-0.00787083  0.00811733 -0.00783573 ...  0.00785366 -0.00725577
  -0.00773987]
 [ 0.01496394 -0.01936498  0.01488883 ... -0.01879965  0.01364321
   0.01468336]]
layers.2.weight: [[ 0.5455029   0.07799184 -1.551043    0.16519102  0.39318958  0.13383533
  -1.0206951   0.81961507 -1.2317156  -0.544926    1.38034    -0.54373235
   1.0746963  -0.4576701   0.4526303   0.28429228 -0.64414316  1.9141655
  -0.38817394  0.327435    1.3973967   0.10185082  0.13562195 -0.04154406
  -0.80245245  0.739502   -1.0303357   0.8581195  -0.41376045 -1.0731926
  -0.50960314 -0.46983242  0.82973766 -0.69755435 -1.5004692  -0.4389615
   0.7696405  -1.1704376  -1.1788479   0.8384458   0.19094816 -0.31469545
   0.12816739  0.6133594   1.3023374   0.5012081   0.11477072 -1.637381
   0.7813135  -1.0521443  -1.2718098   0.19246474  1.4738595   1.3396145
  -0.16339076  1.952187    0.28634298 -1.0331134   1.0722051  -0.50246793
  -1.4127936   1.4064445  -0.03809107  0.31136325  1.1178534  -0.30140328
   1.3237274  -1.5089766   1.1283116   0.11073146  1.7873967   1.0347755
   1.641655    1.6925715   1.2385489  -1.0568542  -1.9069507   0.24732749
  -0.9297993   1.543215    1.8343413  -1.1792951   1.0903331   1.7538869
  -1.8401223  -1.3313253  -1.7118211  -0.20890655 -0.37617007  1.5452458
  -0.42748803 -1.193397   -0.2336899   0.6569722  -0.3783106   0.64585286
  -0.7684509   0.5318206   0.5058042  -1.0898032   0.80665505  0.95730656
   1.1267598  -0.10894652 -0.951385    1.3645024  -0.49707928 -0.04269176
  -1.160779   -0.32677132  0.9589917   1.1366762  -1.0696764  -1.8766394
   0.24753147 -0.54154724 -1.1028054   0.08204637  1.0523578  -1.7162871
  -1.0296209  -1.4104234   1.5685961  -0.30937138  0.683265   -0.39487857
   0.36069816 -0.7724537 ]]

Final Loss: 3.3517
Distance Metric: 38.3738
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1405

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [-0.00558566  0.0060427  -0.00180568 ...  0.00024395 -0.00031448
   0.00018225]
 [-0.00750242  0.00831841 -0.00258758 ...  0.00014131 -0.0003373
   0.00013333]
 ...
 [-0.00018817  0.00010253  0.00012361 ...  0.04162293 -0.04535536
   0.01425448]
 [-0.0004558   0.00119213 -0.00032334 ... -0.00367078  0.00397703
  -0.00146847]
 [ 0.07480632 -0.08178736  0.02517789 ...  0.          0.
   0.        ]]
layers.1.weight: [[ 0.          0.01676014  0.02302874 ... -0.0005728   0.
  -0.01146855]
 [ 0.          0.02205634  0.03030978 ... -0.00070174  0.
  -0.0151193 ]
 [ 0.          0.         -0.00010143 ... -0.00083143  0.00042231
   0.00273422]
 ...
 [ 0.          0.00136983  0.00193379 ...  0.          0.
  -0.00093346]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [-0.0009119   0.00023698  0.00025543 ...  0.01308776 -0.00545714
   0.        ]]
layers.2.weight: [[ 2.12785736e-01  2.80312300e-01 -4.39931862e-02  3.69055569e-01
  -9.47472523e-04  3.53571773e-01 -8.46081898e-02 -3.42890434e-03
  -2.23386273e-01  4.17066514e-01 -2.22198680e-04  1.17596470e-01
   0.00000000e+00  7.41800666e-02 -9.54215787e-03 -2.69379903e-04
   2.29928613e-01 -7.36606479e-01 -5.34575840e-04 -4.34047281e-04
  -2.70934962e-03 -3.59946750e-02 -1.56761352e-02  1.02055231e-02
  -8.73318240e-02 -5.79291880e-01  6.02134466e-02 -2.45574309e-04
  -3.15524414e-02 -3.35346699e-01  0.00000000e+00 -1.08247176e-02
  -3.99513692e-02  4.55751628e-01 -1.02205746e-01  2.20179513e-01
   7.07861483e-02  2.34148633e-02  3.59985590e-01  3.88651848e-01
  -1.09770130e-02  5.20025268e-02  4.67283517e-01 -1.33486814e-04
  -3.44411358e-02  3.63750421e-02 -3.85698862e-03  2.86080496e-04
   9.71390784e-01  2.42129162e-01 -1.79489449e-01 -3.23716039e-03
   3.63163613e-02 -3.25383730e-02  2.61013210e-01 -6.77995209e-04
   4.24423486e-01  2.94221845e-02 -9.24133172e-04  7.81998783e-02
  -1.18466938e+00 -2.68586934e-01  4.16904539e-01 -9.49986577e-02
   3.98010164e-02  7.15552196e-02  6.56681955e-02  7.26112947e-02
  -1.13697901e-01 -2.77783155e-01 -7.72637576e-02  1.27531752e-01
  -1.02903962e-03  6.58895122e-04 -2.93807988e-03 -1.40951553e-04
   1.57722309e-02 -2.84450420e-04 -7.50705451e-02 -4.52351868e-02
  -3.91941592e-02 -1.08709417e-01 -3.74823630e-01  4.86215562e-01
  -4.50604558e-02 -3.69864487e-04 -1.19749323e-01  2.37468898e-01
   2.55016275e-02 -3.35298404e-02  4.03238721e-02 -1.77016053e-02
  -6.35970116e-01  4.94155318e-01 -8.38597298e-01 -1.58962898e-03
  -2.51218546e-02 -3.42649780e-02  3.73245269e-01 -6.07611099e-03
  -2.85609975e-03  1.34030981e-02  6.42627524e-03 -2.03727846e-04
   3.66412163e-01  4.20781642e-01  1.43711090e-01 -7.64042288e-02
  -1.04986373e-02 -1.12961931e-03  3.10112715e-01  2.29540486e-02
  -6.93019433e-03 -2.66449177e-04  2.68176317e-01 -3.72212827e-02
  -8.79383006e-04 -1.94761232e-02  2.06403926e-01 -4.20100316e-02
   5.46763167e-02  1.47227228e-01  3.30378680e-04  2.01822683e-01
  -2.71086674e-03  1.76862534e-02 -1.60975498e-03 -4.34350669e-01]]

Final Loss: 0.0003
Distance Metric: 12.3487
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 697

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.10878463  0.07184784  0.01151625 ... -1.3244897   1.4088367
  -0.4102094 ]
 [ 0.03758262 -0.02422347 -0.02698639 ... -0.03403546 -0.00940772
  -0.04871788]
 [-0.06188313  0.05860924 -0.03217941 ... -0.06326038  0.06989841
  -0.04715464]
 ...
 [ 0.00174287 -0.00378045 -0.02089862 ...  0.00961349  0.07131685
  -0.07448412]
 [-0.71847385  0.66272604 -0.08878309 ... -0.58112407  0.11369862
  -0.13254188]
 [ 0.0629159  -0.06499457  0.01911811 ...  0.04499406 -0.03224958
   0.01722059]]
layers.1.weight: [[-3.5980842e-03 -3.3329204e-03  3.5475625e-03 ... -5.6471438e-03
   8.9905038e-03 -4.0780972e-03]
 [ 1.6932820e+00 -8.3582193e-02  4.8385017e-02 ...  8.1404358e-02
   1.0775781e-01 -2.0845665e-02]
 [-9.3788502e-04 -9.7550103e-04  1.0690497e-03 ... -1.6360611e-03
   2.2538302e-03 -1.2053733e-03]
 ...
 [ 2.7438863e-03  2.6514337e-03 -2.8500701e-03 ...  4.4808197e-03
  -6.7745908e-03  3.2561875e-03]
 [ 1.2199732e-03  1.2549929e-03 -1.3769851e-03 ...  2.1166834e-03
  -2.9415770e-03  1.5553549e-03]
 [-3.9103148e-03 -3.5658525e-03  3.7833266e-03 ... -6.0510747e-03
   9.8109469e-03 -4.3589585e-03]]
layers.2.weight: [[-0.02145952  0.7278958  -0.00640665 -0.01652142  0.01086046 -0.00260663
  -0.00250127  0.0209799   0.01505472 -0.01711406 -0.00335723  0.93884736
   0.02085194 -0.00094878  0.01102575  0.00864013  0.94051707  0.00801473
   0.01439071  0.00542154 -0.0120298   0.01718567  0.01402095 -0.01962802
  -0.01185844  0.01661982 -0.01288505  0.00288441  0.01548442 -0.00825994
  -0.01089516 -0.01813961  0.01045813 -0.00280573  0.01358691 -0.01422798
  -0.00422265  0.01420547  0.01211291 -0.00827517  0.01936011 -0.01073262
  -0.00164158  0.62465054 -0.72501165  0.00344406 -0.02107836  0.01241419
  -0.00524473  0.0165642   0.01429296 -0.02021174  0.00515686 -0.01252385
  -0.01254212  0.00998031  0.0150503   0.01577436  0.01569413  0.01574535
   0.00887458 -0.01255685 -0.00404931  0.01501864  0.00193759 -0.01227476
   0.00855495 -0.22484845  0.00195414  0.00829036 -0.00492715  0.00338282
   0.23668875 -0.00798955 -0.00279536  0.016597    0.00283243 -0.01588342
  -0.01110836 -0.01744952  0.01278025  0.6562419  -0.01263174  0.01613108
   0.01881959 -0.02075322 -0.00939371  0.582546   -0.01629747 -0.01411872
  -0.01799118  0.01920731  0.01858619 -0.00249924  0.01759368  0.012514
  -0.02344521  0.01060926  0.01466753  0.01600236 -0.01788143  0.0123864
  -0.01222795 -0.00120993 -0.01345476  0.01915114 -0.02127991 -0.01065834
  -0.01663925  0.01622318  0.01112345  0.01646068  0.00953041  0.6617797
   0.01565231  0.02250126 -0.02002728 -0.01450586  0.01658871  0.01144763
  -0.00290465  0.01943257 -0.01835158  0.01811356  0.01225123  0.01718368
   0.00825412 -0.02292492]]

Final Loss: 3.5878
Distance Metric: 23.3131
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1021

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.01453914 -0.01761486  0.00680404 ...  0.00248007 -0.00286987
   0.00104497]
 [ 0.00962771 -0.01270066  0.00470477 ... -0.00020323  0.00159581
  -0.00016532]
 [-0.00722036  0.00434115 -0.0027691  ... -0.0007095   0.00580869
  -0.00116435]
 ...
 [ 0.00202676 -0.00495183  0.00140002 ...  0.00399387 -0.00215986
   0.00098652]
 [-0.00544714  0.00262353 -0.00186443 ...  0.00561092 -0.0026119
   0.0011937 ]
 [ 0.1484613  -0.16203257  0.06354681 ...  0.09861357 -0.10544156
   0.01725581]]
layers.1.weight: [[0.00023105 0.00026381 0.0003025  ... 0.00027953 0.000306   0.01749573]
 [0.00020476 0.00023658 0.0002743  ... 0.00025179 0.00027757 0.01687168]
 [0.00017474 0.00020509 0.00024152 ... 0.00021971 0.00024452 0.0161577 ]
 ...
 [0.00014931 0.00017895 0.00021474 ... 0.00019329 0.00021714 0.01556394]
 [0.00010305 0.00013076 0.00016424 ... 0.00014385 0.00016609 0.01447338]
 [0.00046527 0.00050658 0.00055394 ... 0.00052628 0.0005603  0.02323648]]
layers.2.weight: [[ 0.05684914  0.055657    0.05429295  0.05145788  0.07093787  0.06532492
   0.07252056  0.05597055  0.05398355  0.05906159  0.07165607  0.0535872
  -2.6899765   0.05295003 -3.329137    0.06758375  0.0569502   0.05352443
   0.07764871  0.07175344  0.06678229  0.05564544  0.0535131   0.05475083
   0.0703294   0.06325847  0.05657803  0.07074065  0.05328661  0.05927955
   0.05753483  0.058295   -7.480567    0.05350519  0.05512782  0.06819382
   0.05239826  0.06289124  0.05441677  0.05593313  0.05668781  0.05380343
   0.05405498  0.0554447   0.07275766  0.06597071  0.05839768  0.0588764
   0.07251689  0.01331673  0.05196757  0.05814129  0.05674416  0.06468819
   0.06922268  0.05261716  0.07067855  0.05335694  0.06682942  0.05554745
   0.04785435  0.07123545  0.06249731  0.05544629  0.05317009  0.07466345
   0.02508121  0.06195252  0.05603362  0.05475333  0.069043    0.05451141
   0.06436498  0.05419432  0.05412582  0.07071993  0.06076016  0.0542892
   0.06743879  0.05736701  0.06991165  0.06797293 -2.8888688   0.06692747
   0.05456101  0.06800992  0.05953024  0.04224924  0.05086086  0.05906186
   0.06669877  0.05361567  0.05585976  0.053172    0.05505174  0.04703672
   0.06809535  0.05456823  0.06394491  0.0548691   0.05382986  0.05979424
   0.05866458  0.06100013  0.05426505  0.07111458  0.05348471  0.06403991
   0.06921487  0.05265279  0.06931901 -0.01023588  0.05529956  0.06154461
   0.06116403  0.05533671  0.05494622  0.06031083  0.05244246  0.05427108
   0.07097036  0.06220559  0.04022602 -3.42895     0.06926401  0.05316019
   0.05106791  0.06781549]]

Final Loss: 0.1888
Distance Metric: 33.4642
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 1336

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.04752809 -0.02910485  0.06362563 ...  0.03436827 -0.07618177
  -0.03424298]
 [-0.00667323 -0.12380239  0.04464814 ...  0.17886288 -0.00059658
  -0.08664672]
 [-0.02252194  0.04665736 -0.02321418 ... -0.26678902  0.30993095
  -0.08046344]
 ...
 [-0.00700827 -0.07624386 -0.01359874 ... -0.00932936  0.17689987
   0.13122079]
 [-0.01285364 -0.07408039  0.07011221 ...  0.00272802 -0.14215602
   0.12402798]
 [ 0.34789675 -0.30483702  0.10705156 ... -0.07103211  0.07936548
  -0.01025602]]
layers.1.weight: [[-0.01107123 -0.04164216  0.00425662 ...  0.00514459  0.02812058
  -0.01063037]
 [-0.00112424  0.05172098 -0.01972694 ... -0.14524445  0.05096482
   0.03087468]
 [-0.04464836  0.03811386  0.09102967 ...  0.00672656 -0.00234713
   0.04232245]
 ...
 [-0.03598863 -0.03434036 -0.0069802  ... -0.03874286  0.01527692
   0.04819685]
 [-0.0101435  -0.01982964  0.02916569 ... -0.02378499 -0.03840816
   0.0007885 ]
 [-0.16417697  0.09023147 -0.05212288 ...  0.02151464  0.11817245
  -0.01292544]]
layers.2.weight: [[-0.26645124 -0.24013722  0.21276842  0.24855591  0.12106814 -0.32853773
   0.00966517 -0.32297766 -0.3306227   0.3605262  -0.25477943  0.36854067
   0.06709974 -0.24869832 -0.30265656 -0.46808252  0.40015784  0.17454989
   0.10641201 -0.21915133 -0.41680855 -0.43084273  0.3412315  -0.2216557
   0.16111498 -0.32698825 -0.47131172 -0.42152533  0.4649392   0.13714007
   0.09819566  0.47043774 -0.5028074   0.30687577  0.32372236  0.46877602
  -0.4097594   0.22309841 -0.5096584   0.11267989  0.06864929 -0.7065202
   0.30508178 -0.25700587  0.07142664 -0.60345286 -0.31673005  0.08699974
   0.44401777  0.33662847 -0.3161957   0.24889773  0.07501328  0.14235714
   0.19423619  0.1431941  -0.3028452  -0.37069008  0.40242422  0.10259991
  -0.38416973 -0.7140263  -0.62113506 -0.33720383  0.13277431  0.07170168
  -0.31123006  0.4010348   0.32728475  0.33817822  0.4103865   0.27728802
   0.15458943 -0.32733488  0.38400185 -0.3858039   0.4232104  -0.46807748
  -0.18314567  0.5485546   0.33059657  0.257245   -0.37927026 -0.32318944
  -0.58590645 -0.58430904  0.32113072 -0.37841612 -0.30561033  0.22808044
   0.2561267  -0.35662812  0.24427724 -0.41431913 -0.32886285 -0.49805441
  -0.4354258  -0.37773338  0.26960924  0.21831536  0.3632657  -0.45835906
  -0.46299723  0.11509122 -0.30290648 -0.32481757  0.36822096 -0.28005925
   0.08084679 -0.37290707 -0.20347479 -0.2865682  -0.32799655 -0.28666475
  -0.28224915 -0.3082188  -0.37698075 -0.41254315  0.33741254  0.15560354
  -0.25534937 -0.322778    0.13021508  0.17312694 -0.15441306 -0.31940666
   0.08803734  0.30827367]]

Final Loss: 0.1912
Distance Metric: 20.0186
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 2144

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-1.1260164e-03  8.0653524e-04 -2.1990987e-04 ...  3.6417670e-03
  -3.8103536e-03  1.1029565e-03]
 [-6.2079472e-03  6.2347660e-03 -1.8831939e-03 ...  7.2096935e-03
  -7.6252972e-03  2.1089558e-03]
 [ 7.0026801e-03 -6.3460204e-03  3.1133234e-03 ... -2.1910355e+00
   2.3942318e+00 -7.3560303e-01]
 ...
 [-1.3414579e-03  1.1439394e-03 -2.3001252e-04 ...  2.0764181e-03
  -2.3504396e-03  7.7430456e-04]
 [-4.6341978e-03  5.5785566e-03 -1.3478310e-03 ...  0.0000000e+00
   0.0000000e+00 -3.8651610e-04]
 [-8.0767571e-04  0.0000000e+00 -1.3578135e-04 ...  2.7247064e-03
  -3.3881057e-03  1.1579294e-03]]
layers.1.weight: [[ 0.          0.         -0.01494557 ...  0.          0.00023859
   0.        ]
 [-0.00014725  0.         -0.01414733 ...  0.          0.00017434
   0.        ]
 [ 0.          0.00012301 -0.0053972  ...  0.          0.00019044
   0.00011567]
 ...
 [ 0.          0.          0.00810844 ...  0.         -0.00016197
   0.        ]
 [ 0.00017723  0.00013854  0.00656428 ...  0.00016278 -0.00013503
   0.        ]
 [-0.00026347  0.         -0.0218444  ...  0.          0.00037291
   0.        ]]
layers.2.weight: [[ 3.32662538e-02  3.09703834e-02  1.17198657e-02  1.11135868e-02
   1.62118021e-02  7.56114498e-02 -3.35046314e-02 -2.63469536e-02
  -3.78274247e-02  1.10965630e-03  1.90540571e-02  3.23348418e-02
   6.26560971e-02 -2.07592715e-02  6.29765317e-02  1.84834898e-02
  -3.50414552e-02 -5.54552525e-02  4.42039371e-02 -1.81716494e-02
   8.43703970e-02 -3.65618318e-02  3.18882540e-02 -3.13253142e-02
   1.54970936e-03  9.98598337e-03 -1.42709473e-02  8.78403038e-02
   4.95089404e-02  2.14393549e-02  7.29002953e-02 -2.32947543e-02
  -4.01453711e-02 -2.41351016e-02  2.52094716e-02 -2.73270309e-02
  -2.27668695e-02 -3.90929319e-02  1.90348190e-03  3.88502181e-02
  -2.18506027e-02  2.76182070e-02 -2.49039140e-02 -4.58501093e-03
   5.59414513e-02  2.58821473e-02 -4.86815497e-02  9.80889238e-03
   2.06963234e-02 -3.09880860e-02 -3.92294787e-02  5.08759916e-02
   1.29819633e-02 -3.37938108e-02  2.36679018e-02  5.64120477e-03
   8.90578888e-03 -3.51229683e-02 -3.90497521e-02 -1.01756310e+00
   1.06256083e-02  3.97668965e-03  2.70187501e-02  3.21042128e-02
   2.32004710e-02 -4.05918993e-02  5.47374890e-04 -8.98217596e-03
  -7.07800873e-03 -3.00423838e-02  3.46480906e-02  6.81316946e-03
   3.91486920e-02  2.93245949e-02 -4.84551713e-02  8.18842649e-02
   9.99980688e-01 -2.08217204e-02  1.64095424e-02  2.64058653e-02
  -1.02806343e-02 -1.76221028e-03  6.01223409e-02 -1.64376907e-02
   6.23870920e-03 -3.78471687e-02  3.14846151e-02  9.18411184e-03
   4.58426215e-02  4.70637642e-02  5.82578368e-02  5.50567843e-02
  -2.68632285e-02 -7.52993952e-03 -4.26895395e-02 -3.00256535e-03
  -6.57971650e-02 -3.77383940e-02  2.19449326e-02  4.81447428e-02
   8.14162269e-02  1.12243257e-02 -5.99657232e-03 -4.22561634e-03
  -6.08352236e-02 -1.07059805e-02 -2.92997720e-04 -3.63295004e-02
  -3.52550484e-02 -5.26330881e-02 -3.86739895e-02  3.34599391e-02
  -2.29994301e-02 -1.65584832e-02  2.00453978e-02 -5.10881916e-02
  -2.98207384e-02 -3.00964210e-02  1.21229216e-02 -4.02930416e-02
   4.17567529e-02 -2.57751513e-02  3.30143124e-02  1.03351967e-02
   4.54109497e-02 -1.75865348e-02 -1.44848228e-02  4.80438024e-02]]

Final Loss: 0.0007
Distance Metric: 8.9172
L1 norm: 0
L2 norm: 1e-05
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 0.2
seed: 4
stopped after epoch: 621

================================================================================

