Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.04188275 -0.01551997  0.01300445 ...  0.13440403  0.00693104
   0.04789461]
 [-0.16541857  0.02732722  0.15978545 ...  0.18962461  0.17971373
  -0.07873288]
 [ 0.11018369  0.11024482  0.13044505 ... -0.0318143  -0.06489265
   0.01997304]
 ...
 [-0.02228531 -0.16796643  0.08510032 ... -0.10609841 -0.02633422
  -0.13703735]
 [-0.08937141  0.15852953  0.1709475  ...  0.02556537 -0.00293933
   0.03289799]
 [-0.00321942 -0.11798965  0.09604646 ...  0.10061803  0.06672711
  -0.14431067]]
layers.1.weight: [[-0.03872181 -0.04021938 -0.07242402 ... -0.02570332  0.07554619
  -0.11262006]
 [ 0.09768746 -0.1503659  -0.08865445 ... -0.04580234 -0.13152581
   0.05056024]
 [ 0.11983357  0.08774518 -0.09607229 ... -0.1427883   0.07155225
  -0.02766302]
 ...
 [ 0.01538765 -0.0213146   0.0634601  ... -0.00624763 -0.08002695
  -0.14759776]
 [-0.00576414  0.06781968 -0.10035029 ... -0.08101053 -0.05609176
   0.1003195 ]
 [ 0.11181696  0.02714967 -0.03860513 ...  0.01688447 -0.02119899
   0.13781732]]
layers.2.weight: [[-0.33059928 -0.00731677 -0.09974022  0.18384181  0.23372515 -0.04415378
  -0.14562069 -0.11563332  0.06359592 -0.07499079 -0.19134763 -0.10955644
  -0.12487181 -0.20559704 -0.08179731 -0.26000527 -0.09711649 -0.1812148
  -0.24717975  0.15826495  0.02152828  0.03051443  0.21377163 -0.10547271
   0.27549568  0.23552817  0.2641483   0.11584336 -0.08968816  0.19395803
   0.0807142   0.0165449  -0.0645674   0.11688051 -0.14561403  0.00407519
  -0.1429913   0.0011419  -0.09972092 -0.08943773 -0.16994499  0.33321935
   0.05688405 -0.1019019   0.08367889  0.15764213 -0.20972048 -0.11741186
   0.04150884  0.00765352  0.18658948  0.2781892   0.2584815   0.02694981
  -0.07804437  0.11162603  0.09254228  0.04758631  0.18471679  0.0126828
  -0.23754987 -0.15105224  0.17518687  0.15907596 -0.08263821  0.00123058
   0.1294215   0.18640012 -0.19509508  0.02200208  0.04516232 -0.15762103
   0.05237761 -0.05746955 -0.09811705 -0.01775141 -0.13105333  0.17326234
  -0.08348646  0.20839195  0.23925243 -0.28458726 -0.02956277 -0.01430948
  -0.03584503 -0.17500164 -0.14335562  0.28250223 -0.1509566   0.1527063
   0.04621061  0.06910425 -0.20429492  0.22668314  0.12424749 -0.17535576
  -0.20628539 -0.00182076 -0.09602982 -0.26801383  0.3122925   0.22207005
   0.12803358 -0.22105639  0.05992038 -0.29883838 -0.18066725  0.06396008
  -0.14992185  0.18475771 -0.07358246 -0.20613721  0.05653458  0.1283352
  -0.13625541 -0.03593257  0.21828341 -0.08416673 -0.07443749  0.25517496
   0.06069266  0.24903117  0.0764263  -0.00039333  0.05109211 -0.28255922
  -0.14026682 -0.14452325]]

Final Loss: 0.0000
Distance Metric: 23.6618
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 25210

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.00470805 -0.05473269  0.10440262 ...  0.07949144 -0.15369743
  -0.10159819]
 [-0.29743338  0.05616945 -0.00539096 ... -0.2563969   0.3385708
  -0.03052813]
 [-0.03872849 -0.03209522 -0.20446087 ...  0.00890139  0.02597461
   0.0523573 ]
 ...
 [-0.00531907 -0.09262127  0.10319324 ...  0.18697663  0.05326655
   0.23356916]
 [ 0.0173501   0.16031708 -0.0024044  ...  0.2434399   0.14544342
   0.12009788]
 [ 0.04651717  0.18781307  0.09055708 ... -0.18289436  0.0168471
  -0.04074133]]
layers.1.weight: [[-0.05279021 -0.05088796  0.1752107  ... -0.06081565  0.27305308
  -0.1216289 ]
 [ 0.17270456 -0.16987099 -0.11640335 ...  0.21160425  0.0920236
   0.03570981]
 [-0.16426823 -0.153981   -0.1600573  ... -0.07995728 -0.0354991
  -0.03393148]
 ...
 [ 0.16342928  0.19451068  0.22678453 ... -0.00359277 -0.02497182
  -0.01272617]
 [-0.08760538  0.08356407 -0.23670037 ...  0.17381306 -0.01788932
   0.20982595]
 [-0.09401537  0.06280033 -0.07284354 ... -0.00629546 -0.09445056
   0.02890521]]
layers.2.weight: [[-1.4713246   1.9960699  -0.03713438  2.9777122   0.83183074  0.09351014
   1.841458    1.4895543  -1.5960104  -0.38325763  0.39503568 -1.3390218
  -0.64881736 -0.884913   -1.2950299   1.3166656  -1.0924712   1.4219277
  -0.68777436 -0.4674982   1.0952531   0.71273077 -0.9371345   0.06444836
  -0.89911366 -0.9522926  -0.14380187  1.7126119   1.989588   -1.2004191
  -1.742536   -2.6118479  -0.77723783  0.87435025  1.2213318  -2.1478338
  -0.5199223   1.6412632  -1.813847   -0.06038969  0.00910521 -0.05125478
   0.23594983 -0.527825   -0.1358242  -2.3187907  -0.02866713 -2.9818375
  -0.75505793 -1.8494061   1.8759525  -3.1571126   2.05969     0.75650316
  -0.6758137   2.2103796  -0.3939875  -0.22223154  2.0009708   1.5493151
  -0.13924614  1.214581    1.5331895  -0.15185697 -0.2390731  -0.30071086
  -1.3475229  -0.12610264 -1.1602666   0.73168546 -0.7820867   0.38873044
   0.07445435  0.29391637 -0.61400497 -1.9082799  -1.779772   -0.07647048
  -1.2700742   0.9435902  -1.5845053  -2.1322029  -1.1212056   0.04424977
  -0.02783104  1.093713   -0.4921701   1.0716368   1.1646518  -1.3561022
   1.8390864   0.62018925  0.6475903   1.7432071  -0.5399488  -0.9957262
  -0.28583765 -0.22349827  3.2024794  -2.5937865  -1.3364608   1.5430664
  -0.41762674  0.7493349   0.06254884  0.5675767   0.50550956 -1.6447908
  -2.4933538   0.54097265  1.6436794  -1.6020207   0.3409444  -1.0153043
  -1.708533   -2.57695    -0.8808738  -0.55379695  0.44429207 -0.05664411
  -0.04585852  2.3709733   1.2174864   0.08184531  0.7508348  -1.6938647
   0.88518846  3.6688867 ]]

Final Loss: 0.2549
Distance Metric: 41.4478
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 193

================================================================================

