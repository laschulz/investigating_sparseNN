Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.11090421 -0.01296601  0.1715945  ... -0.07239016 -0.08490682
   0.08718454]
 [-0.11236301  0.04364109  0.13435708 ...  0.05213488  0.0157578
   0.10831795]
 [-0.0493792  -0.08629779  0.00789606 ...  0.10682818 -0.06504744
   0.15731038]
 ...
 [ 0.1331327   0.13261965 -0.16089417 ... -0.07003401 -0.08001494
  -0.09851553]
 [-0.13538624 -0.20163088  0.13485003 ...  0.09501719  0.03255723
   0.18310201]
 [ 0.2050975  -0.04313407  0.13988557 ... -0.10057165 -0.05328923
   0.08403428]]
layers.1.weight: [[-0.10628015 -0.08891685  0.01758862 ...  0.12044744 -0.08309876
  -0.06832144]
 [-0.0285569   0.06069671 -0.0849233  ... -0.06122673  0.10482077
  -0.05314655]
 [-0.0798354   0.13158314  0.13569534 ... -0.0294996   0.04449065
  -0.02862479]
 ...
 [-0.01063687 -0.08151902 -0.15044534 ... -0.15543644  0.0493059
   0.10314922]
 [-0.06714693  0.05664114 -0.14579792 ...  0.10802247  0.0795899
  -0.01092507]
 [-0.04707929  0.03550743 -0.06925655 ...  0.14112133 -0.1341948
   0.12729718]]
layers.2.weight: [[ 0.17581965  0.22556606  0.20517099  0.43326038  0.16131018 -0.0856441
  -0.1217947  -0.06574605 -0.00201358 -0.21147245  0.17650731 -0.03580581
   0.09042741 -0.11041208 -0.11497549  0.08399811  0.08646847 -0.00769383
  -0.08538904  0.06757767 -0.07793207 -0.14303187 -0.01498841 -0.1040979
   0.04245329 -0.01044921 -0.08379471 -0.04808142  0.2325573   0.0872778
   0.04662554 -0.1522701   0.03227527  0.15566993 -0.30297136 -0.17992586
  -0.05102134 -0.21728     0.14161025 -0.44314212  0.05754574  0.22726698
   0.02888021  0.34503573 -0.00784221  0.11600014  0.00842637  0.21950975
  -0.08643309  0.2937716  -0.13422197 -0.11117151  0.09891149  0.15428479
  -0.00054643 -0.1983519   0.15826426 -0.1721059   0.13961302 -0.01085601
  -0.1999989  -0.17326303 -0.09853266 -0.1978343  -0.04028253 -0.11237777
   0.03746907  0.2652403   0.18866165 -0.15657336 -0.14863132  0.03360965
  -0.00621534  0.20229961  0.11859079  0.10078676 -0.06191703  0.20603041
  -0.16856176  0.18019715  0.13974915  0.01721587  0.03896877  0.10415035
   0.20035474 -0.24887651 -0.18770191  0.03378277 -0.07384993 -0.18786718
  -0.15282731 -0.15104802 -0.08661792 -0.10651606 -0.0839981   0.04727307
   0.26636395 -0.11698534 -0.11184556 -0.02370157 -0.09814606 -0.06153096
   0.15010495  0.17008561 -0.04392888 -0.00578037 -0.17806949 -0.15236005
   0.19481646  0.17511575 -0.1302478   0.03005799 -0.2798302   0.07579555
   0.00763181 -0.10266415  0.2521702  -0.07307431  0.04871181  0.19781522
   0.02098828 -0.0408134  -0.15761296  0.17377348 -0.04618696  0.17016262
   0.02637767 -0.08844552]]

Final Loss: 0.0000
Distance Metric: 23.1080
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 15296

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.0502501  -0.09951004 -0.09103363 ... -0.11446728  0.09606765
  -0.17547502]
 [-0.14055227  0.08316551  0.06853329 ... -0.06471258 -0.09559765
  -0.03138344]
 [-0.14948517  0.29743207 -0.08946    ...  0.07224586  0.03880658
  -0.07815336]
 ...
 [-0.04446092 -0.11736259 -0.02099459 ...  0.0689228   0.00151798
  -0.04491535]
 [-0.01246406  0.20594297  0.06985501 ... -0.07503758 -0.0232909
   0.13691579]
 [-0.15212396  0.22539476 -0.06458928 ... -0.07841352  0.12980156
  -0.15609254]]
layers.1.weight: [[-0.12709181  0.0517916   0.05395733 ... -0.0691476   0.03915018
  -0.14795077]
 [ 0.16596809 -0.05682604 -0.06238732 ... -0.01705682 -0.11679067
  -0.22049321]
 [ 0.10546793  0.11054326 -0.05846684 ...  0.01949085 -0.11669961
   0.24009003]
 ...
 [ 0.0462608  -0.06805489  0.03843046 ... -0.00947896  0.21533063
   0.06331287]
 [ 0.24766925 -0.1719207  -0.00321206 ...  0.07955811  0.04554853
   0.08658062]
 [ 0.09891831 -0.07893433 -0.06472836 ...  0.18927555 -0.0457701
   0.25935486]]
layers.2.weight: [[-1.3405633   1.8034168  -1.9212325  -0.7528247   2.8587556  -1.3530003
  -1.3650259  -0.07668743  1.3921562   2.0894072   1.184526    1.4779112
  -0.09679303 -0.5921859  -0.02959364  1.2575226   0.18513466  1.6452132
   0.9415588  -1.2387515   0.10820325 -2.8792877   1.9497708  -0.47906557
   0.11672824 -1.1961644  -0.3056712  -1.3055824   0.18493716 -0.6119404
   0.6589001   2.4362507  -4.5719957   0.13626921 -1.7491726   1.409386
   0.8912894  -0.1694582   0.55914086  0.61770093  0.56988716  0.98792166
  -0.655119   -0.03140107  0.39771318 -0.0356434   1.7729431  -1.3543336
   2.4452178   2.499979    0.83041555 -0.22283462  1.150272   -2.4841971
  -1.1968361  -1.9855312  -0.26019925  0.29699722 -0.3639363   0.5263423
   1.2843106   0.67350197 -0.55496716 -0.9647436  -1.4270079   0.29809913
   0.9650064  -1.5816182  -2.6398628   0.50110626 -1.8166546  -0.20338951
  -1.2748398  -0.9061805   0.7280353  -0.4108084  -0.14515632 -0.40240607
   1.0937049   2.088161   -1.8742048  -2.7598      1.2311883   0.74509895
   2.7093968  -1.5444783  -1.1454309   0.2400511   1.9267657  -1.1127558
  -2.247029   -1.78614    -0.48827627  0.811275   -4.022536    0.42830124
  -1.0679326  -3.5936573  -2.3078904  -2.2870245  -1.2740202  -0.8493581
  -2.4984484  -1.8991501   0.7903578  -1.2637357   0.5706069  -0.6507706
   0.6161514  -0.8460049   2.982385   -0.48995188  1.2739867   1.0382205
   1.8894243  -0.31824538  1.2299516   0.4927683   0.51668006  1.0441222
  -0.06212111 -0.15685679 -0.18999171  1.2980152  -0.9900943  -0.24146557
  -2.1642282   0.19944376]]

Final Loss: 0.2550
Distance Metric: 41.6657
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 222

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 1.0934403e+00  5.0008518e-01 -1.0276548e+00 ... -7.8676695e-01
   3.7467679e-01 -2.0492792e+00]
 [-1.6196460e-01  1.9344155e-01 -7.7305073e-01 ...  2.7791338e+00
   2.1749417e-01 -1.2019812e+00]
 [ 3.6226313e+00 -3.7340529e+00  2.2062416e+00 ...  5.6933150e+00
  -3.3648217e-01 -2.6310928e+00]
 ...
 [-2.0775919e+00  1.6591488e+00  1.2519150e+00 ...  7.1907616e-01
  -1.1231668e+00 -8.9179534e-01]
 [-1.6668636e+00  3.2632644e+00  8.5558563e-02 ...  1.5716872e+00
   2.4733572e+00 -3.2658670e+00]
 [ 1.8156241e-01  2.5948138e+00 -3.0095375e-01 ...  1.9672820e-03
  -5.3211957e-02  9.5694822e-01]]
layers.1.weight: [[ 0.00252056 -0.00163559  0.00165703 ... -0.00161736  0.
   0.0035017 ]
 [ 0.00278579  0.00423029 -0.00052968 ...  0.00363609  0.00297696
  -0.00563471]
 [-0.00051828  0.00204217  0.00208969 ... -0.00380075 -0.00366197
   0.00425722]
 ...
 [-0.00553995  0.00509633 -0.00026045 ... -0.00652351 -0.00293035
   0.0040558 ]
 [ 0.00874663 -0.00649844  0.00557234 ...  0.00307123 -0.00530594
  -0.00746622]
 [-0.02133962 -0.01837592  0.01393425 ... -0.00217156  0.0086798
   0.00571958]]
layers.2.weight: [[ 3.35180201e-03 -1.68805756e-03 -7.72606290e-04  2.60593486e-03
   7.70948408e-03  4.92902519e-03  4.94279573e-03  1.09921284e-02
   1.95271045e-03 -7.29317428e-04  4.98934137e-03  1.25982799e-04
   1.62014365e-02 -2.19046194e-02  6.60228450e-03 -9.53786820e-03
   7.41837081e-04 -1.76806690e-03 -1.66294016e-02 -8.38277582e-03
   9.82345734e-03 -6.31551305e-03  1.10767754e-02 -1.43782282e-02
  -1.19435057e-01 -6.41983002e-03  1.04182272e-03 -5.25969826e-03
   1.06591573e-02  1.93971989e-03  3.56278033e-03  1.27987340e-02
   2.43657129e-03 -8.57000127e-02  3.50541342e-03  1.22038601e-02
  -3.59140709e-03 -1.22434869e-02 -5.66754397e-03  1.63370054e-02
   0.00000000e+00  4.03531501e-03  1.40200645e-01 -3.01442039e-03
   9.15082754e-04  2.79527437e-03  1.46766286e-02 -1.34375342e-03
   8.79806280e-03  1.97664555e-03 -1.55397662e-04 -8.63197143e-04
  -5.08027617e-03 -1.41826086e-02 -6.38034893e-03  7.51568750e-03
  -1.27902655e-02  2.84857093e-03  3.89280100e-03  5.61358361e-03
   5.62224246e-04 -1.14907103e-03  2.27000378e-02 -3.94080533e-03
  -2.15124036e-03  4.29334445e-03  5.74093137e-04 -1.75769627e-03
  -5.56419278e-03 -2.53587123e-03 -4.84940521e-02  1.40916614e-03
  -9.66663647e-04 -1.23176547e-02 -1.08236320e-01 -1.49420664e-01
   8.59546065e-02  1.14620961e-01 -9.62368213e-03  3.79701331e-03
   8.64474941e-03  5.48597332e-03 -1.20128728e-02  3.80496983e-03
   8.07437487e-03 -3.38631473e-03  4.56351694e-03 -7.07315132e-02
  -1.28251023e-03 -1.61533784e-02 -1.23694902e-02  5.38434880e-03
  -9.34407786e-02  6.60496950e-03 -7.82073848e-03 -2.13301042e-03
  -1.02567896e-02  6.80204527e-03  1.11758992e-01 -5.22031402e-03
   5.17225591e-03  1.70113090e-02 -1.83516741e-02 -3.21045332e-03
  -7.68134370e-03  6.85907807e-03  2.54865782e-03  1.90168712e-02
   2.32091034e-03  2.13402347e-03  4.90640895e-03  5.38366288e-02
   9.70706460e-04 -8.29345733e-03  7.30391499e-03  1.11919316e-02
   2.74287397e-03  9.38051753e-03  5.79748815e-03  6.09119423e-04
  -1.02766221e-02 -2.74000526e-03  5.43421600e-03 -6.62696548e-03
   1.53661461e-03 -2.65748217e-03 -5.43829228e-04 -3.03338864e-04]]

Final Loss: 0.2316
Distance Metric: 150.3274
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 5488

================================================================================

