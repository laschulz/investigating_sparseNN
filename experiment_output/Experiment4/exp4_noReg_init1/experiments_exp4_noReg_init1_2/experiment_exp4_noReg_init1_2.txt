Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.67729485 -0.16417599  0.7415323 ]]]
layers.1.weight: [[[-1.0064725  -0.85622764]]]
layers.2.weight: [[[-0.82314664  0.6537249 ]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.0285924  -0.00542111 -0.04416099 ... -0.01869443 -0.17473426
  -0.12926528]
 [ 0.16814421 -0.07218292  0.1360866  ...  0.08293058  0.06314814
  -0.0835011 ]
 [ 0.1490662   0.19864693 -0.22003998 ... -0.01369312 -0.02209477
  -0.13871953]
 ...
 [ 0.06168729 -0.06390355  0.04648077 ... -0.0230529   0.01882116
   0.18989253]
 [-0.02963907  0.10237209  0.1284416  ... -0.0645377  -0.00854781
  -0.0048609 ]
 [ 0.02187141 -0.07641958  0.1214134  ... -0.0738892  -0.19548458
   0.10702317]]
layers.1.weight: [[ 0.11711463  0.11462086  0.13240112 ... -0.04472078 -0.06927884
   0.00724199]
 [-0.10072198 -0.04676708  0.07469345 ... -0.02969854 -0.06718053
   0.03769545]
 [ 0.10830369  0.13502333 -0.07925613 ... -0.00634056  0.0842468
   0.11385383]
 ...
 [-0.10180497 -0.03105518 -0.14963508 ... -0.03997738  0.02200977
   0.04888428]
 [-0.12464599 -0.07458813 -0.04445557 ... -0.12303605  0.02975987
  -0.00503523]
 [-0.1275909   0.02985841  0.14299396 ...  0.1218529   0.10243452
   0.08836539]]
layers.2.weight: [[ 0.08817235 -0.16242121 -0.23808835 -0.14238265 -0.10994068 -0.19062568
   0.16407683 -0.16165598  0.00877941 -0.16681841 -0.01799465  0.17115939
   0.26128724 -0.19145907 -0.08657546 -0.0340243  -0.13385543  0.18110168
  -0.05931095  0.08563394 -0.05653254 -0.12855563  0.12920427 -0.11570533
  -0.22484887  0.05884206  0.03105182  0.25640664 -0.09170747 -0.02622477
   0.25313494 -0.1682776  -0.06264266 -0.02781254 -0.23717545  0.17440006
   0.07672009  0.05788974  0.08887292 -0.04275195 -0.21613076  0.01266369
  -0.09856696  0.094168    0.00882369  0.04419105 -0.1199513  -0.08804502
  -0.17405434 -0.22731858  0.12727958 -0.10508339  0.1094755  -0.17223498
  -0.2050543  -0.08299347 -0.03522886  0.00170869  0.18971223  0.03194171
  -0.08145586  0.15350601  0.21828617  0.03232662  0.00623228  0.10152894
   0.07752941 -0.03908696  0.04859858 -0.02559892 -0.22745138  0.22548209
  -0.10071218 -0.14817579 -0.0386304   0.19705904  0.01767458  0.05924995
  -0.15599558  0.18459453 -0.11508873 -0.04208511  0.18911017 -0.08300962
  -0.20403974  0.03600061 -0.06301506  0.21155028  0.06136698 -0.12349615
  -0.02849058  0.06106831  0.05792781  0.27295774 -0.22747654 -0.24091083
  -0.23351547 -0.0283115   0.17620163  0.04515261 -0.2054588   0.10809016
   0.2641694   0.21253514  0.15412699 -0.03958228 -0.0647378   0.20939712
  -0.02073839  0.06366325  0.11837742 -0.19731805 -0.11333124  0.07244157
  -0.24785344  0.10601585  0.23062149  0.19018021  0.08870929  0.04407379
   0.17101164 -0.02250328  0.01405956  0.12273712 -0.20145777  0.00344798
   0.14498636 -0.1168184 ]]

Final Loss: 0.0000
Distance Metric: 19.6907
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.04234873 -0.11957029 -0.13201629 ... -0.05461527  0.16293348
   0.22067817]
 [ 0.06641734 -0.01939372 -0.08369152 ... -0.01756795  0.04293387
   0.18005216]
 [-0.09165028 -0.2588789   0.03814444 ... -0.00740048 -0.14764106
   0.11590473]
 ...
 [-0.23399892  0.04990022 -0.1235816  ... -0.12656246 -0.02530621
  -0.06621832]
 [-0.07591746 -0.0307416   0.21214928 ...  0.3978957  -0.03514948
  -0.4573914 ]
 [ 0.04857058  0.15322912  0.17509852 ... -0.04665691 -0.11409641
   0.0165813 ]]
layers.1.weight: [[-0.11522194 -0.03916509  0.0515075  ...  0.08137869  0.06352606
  -0.00363806]
 [ 0.18561955  0.1330422  -0.05659082 ...  0.08980852  0.04820409
  -0.02285201]
 [-0.22468625  0.1350135  -0.14937185 ...  0.04139566  0.03953248
   0.00355361]
 ...
 [-0.21290444 -0.04310882 -0.29676497 ...  0.02861818 -0.07951225
   0.15127203]
 [-0.02338662 -0.15618987 -0.05995899 ... -0.08980827 -0.12656268
   0.10206665]
 [-0.15826696  0.16281894  0.0632469  ... -0.2408932   0.1351349
  -0.13062912]]
layers.2.weight: [[ 5.6832276e-02  5.1040572e-01  2.6450036e+00  8.4323442e-01
   4.5854270e-01 -1.8034799e-01  3.9951265e-01  2.3492083e-01
   1.6828132e+00  7.5649637e-01 -2.4716227e+00  5.9382254e-01
  -4.9199954e-01 -1.9542597e-02  1.4322938e+00  1.6297526e+00
   1.0518768e+00  4.6693715e-01 -5.0268185e-01  2.5237098e-01
  -7.3404819e-02 -4.9137896e-01 -9.6103901e-01  1.2896345e+00
  -1.2238697e+00  1.4365466e+00 -5.4040804e+00  1.0382549e+00
  -7.1797550e-02  6.3320655e-01 -8.5837233e-01  1.2597586e+00
   1.5322093e+00 -2.1379566e+00  2.2434905e-01 -1.7062545e+00
  -1.7492088e+00  1.8775438e+00  1.6979855e+00  1.0758567e+00
   1.9566082e+00 -2.1685443e+00  1.9501864e+00 -7.7569366e-01
   1.3441578e+00 -1.8691774e-01 -1.3251534e-01 -8.3025920e-01
  -1.8056469e-01 -2.3852587e+00 -4.6453605e+00 -1.2948403e+00
   6.1196242e-02 -9.5498675e-01 -8.5354760e-02  7.7338713e-01
   6.4465153e-01 -1.7973877e-01 -9.5422369e-01  4.1091225e-01
  -1.2307013e+00 -3.6987144e-01 -1.2797582e-01 -8.6782914e-01
   1.0152655e+00  7.3989943e-02  3.3344632e-01  6.7357349e-01
  -4.6128464e-01 -1.7091999e+00  4.0532770e+00 -8.0376911e-01
   1.1261421e+00  2.2867658e+00 -3.2467213e+00  4.6363205e-02
   1.8671145e+00  2.0950298e+00  3.2709239e+00 -3.5039210e+00
   3.5850604e+00 -1.7252415e-02 -1.3315109e+00  1.0189503e+00
  -2.0801535e+00  1.2143208e+00 -8.2027930e-01 -1.7675735e+00
   1.2108413e+00 -1.5865034e+00  6.2963706e-01 -3.1242762e+00
  -6.3950330e-01  3.1071231e-03  1.2894163e+00 -1.5099691e+00
  -5.5280644e-01  2.9463978e+00  9.6356019e-02  3.7501624e-01
  -1.7474067e-01 -9.5586187e-01  2.1853538e+00 -4.1601437e-01
   4.4404724e-01  7.1644783e-01  2.0945628e+00 -1.2401146e+00
   1.5943083e+00 -1.3066783e+00  1.2306162e+00 -8.0431730e-01
   1.6177686e+00  2.6799372e-01  4.2604795e-01 -1.3969285e+00
   2.8274959e-01  8.6612797e-01  9.8984712e-01  1.3654542e+00
   9.8548084e-01  9.9042201e-01 -1.6811303e+00 -9.0185899e-01
   3.1735617e-01  8.3184004e-01  1.4762081e+00 -8.9709574e-01]]

Final Loss: 0.3264
Distance Metric: 41.8233
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 217

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.03779706  0.0196047   0.1121609  ...  0.01694042  0.04434932
   0.06711511]
 [ 0.0667646   1.3931613   0.13057236 ...  0.10790095 -1.5658826
   0.48159876]
 [-0.10309218  0.02984086  0.10696017 ...  0.06061871 -0.00818965
   0.04688566]
 ...
 [-0.41553417  1.227433    0.78420293 ... -0.21439418  0.34539217
   1.5513088 ]
 [-0.2280862  -0.26005363  0.22562487 ...  0.2455577  -0.8819813
   0.01821076]
 [-0.4850736   0.00869539  0.6346526  ... -0.21869585 -1.6142435
   0.8387146 ]]
layers.1.weight: [[-0.05831631  0.02179099  0.06757586 ...  0.03089239  0.01439824
   0.00436554]
 [-0.13210426 -0.00281917  0.11280394 ...  0.01161922 -0.01143777
   0.0206315 ]
 [-0.06165165 -0.02253946  0.10675584 ... -0.00423037 -0.00739654
   0.00515088]
 ...
 [ 0.03078265  0.0272785   0.095156   ... -0.00695872 -0.02679363
  -0.000802  ]
 [-0.00208921  0.00413436 -0.07209034 ... -0.01733319  0.00739747
  -0.01055009]
 [ 0.11368218  0.00096482 -0.02475026 ... -0.00359854  0.02461975
   0.00751427]]
layers.2.weight: [[-1.55291371e-02 -1.56043505e-04 -2.88533769e-03  6.23877160e-04
  -5.62959304e-03  4.71618678e-03  1.37861967e-02 -2.53755390e-03
   5.36110811e-03  3.57996090e-03  9.68169793e-03 -3.76428012e-03
   0.00000000e+00 -1.98002183e-03  1.40147540e-03  1.54103469e-02
  -2.88224034e-03  1.05714072e-02 -4.18132870e-03  6.29171124e-03
  -2.74590589e-03  1.00094033e-02 -1.22920275e-02 -1.79075124e-03
  -1.85751589e-03 -1.65837351e-02 -8.38016905e-03 -1.60489767e-03
   6.57113642e-03 -7.79785495e-03 -5.14751067e-03 -2.15457641e-02
   3.02710733e-03  2.12506414e-03  7.88990036e-03  2.01282068e-03
  -6.38643093e-03 -5.73963439e-03  1.30466092e-03 -1.08248810e-03
   5.60486224e-03 -1.65983860e-03  0.00000000e+00  1.02534471e-02
  -5.64611843e-03 -3.39436647e-03  1.29284179e-02 -3.18631972e-03
  -1.92887053e-01  5.94381988e-03  3.59147275e-03 -8.63840047e-04
   7.37089431e-03 -1.28226727e-02  1.12300916e-02 -1.03317453e-02
   2.32059578e-03 -7.09529733e-03  4.59560251e-04  6.00976078e-03
  -7.89954211e-04 -2.45531113e-03 -7.84611329e-03 -3.88908404e-04
   4.24877647e-03  4.80928645e-03 -5.61617780e-03  0.00000000e+00
   1.13438554e-02  4.78310452e-04  1.93934585e-03  1.02851382e-02
   5.52132318e-04 -1.73291247e-02 -4.64292709e-04  5.75905200e-03
  -4.10832278e-03 -1.98635971e-03 -5.95595641e-03 -3.81726213e-03
   9.17199813e-03 -1.54033834e-02 -4.95537475e-04 -4.25841752e-03
  -1.16346776e-02 -5.93788456e-03 -8.85538943e-03 -9.19196755e-04
  -2.65457435e-03  5.33706089e-03  1.20238476e-02 -1.04029169e-02
  -1.09397294e-02  7.10002903e-04  2.11111736e-03  2.03240290e-01
   1.97647815e-03  7.64292199e-04  6.12163171e-03  1.06028086e-02
   1.56949437e-03  5.67490328e-03 -1.26562705e-02 -3.41098849e-03
  -5.30973030e-03 -4.65307012e-03  2.21016142e-03  1.67183310e-03
   9.58083605e-04 -6.70673791e-03 -7.12027634e-03 -1.28318407e-02
   7.81909842e-03  1.13213444e-02  1.92877604e-03  1.40388217e-03
   1.55478204e-03  2.46427348e-03 -4.11183370e-04 -2.08922895e-03
   1.26733743e-02 -2.86285835e-03  1.26002866e-04 -5.01790456e-03
   2.49595963e-03 -7.61436345e-03  1.14918279e-03 -6.17452944e-03]]

Final Loss: 0.2928
Distance Metric: 66.7424
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.03561274 -0.09818684 -0.09889408 ... -0.20555767  0.18229258
   0.06897901]
 [-0.16957891 -0.14884168  0.16443068 ... -0.12130567 -0.1232055
  -0.17464668]
 [-0.16378547  0.02361224  0.08770795 ...  0.06252778  0.15576538
   0.03541443]
 ...
 [ 0.07542304 -0.16793153  0.18624365 ... -0.19596267 -0.09355243
   0.03064834]
 [ 0.05953002  0.08727115  0.06058035 ... -0.05148328 -0.02126345
  -0.10867847]
 [ 0.18512313  0.16745038 -0.10247341 ...  0.07586583 -0.02050255
   0.06230377]]
layers.1.weight: [[ 0.06438523 -0.11430628 -0.02447662 ... -0.03832491 -0.04371366
   0.10821448]
 [ 0.13122223 -0.05900302  0.02372287 ... -0.14215411  0.07832949
  -0.1037823 ]
 [ 0.13005397 -0.01057748  0.0231205  ... -0.01038146 -0.07410916
  -0.119932  ]
 ...
 [-0.06004944  0.00159602  0.06755821 ... -0.0415203  -0.08894742
  -0.10745466]
 [ 0.04278865  0.05843794 -0.11585532 ...  0.10302351  0.10213709
   0.00499147]
 [-0.12329616 -0.09062132  0.02820388 ... -0.02028469  0.00175393
  -0.07832847]]
layers.2.weight: [[-0.2327464   0.01791869 -0.29530787  0.01966164 -0.13746959 -0.25
  -0.19589296 -0.13209555 -0.11156572  0.08739372 -0.19612414 -0.3192819
  -0.18948409 -0.19902663 -0.10499906  0.07544427 -0.25        0.04468806
  -0.24026172 -0.0625     -0.16634187 -0.07693418 -0.21621731 -0.02002591
   0.00060426 -0.0625     -0.05409582 -0.02668187 -0.09158012 -0.1004711
  -0.149427   -0.27173147 -0.07054053 -0.186472   -0.05204841 -0.1571878
   0.13014421  0.09791329  0.02520191 -0.31158257 -0.00278454  0.12952547
  -0.1531812  -0.18484673 -0.26139176 -0.35130876 -0.2039952  -0.2042903
  -0.0625     -0.11526924 -0.125      -0.01041182 -0.06127261 -0.07704224
  -0.01874673 -0.11848065 -0.0294407   0.02681797 -0.21425003 -0.0625
  -0.03045453  0.0675834   0.11524331 -0.25592786 -0.22893107 -0.05501219
  -0.3527647  -0.24205457 -0.2197727  -0.28332224 -0.00159771  0.08311219
  -0.10680271 -0.13600978  0.12870008 -0.08225359 -0.17628157 -0.125
  -0.02735539 -0.28634623  0.00928248 -0.14872612  0.03937573 -0.09557318
  -0.3352846  -0.17550404 -0.09588786 -0.26043633  0.06596024 -0.08326636
  -0.16225621 -0.1118209  -0.15399894  0.03711207 -0.21439242 -0.19480401
  -0.29234198 -0.21453188 -0.02864685 -0.18229356 -0.03125    -0.2927405
  -0.03125    -0.0625     -0.16472928 -0.1442864  -0.09950399 -0.25
  -0.13472463 -0.07429944 -0.10696377 -0.30099416 -0.25137773 -0.01518636
  -0.0625     -0.2398365  -0.24780178 -0.2854595   0.08362291 -0.19289564
  -0.20446484 -0.27958593 -0.0625     -0.11961399 -0.03614349 -0.1222214
   0.00548668 -0.07215266]]

Final Loss: 0.0000
Distance Metric: 22.0799
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.29875556 -0.26092258  0.22404538 ... -0.13898285  0.02815995
   0.19784297]
 [ 0.09910034  0.14355879 -0.12503847 ... -0.0008809  -0.05088209
   0.12940434]
 [ 0.06289042 -0.2716908  -0.08500834 ... -0.1494001   0.2322644
   0.07203939]
 ...
 [-0.43355003  0.21964012 -0.06703001 ...  0.01066935 -0.002519
  -0.22580133]
 [-0.5160984  -0.06829812 -0.21243359 ...  0.20319244 -0.28577685
  -0.16301431]
 [ 0.22604695 -0.09804353 -0.06936345 ... -0.26048487 -0.00515619
  -0.12809798]]
layers.1.weight: [[-7.85986558e-02  6.17407747e-02 -3.22095826e-02 ...  7.56010562e-02
   1.60113335e-01 -2.98005909e-01]
 [-3.40434283e-01 -1.30042478e-01  7.37743601e-02 ... -4.08636391e-01
  -4.66818035e-01 -1.13730766e-02]
 [-2.70467192e-01  1.36536971e-01 -3.40810120e-02 ...  3.02516408e-02
   3.20516154e-02  1.00002579e-01]
 ...
 [ 5.92613705e-02 -1.34043038e-01 -8.91067088e-02 ...  1.94145322e-01
   6.16441295e-02  1.02938876e-01]
 [-3.30860503e-02  1.78190649e-01  5.16341403e-02 ... -1.29414961e-01
  -7.21259341e-02 -2.83794049e-02]
 [-5.25509156e-02 -2.29322743e-02 -8.85090977e-02 ...  6.31815791e-02
   4.17705101e-04  1.55249208e-01]]
layers.2.weight: [[-8.0843347e-01  2.7736433e+00 -2.8335166e-01  3.0589926e+00
   5.5458975e-01 -4.0788624e-01 -9.5619476e-01  9.9860185e-01
   2.2180151e-02 -2.7549770e-02  4.7748250e-01 -1.2853332e-01
   1.1538050e+00  6.1453533e-01 -4.6978015e-01 -1.2218026e+00
   5.8988369e-01 -1.6549793e+00 -9.5219630e-01 -5.0776982e-01
  -1.1306157e+00 -1.9674897e+00 -6.9911230e-01  1.5758305e+00
  -7.0926434e-01 -1.7749195e+00 -4.6195951e-01 -1.9447902e+00
  -8.1449336e-01 -4.6783561e-01  4.8024943e-01  1.9616952e+00
   8.4814042e-02  9.9030644e-01 -1.0423635e+00 -6.2828046e-01
   9.3465865e-01  4.0963616e+00 -7.9763323e-01 -1.7487277e-01
  -2.1765757e+00 -8.3017856e-02  1.4711642e+00  1.1810126e+00
   3.9584055e+00  3.3325586e-02  2.8529668e+00  5.6696051e-01
  -2.7593553e-01  1.1096450e+00  1.1198273e+00 -2.0636795e+00
   8.8874459e-01  1.8020897e+00  2.7368755e+00  1.1290513e+00
  -1.7159050e+00 -8.6573529e-01  7.1405590e-01 -1.6174008e+00
  -1.4844151e+00 -2.0982837e-02 -7.2938013e-01  1.1176523e+00
  -1.2378637e+00 -2.1676948e+00  1.9361804e-01 -5.7871163e-01
  -7.7036661e-01 -7.0139742e-01 -4.9073759e-01  3.4728494e+00
   1.4019963e-01  1.0862827e+00 -1.4849342e-01 -1.1117792e+00
   1.1046270e+00  2.0156342e-01 -2.2441285e+00  1.1988220e+00
   2.0293076e+00 -7.0345604e-01 -9.9315786e-01 -1.8190764e+00
  -2.0546147e-01  3.4190726e-01  8.6178470e-01 -3.8058427e-01
  -1.3053159e+00 -1.3326305e+00  2.1571548e+00 -1.9423856e-01
   1.9722532e-03 -1.2400500e+00  1.6754388e+00  5.8734119e-01
  -1.6799362e-01 -1.0328506e+00 -2.5448472e+00  9.4422907e-01
  -9.6457392e-01 -7.1653622e-01 -2.1197073e+00  1.5098217e+00
   3.1248647e-01  7.7214308e-02 -9.5498599e-02  3.5369200e-01
   1.7113360e+00 -3.3608335e-01  9.6076936e-02  9.8362875e-01
  -3.6191285e-01  2.3132374e+00 -1.5383853e+00 -6.5345412e-01
  -3.1945851e-01 -9.4250977e-01  2.0706072e+00 -1.0812515e+00
   8.5808498e-01  2.2993159e-01 -1.2599972e+00 -5.4649609e-01
  -7.6876169e-01  9.6442431e-02 -2.1621580e+00 -3.3553413e-01]]

Final Loss: 0.0000
Distance Metric: 46.3933
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 101

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.5228121   4.7043343  -2.0476458  ... -0.8212837   2.9575164
   2.6058278 ]
 [-0.00918415  0.09780222  0.02789821 ...  0.37536398 -0.5350761
  -0.6008549 ]
 [-0.47038805  0.67213553  0.8771492  ... -2.4141483  -0.93808675
   1.290894  ]
 ...
 [-0.92131245 -2.8154407  -1.9946421  ...  1.0452888  -0.19591823
  -2.5990224 ]
 [-1.7918696   3.0416856  -0.9027682  ... -1.2779025   2.1019828
   0.4972389 ]
 [ 1.4293356  -0.41228005 -0.08128182 ...  0.49403206  1.1213409
   0.85201824]]
layers.1.weight: [[-1.0431238e-02  1.2208418e-02  2.2385942e-03 ...  0.0000000e+00
  -5.8532818e-03  0.0000000e+00]
 [ 1.9856240e-03  6.0525248e-03 -4.7351937e-03 ... -6.9877245e-03
  -4.1711428e-03  2.9872328e-03]
 [ 1.4256984e+00  2.0822854e-01 -2.0593753e+00 ... -1.7565917e+00
   3.0034912e-01 -9.5277280e-01]
 ...
 [-9.2635294e-03  1.4615048e-03 -4.3066363e-03 ...  5.3110905e-03
   2.9523855e-03  2.3952608e-03]
 [ 9.6493298e-01 -1.4581311e+00 -8.5682714e-01 ...  1.0679487e+00
  -3.3024937e-01 -4.5721519e-01]
 [ 9.4784722e-03 -2.8686700e-02  2.3407789e-02 ...  2.8025230e-02
   1.3487011e-02 -3.9219079e-03]]
layers.2.weight: [[ 1.53917028e-03 -4.05354286e-03  1.19369268e+00  4.55106702e-03
   8.20120573e-02  4.91469586e-03  2.41281861e-03  8.94934654e-01
  -2.95449025e-03  9.80487838e-03  8.60689301e-03  3.12422332e-03
   3.06767179e-03 -1.50701050e-02 -2.40837529e-04 -1.03235245e-02
  -7.21923215e-03  9.50879231e-03  9.63708851e-03  6.17362373e-03
   0.00000000e+00  2.17821868e-03  1.02004898e-03  8.54302645e-01
  -5.80318784e-03  3.87322996e-03 -1.92890421e-03 -5.88814612e-04
   1.10211840e-03 -2.38953182e-03 -7.22759170e-03  1.73987597e-02
   1.42182689e-02  1.00776963e-01  1.35676265e-01  1.34990492e-03
   2.54800260e-01 -1.20445685e-02  1.89104695e-02 -9.29622038e-04
   4.03346960e-03 -8.51372629e-03  1.09681403e-02 -4.05005157e-01
  -1.17671173e-02  4.40416858e-03 -1.19524065e-03 -7.51076825e-03
   4.99671744e-03 -1.11103116e-03  4.49454039e-03  1.66952744e-01
   2.87037436e-03 -1.21434489e-02  5.73106576e-04 -1.89015970e-01
   1.44880554e-02  2.67877340e-01 -6.51735812e-03  1.73846073e-03
   5.59418136e-03  5.37726190e-03  5.21003664e-01  8.50236230e-03
  -2.88987206e-03 -2.75175064e-03  1.87237695e-01 -6.96947519e-03
  -1.04178600e-02 -1.14051474e-03 -1.88452215e-03  3.48157180e-03
   1.64572557e-04  3.62716685e-03 -3.69965652e-04 -1.11294752e-02
   5.52141201e-03  6.41353149e-03  1.82482006e-03 -2.86660274e-03
   3.13809887e-03  8.66105605e-04  1.31252119e-02  1.62464543e-03
  -1.25301760e-02  1.24634884e-03 -1.65323145e-03 -5.69726015e-03
   1.82104111e-03  4.70587425e-03  3.84343136e-03  1.22778594e+00
   3.11861979e-03 -9.78460833e-02 -4.76457085e-03  3.05219600e-03
   2.24372558e-03  1.17940526e-03  6.01845561e-04  7.64032966e-03
   2.12236471e-03  1.85151145e-01  1.04820868e-02  6.01667166e-03
   4.13048446e-01 -2.42754310e-01 -4.74145859e-01 -7.53917396e-02
  -9.85006802e-03  2.40182690e-03  5.12168370e-03  5.22533257e-04
  -3.76050681e-01  2.00037775e-03  7.89182913e-03  1.18789780e-04
  -1.84754154e-03  0.00000000e+00  7.01100081e-02 -1.93201657e-02
   9.27424431e-03 -5.05258935e-03  7.20887771e-03  5.21035254e-01
   1.02005964e-02 -2.82645947e-03 -1.95279300e-01 -9.97988041e-03]]

Final Loss: 0.9755
Distance Metric: 130.8623
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-5.70618212e-01  4.47183132e-01 -3.67261171e-02 ... -3.37483026e-02
  -1.04380414e-01 -1.18973352e-01]
 [-2.34733701e-01 -8.04721043e-02  9.52250808e-02 ... -2.24673226e-01
  -1.26929760e-01  1.16551384e-01]
 [ 3.31065476e-01 -6.76764771e-02 -8.06302056e-02 ... -1.81591615e-01
   5.46341427e-02  6.75603300e-02]
 ...
 [ 5.56671679e-01 -1.46612659e-01  2.60067612e-01 ...  2.24523798e-01
  -2.21062899e-01  1.89941525e-01]
 [-7.23732769e-01  5.70537508e-01 -3.30185175e-01 ...  1.04214631e-01
   1.15703944e-04  6.84281662e-02]
 [ 2.46426210e-01 -2.28548925e-02  1.54392481e-01 ...  2.31649071e-01
   1.60805181e-01 -1.17263295e-01]]
layers.1.weight: [[ 0.05033636 -0.04055584 -0.11085305 ... -0.03451797 -0.1251732
   0.00317474]
 [ 0.19935161 -0.04300564 -0.08823434 ... -0.25787577 -0.01749583
  -0.01940436]
 [-0.24426064  0.07241566 -0.00538425 ...  0.07426877 -0.2605987
  -0.03335891]
 ...
 [ 0.08416691 -0.08635147  0.08563542 ... -0.10937379  0.1684495
  -0.17717612]
 [ 0.00192937  0.08461042 -0.05469937 ... -0.0023616   0.07292941
  -0.15528917]
 [-0.32393983  0.08306535  0.17812273 ...  0.42575878 -0.54089886
   0.288749  ]]
layers.2.weight: [[ 6.14165664e-02 -1.62288117e+00  1.86594367e+00 -2.44820982e-01
  -8.35753620e-01 -5.71846247e-01 -2.62568027e-01 -4.20060337e-01
  -6.33980930e-01 -5.66049337e-01  5.91381669e-01 -1.20168483e+00
   2.07985312e-01 -5.95008671e-01 -7.96202064e-01 -1.35955706e-01
   7.25844502e-02 -1.15840316e-01  1.59603024e+00 -4.45353687e-01
  -1.43212247e+00 -8.28093052e-01 -4.17576998e-01 -2.18246177e-01
   8.04102480e-01 -1.64873815e+00  3.00976515e-01 -6.12534523e-01
   2.32817650e-01 -7.07141042e-01 -1.43118292e-01 -5.81038773e-01
  -9.37311500e-02  7.77220428e-01  7.26415142e-02 -2.80872673e-01
   1.59091607e-01  4.72181976e-01 -2.94485629e-01 -3.23483109e-01
  -5.61679363e-01 -4.24608022e-01 -9.58564579e-01 -4.78208363e-01
  -4.36539322e-01 -2.23444968e-01  8.64688218e-01  4.25730735e-01
  -8.57611954e-01  3.16627502e-01  3.41140985e-01 -3.84100646e-01
  -7.72251070e-01 -6.54669166e-01 -1.51836634e+00 -5.97790956e-01
  -8.66078734e-01 -2.10291043e-01  6.06593583e-03 -3.57098788e-01
   1.11579066e-02  7.20479200e-03 -1.32632390e-01  1.02094221e+00
   1.75646454e-01  1.65295318e-01  4.89050776e-01 -7.23314285e-01
  -3.08578432e-01  3.24211955e-01  1.07714832e-01 -1.67789659e-03
  -5.23916602e-01 -3.99865806e-01  5.96171916e-02 -9.73198831e-01
  -8.81022930e-01 -4.76000756e-01 -4.62238789e-01  2.92676836e-02
   1.68126130e+00 -2.56985426e-01 -5.44845164e-01  6.36808157e-01
   2.94653624e-01 -1.09302044e-01 -3.64170015e-01  1.29621878e-01
  -1.25624537e+00 -2.36804679e-01 -1.21884719e-01  3.59443694e-01
  -1.14882641e-01 -3.56629157e+00  4.38337028e-02 -2.25553274e-01
  -1.62536633e+00  1.44043589e+00  3.67628992e-01 -1.71935707e-01
  -1.24890439e-01  8.34091678e-02  2.62324095e-01 -7.66093433e-01
   1.11639515e-01 -1.04186928e+00 -5.06654561e-01  2.21933782e-01
  -6.62093937e-01  1.14164986e-01 -4.33056414e-01 -1.47467688e-01
  -4.40451592e-01 -4.47703630e-01  9.88498479e-02 -2.12858632e-01
  -1.81394374e+00 -5.33343375e-01 -3.14665198e-01 -2.41087332e-01
  -5.14634323e+00 -5.11363983e-01  1.30877346e-01 -1.42749399e-01
  -8.10937285e-01 -1.03696728e+00 -1.18371522e+00  6.40981245e+00]]

Final Loss: 0.3501
Distance Metric: 53.0016
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.04994275 -0.12681018 -0.11639497 ... -0.07670005  0.16073595
   0.21482792]
 [ 0.05160449 -0.05066868 -0.10912763 ... -0.00517835  0.04449293
   0.20251213]
 [-0.03208371 -0.26982856  0.05146759 ...  0.00881983 -0.14287353
   0.09753768]
 ...
 [-0.33296093  0.11481901 -0.1442033  ... -0.12559487 -0.0559278
  -0.07258569]
 [-0.09376863  0.01063682  0.20541522 ...  0.4388467  -0.05055404
  -0.50521207]
 [ 0.06584859  0.15059747  0.17913885 ... -0.04212756 -0.11351642
   0.01121136]]
layers.1.weight: [[-0.11502592 -0.03930899  0.05123406 ...  0.08122803  0.06353945
  -0.00365267]
 [ 0.18493532  0.1280089  -0.05922519 ...  0.08734062  0.04521355
  -0.02297989]
 [-0.21973228  0.11085398 -0.1607581  ...  0.02954986  0.02573972
   0.00293984]
 ...
 [-0.2139395  -0.05067821 -0.30075064 ...  0.02488448 -0.08021968
   0.15107861]
 [-0.01899583 -0.15618987 -0.0592882  ... -0.08980827 -0.12626334
   0.10206665]
 [-0.15939145  0.16281626  0.06282398 ... -0.240969    0.13889422
  -0.13062912]]
layers.2.weight: [[ 0.0547622   0.4855417   2.6077614   0.84324795  0.41377375 -0.18059723
   0.39549965  0.22428153  1.6704401   0.7507692  -2.4831917   0.5806269
  -0.5186433  -0.02357613  1.4267699   1.6286016   1.0282084   0.44909185
  -0.5102768   0.23610388 -0.07362545 -0.5015516  -0.9846073   1.284426
  -1.2313342   1.4237239  -5.4259934   1.037848   -0.0741315   0.5956325
  -0.860082    1.2467567   1.5169622  -2.171189    0.2057655  -1.712286
  -1.7493192   1.8749368   1.6951021   1.0659579   1.9568244  -2.1886125
   1.9477392  -0.7849517   1.3302743  -0.18693274 -0.13197848 -0.83768874
  -0.18597943 -2.3850744  -4.649974   -1.3062798   0.03845946 -0.9683714
  -0.08535476  0.7592938   0.64118505 -0.18945915 -0.96273524  0.40699565
  -1.2307013  -0.3701981  -0.15327007 -0.87390274  1.0077477   0.07398994
   0.32567292  0.6668268  -0.46128464 -1.7092214   4.0459075  -0.8038357
   1.1209257   2.2862697  -3.2635157   0.04083962  1.8542892   2.07177
   3.2652273  -3.506414    3.5646327  -0.03363378 -1.3315109   1.0020041
  -2.0839157   1.1924249  -0.8202793  -1.7836775   1.210445   -1.586364
   0.62462276 -3.1501527  -0.640302   -0.00567786  1.2850658  -1.5123773
  -0.6106216   2.936522    0.0701393   0.37055585 -0.18601175 -1.0018175
   2.1783247  -0.41897526  0.42084217  0.70581067  2.09334    -1.2560115
   1.5764501  -1.3131405   1.2110844  -0.81501687  1.6119131   0.23714405
   0.42591617 -1.3954389   0.28285998  0.8591953   0.98241353  1.3626332
   0.9842054   0.9688033  -1.7037796  -0.90792125  0.295792    0.8094241
   1.4772813  -0.8991922 ]]

Final Loss: 0.7009
Distance Metric: 44.0263
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 195

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.21100481 -0.01793182  0.17459393 ... -0.00446262 -0.00204156
  -0.2544306 ]
 [ 0.12979084 -0.06935236 -0.03243542 ... -0.3860599   0.38219178
  -0.17760587]
 [ 0.03062835  0.16438003 -0.17775851 ...  0.06109515 -0.10977256
   0.0686426 ]
 ...
 [-0.1816237   0.2716712  -0.15502888 ...  0.02160173 -0.12912099
  -0.07511748]
 [-0.00883138  0.06798061  0.08471983 ... -0.01902729 -0.04679872
  -0.02731851]
 [-0.05876988 -0.15048535 -0.23021203 ... -0.03045371  0.1800892
   0.10622152]]
layers.1.weight: [[-0.07853779  0.22637624 -0.10933388 ...  0.23565663  0.23902647
   0.1230187 ]
 [-0.01070843  0.2545243  -0.07374768 ...  0.10403232  0.24384879
   0.23104411]
 [-0.05793617 -0.17689914  0.08171678 ... -0.20175838 -0.04414314
   0.07708529]
 ...
 [ 0.03516788 -0.20149244  0.22538869 ... -0.03302614 -0.0474641
  -0.15864855]
 [-0.06805013  0.04193419 -0.10946628 ...  0.12199681 -0.05459952
   0.04146609]
 [ 0.16641434 -0.10461919  0.21483468 ...  0.10434922 -0.00721322
  -0.07131933]]
layers.2.weight: [[ 1.53274086e-04  1.03142895e-02  0.00000000e+00 -3.30283609e-03
  -6.40527997e-03 -7.17576267e-03  3.47239478e-03  2.03554402e-03
   1.58846262e-03  2.65565123e-02 -1.33401842e-03  4.75121886e-02
   0.00000000e+00  3.34816985e-03 -4.33131820e-03  0.00000000e+00
   1.45027065e-03 -9.01848730e-03 -2.36312742e-03  3.03128432e-03
   1.21420445e-02  1.26438114e-04 -4.33423556e-03 -3.52590170e-04
   9.88693628e-03  3.89645039e-03 -3.61145067e-04  2.66910233e-02
   1.69533084e-03 -2.71641440e-03  1.16011482e-02 -3.89638240e-03
  -2.18947791e-02  8.22515227e-03 -3.75078380e-04 -2.93550268e-03
   1.53282890e-03 -2.83498463e-04  1.77146588e-02  5.62766008e-03
   7.64080044e-03  1.07340503e-03 -1.30440737e-03 -5.46530308e-03
   6.86344085e-03 -4.64311475e-03  3.48530477e-03  2.68793432e-04
   4.88631427e-04 -5.43099875e-03  6.12744305e-04  0.00000000e+00
  -1.54751563e-03 -1.07111351e-03  2.00062495e-04 -1.36183528e-03
   1.62122468e-03 -5.44137042e-03 -6.02802087e-04 -1.65292602e-02
  -3.36904981e-04  1.08219557e-01  3.81340971e-04 -4.82807955e-04
  -8.78805993e-04  9.94895818e-04  6.53105031e-04  9.77313053e-03
  -1.90269258e-02 -1.81382835e-01  1.85917004e-03  1.10985304e-03
  -1.78700890e-02  0.00000000e+00 -5.89454127e-03 -9.20154899e-03
   1.13355182e-02 -4.86702396e-04  5.06921264e-04  4.18068748e-03
   1.65499821e-02  8.83478858e-03 -2.07025581e-03  1.78792384e-02
   4.08072257e-03  3.46030235e-01  2.19866738e-01 -3.19383165e-04
  -2.43306905e-03  5.34522487e-03 -1.28393993e-02  2.46726219e-02
  -1.75008352e-03 -4.35159687e-04  0.00000000e+00  5.66819347e-02
  -3.65634495e-03 -2.45692371e-03  0.00000000e+00  1.79700623e-03
   1.55818779e-02  2.29747361e-03 -1.11693761e-03  1.33173785e-03
   7.31050677e-04  4.70970338e-03 -3.22245783e-03  3.94636625e-03
   1.90793967e-03 -1.14971735e-02  3.87455570e-03 -3.54305863e-01
  -7.67714297e-03  2.62379297e-03 -9.60155448e-04  1.21180678e-03
  -3.28744849e-04  7.98414182e-03  0.00000000e+00  8.96113459e-04
  -3.05672671e-04  2.70451186e-03 -3.01593821e-03 -1.19822507e-03
  -2.70668505e-04  3.16928793e-03 -2.07975090e-01 -5.90615580e-03]]

Final Loss: 0.0000
Distance Metric: 26.6179
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 2
stopped after epoch: 1999

================================================================================

