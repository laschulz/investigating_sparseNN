Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.94793487  0.92775726 -0.33166587]]]
layers.1.weight: [[[ 0.4249338  -0.22800314]]]
layers.2.weight: [[[-0.97687685  0.18303221]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.21793802  0.08949588 -0.06022836 ... -0.16496173 -0.17522158
   0.10702264]
 [ 0.13327092  0.01466363 -0.1893989  ... -0.00702053  0.01533637
   0.15991102]
 [ 0.13348667  0.07951747  0.0257898  ... -0.13848555  0.11946669
  -0.09412962]
 ...
 [ 0.05360081 -0.13652097  0.1887657  ... -0.15048662 -0.11623947
  -0.1162871 ]
 [-0.15106702 -0.15937786  0.13463521 ...  0.20240858  0.02469891
  -0.06566117]
 [ 0.09241861  0.01242144 -0.09332513 ... -0.09787488  0.20419845
   0.07423764]]
layers.1.weight: [[ 0.12594989  0.08385288 -0.00812356 ... -0.10488823 -0.02750064
   0.14236057]
 [-0.14537175  0.03396424 -0.01747935 ...  0.10388293 -0.09620925
  -0.10206839]
 [ 0.1488872   0.08121935  0.01457761 ...  0.04981428 -0.0860511
  -0.07070795]
 ...
 [ 0.08600152  0.03429134 -0.03257835 ... -0.08882022  0.09544561
  -0.05403721]
 [-0.01284419  0.01157996 -0.03593989 ... -0.12655675  0.07105289
   0.10238104]
 [-0.03835067  0.06102433 -0.10977329 ... -0.13034323 -0.04349831
  -0.05290455]]
layers.2.weight: [[-0.04256508  0.08082148 -0.05306575 -0.04251068  0.07171325  0.10351606
   0.13182898 -0.08228081 -0.09271468 -0.22942135  0.21861465  0.02834178
   0.17884773 -0.05977824 -0.22010243  0.09072439 -0.18664013  0.17946075
   0.03531116 -0.0631623  -0.14033075  0.13794495 -0.0972973  -0.19324766
  -0.08048107 -0.00692951 -0.16845348  0.02780336  0.09957394  0.18545286
  -0.09745535 -0.14933173 -0.11672071 -0.0397268  -0.01201333  0.00382071
  -0.07565502  0.01149134 -0.22147971 -0.072855   -0.01977095  0.05394923
  -0.00375059 -0.01114701 -0.00554106  0.07002561 -0.08008462  0.18546262
  -0.11165162 -0.01106707 -0.00570519 -0.22730549 -0.04634242  0.00835549
   0.01893706  0.09622496  0.20587955 -0.02374011 -0.06795203 -0.0206885
   0.13775173  0.03220634 -0.17090248  0.0608975   0.10029897  0.19583765
  -0.09642355 -0.17702065 -0.15061976  0.15949228  0.08254372 -0.1435068
  -0.2045121  -0.10604455 -0.2300437  -0.13790014 -0.13429537  0.03721832
  -0.04527966 -0.21225373 -0.01023183 -0.16350545  0.15439825 -0.04007291
   0.17439282  0.0562376   0.07438318  0.13002956  0.01159171  0.05432713
  -0.044191    0.13025208 -0.05334614 -0.05083843  0.11522982  0.05058447
   0.20181654  0.154659   -0.05741362 -0.20755073  0.12261393  0.14247419
   0.03097033 -0.15527213  0.1751012  -0.09785981 -0.01346682  0.06140086
   0.10496572 -0.0170044   0.10844891 -0.00070588 -0.02194053  0.05492647
   0.08568452  0.00106257 -0.16329658 -0.15212388 -0.17198075  0.07066957
   0.17860825  0.16664895 -0.03170411 -0.08167119  0.07019784 -0.16282687
   0.19132102  0.15489677]]

Final Loss: 0.0000
Distance Metric: 18.9240
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.0588882   0.00948971 -0.14455417 ... -0.0767551   0.07389162
   0.06032585]
 [ 0.04877052  0.05743368 -0.17056711 ... -0.0945522  -0.30806437
  -0.00952754]
 [-0.04951108  0.09147179  0.03910448 ... -0.00280596 -0.08960698
  -0.03767897]
 ...
 [ 0.14806683 -0.01197113 -0.30300707 ... -0.09460561  0.08452767
  -0.09553265]
 [ 0.01807444  0.04603689 -0.13358653 ...  0.14359438 -0.15093675
   0.26555213]
 [ 0.11950409  0.01059429 -0.1645123  ... -0.07416289  0.0577569
  -0.02755889]]
layers.1.weight: [[-0.06240832  0.01071417  0.32047686 ...  0.02586136 -0.05693043
   0.14682066]
 [-0.1378019   0.13426596  0.02536454 ...  0.01892697  0.06955078
  -0.2065035 ]
 [-0.00313467  0.01164962 -0.16049738 ...  0.05664834 -0.33065274
   0.2774249 ]
 ...
 [-0.14946023 -0.0691734  -0.00274159 ... -0.18466525  0.13951722
  -0.09312207]
 [ 0.13328765  0.11955319 -0.04092342 ... -0.03874288  0.15508477
   0.01469646]
 [-0.00212798 -0.02994031  0.2797771  ...  0.04990445 -0.00119598
  -0.05448088]]
layers.2.weight: [[ 1.91949940e+00  3.01778585e-01  1.61356795e+00 -3.70512056e+00
   1.19054377e+00  2.20036030e+00  1.53923437e-01 -4.38758761e-01
   3.89858000e-02 -8.77405405e-01  1.12736011e+00  1.91090554e-01
  -3.87702733e-01  2.60460764e-01  2.98898071e-01 -2.06984711e+00
   3.65152448e-01 -1.06565869e+00 -1.60809591e-01 -3.74612547e-02
   9.51317072e-01 -9.43697929e-01 -1.57061768e+00  1.92930996e+00
  -1.23428333e+00  2.20593643e+00  1.79420486e-01 -8.98675263e-01
  -2.88391709e-01 -1.97244391e-01 -8.13710392e-01  4.12033856e-01
  -1.33393264e+00 -2.55880022e+00  1.38951942e-01  4.36470175e+00
  -3.63295823e-01 -9.33210015e-01  9.62858200e-01 -3.05467211e-02
   5.12861907e-01 -3.46911341e-01 -1.83449483e+00 -7.05594569e-02
  -2.25581795e-01 -8.41298699e-03  2.95042336e-01  7.09372818e-01
  -1.51545131e+00 -6.39800072e-01 -1.56592155e+00 -1.28507543e+00
  -2.99970269e+00  5.91058195e-01 -1.12787175e+00 -6.93845272e-01
   1.89887047e+00  8.93392622e-01 -9.70519066e-01 -8.45866203e-01
  -2.57196927e+00  1.34477317e+00  2.07601714e+00  1.73967636e+00
   3.56159091e+00  4.13001060e-01 -3.46428418e+00 -2.12835741e+00
  -2.27783275e+00  1.89894760e+00 -1.33492947e-01 -6.15676701e-01
   1.85385928e-01 -1.05642200e+00 -2.29347885e-01 -2.65219784e+00
  -4.65222687e-01 -7.16674447e-01 -2.03644544e-01 -1.44809514e-01
  -9.87471640e-01  2.84129709e-01 -4.60666716e-01 -4.13344920e-01
   1.17929721e+00 -1.43538105e+00  2.49142960e-01  1.14684057e+00
   1.10540831e+00  1.48135495e+00  1.89481986e+00  3.11446637e-01
   5.38549960e-01 -1.28293514e+00 -1.11943889e+00  1.41131282e+00
   1.93349376e-01  1.84952629e+00  5.97018182e-01 -5.05911291e-01
  -1.58857751e+00 -4.31005090e-01 -1.33652246e+00 -1.18480936e-01
   6.66769266e-01  7.79625833e-01 -5.00430524e-01  1.38296515e-01
   1.03712022e+00 -1.63453549e-01  3.66760325e+00 -1.11465776e+00
  -9.31592107e-01 -3.69023383e-01  3.98019999e-01 -3.99931312e-01
   9.39291000e-01 -1.71711314e+00  1.52424097e+00 -9.39225078e-01
  -6.94401085e-01 -7.99722254e-01  3.60235735e-03 -1.14800978e+00
  -2.62593102e+00 -1.11344385e+00  1.48814738e+00 -4.59505290e-01]]

Final Loss: 0.4424
Distance Metric: 37.3113
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 168

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-5.4935567e-02 -3.7942875e-02  4.3446716e-02 ...  5.8728864e-04
   6.9768094e-02  1.3992822e-02]
 [ 2.9989541e+00  1.7049892e+00 -1.2701424e+00 ... -2.1470704e+00
  -6.9231951e-01  3.5568190e-01]
 [-1.6988378e+00 -9.6497989e-01  1.9979891e+00 ... -1.6093426e+00
  -1.4279377e+00  1.9302158e+00]
 ...
 [-1.4863065e-01 -5.9186077e-01 -1.9448425e+00 ...  1.1313118e+00
  -1.4338400e+00 -3.5081428e-01]
 [ 3.2497618e-02  2.8004799e-02 -2.8805077e-02 ... -2.3894464e-03
  -6.5464824e-02 -3.8516182e-03]
 [ 3.7362811e-01  1.6835699e+00 -2.1601675e-01 ... -6.5563458e-01
  -8.5037291e-01 -9.4362175e-01]]
layers.1.weight: [[-0.00922906  0.00627293  0.00172563 ... -0.00843213 -0.02408847
  -0.01763877]
 [-0.04323345 -0.00050567 -0.00325236 ...  0.00217706  0.04028258
   0.00379048]
 [-0.01246769  0.00674064  0.01656032 ...  0.00721295 -0.03603639
   0.00204126]
 ...
 [-0.02586735  0.02746448 -0.04941985 ... -0.0307542  -0.03986129
  -0.04593807]
 [-0.02892527  0.00411992  0.00066915 ...  0.00233476 -0.01536986
  -0.00925611]
 [-0.00102422 -0.0017634   0.01345259 ... -0.00042923 -0.02089277
  -0.00417875]]
layers.2.weight: [[ 0.00234319 -0.00283864 -0.00431299 -0.0035683  -0.00575887  0.00676817
  -0.01021417  0.01035011  0.00221246 -0.00918109 -0.01679442  0.00636747
   0.00031403  0.00451756  0.01150283  0.00129558 -0.00411191 -0.00821462
  -0.01174297  0.0088874  -0.00599025 -0.00182118  0.00459676 -0.00288662
  -0.00733605 -0.0003108  -0.00306469 -0.00367764  0.00739123  0.0065835
  -0.01005596  0.00281581 -0.01467482  0.01339651 -0.02522391 -0.00237522
   0.00270324  0.00028893  0.00456292 -0.00019236  0.00455869 -0.00807499
   0.00678621 -0.0003938  -0.0024616  -0.00939391 -0.00184811 -0.00956692
  -0.00203126  0.00166511  0.          0.01679046 -0.00170791 -0.01982381
   0.00338547  0.11576381 -0.00814603  0.00127852 -0.00641004  0.00415993
  -0.0011345  -0.00682249  0.00809594 -0.00361644 -0.01037916 -0.01353439
   0.00304014 -0.00310565  0.00210139 -0.0019191  -0.00863497  0.01204246
   0.00213619  0.00097165  0.15257315  0.00240699 -0.00251752  0.01048236
  -0.00950253  0.01144013  0.0022343  -0.013934   -0.00511457  0.00879534
   0.          0.01435842  0.01526085  0.01139995 -0.00789975 -0.01335375
  -0.00252845 -0.00571092  0.00271932  0.00186603 -0.01475439 -0.00437381
  -0.00457935 -0.01692352  0.00775285  0.01029748 -0.00333859 -0.01276374
   0.00460381  0.00397595 -0.00503535 -0.00643555  0.00064977 -0.01536207
   0.0070415  -0.00169691 -0.00178708 -0.00891181 -0.00837077 -0.00711638
  -0.00115075  0.00895342  0.00270715  0.00385248  0.00400166  0.00158771
  -0.00712799  0.00061328 -0.00934336  0.00282697 -0.00721972  0.0091812
   0.00178186 -0.00231193]]

Final Loss: 0.5168
Distance Metric: 67.3286
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 1998

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.11034897  0.03136244  0.7980407  ...  0.0143198  -0.01174856
  -0.01884445]
 [-0.07068947 -0.02122035  0.8363503  ...  0.22518562 -0.08140264
  -3.9432955 ]
 [ 0.06492923 -0.03635358 -0.9651871  ...  0.0404252   0.04071112
   0.47330835]
 ...
 [-0.04764204  0.01571831  0.61943305 ...  0.08605634 -0.02725068
  -1.1719928 ]
 [-0.06234873 -0.07064069  1.0847129  ... -0.05298611 -0.00691851
   0.85573465]
 [-0.1175257   0.05412443  2.3202484  ...  0.08153658 -0.0101844
  -1.2964388 ]]
layers.1.weight: [[ 0.10433014 -0.23915596  0.01736647 ... -0.0524573   0.06234831
  -0.12166682]
 [-0.05919242  0.05213016 -0.07126474 ...  0.11137744 -0.09605893
  -0.1765976 ]
 [ 0.13217139 -0.01205746 -0.04511512 ...  0.01179332 -0.03537375
  -0.15882815]
 ...
 [-0.07876889 -0.13223049  0.13656838 ... -0.04591035 -0.06201291
   0.00393084]
 [ 0.05815044 -0.1180193   0.03237997 ...  0.08329776 -0.07237306
   0.08464482]
 [ 0.01564661  0.14338313 -0.2571863  ... -0.22577886  0.00964726
  -0.05354597]]
layers.2.weight: [[ 0.2775668   0.13221364  0.40629423 -5.3367004   0.998818   -0.07431561
   0.12877849  2.2489164  -0.695799   -0.15149364  0.3252715  -0.27122626
   1.7289224  -0.29210624 -0.01048446 -0.34702685 -0.9357497   0.09954621
   0.4971663   1.8047501  -3.5860782   2.0080447   1.9744054   1.3933375
   0.4631842   0.29027706 -0.33408698  0.10290382  0.4313031   0.38746655
   2.7954376  -0.6080937  -0.14040908  0.03521971 -0.62010294  0.36103976
   0.0721162   0.84097576  0.95665324  0.47290114 -0.41070524 -1.208326
  -0.20959567  1.0622588   0.29623222  0.3630286  -0.26312476  0.3278857
   0.48681432  4.872689    0.01756813 -0.2673375   0.36305615  0.61359876
   0.2712937   0.8866605  -0.2503644  -1.0750179  -0.07146771  1.3253458
   0.12083906  0.89993584  1.8781438   1.4237058   0.28342363 -6.576726
  -3.3489704  -0.6855515   0.4277043   0.3011803   0.27274397 -0.11420124
   2.6729324   0.24208039  0.16961835  0.5303308  -0.07209846  0.59760803
  -0.37240967 -0.25284687  0.47402364 -0.16116802  0.37502128  1.0951316
   0.31868735 -0.07427487 -2.723714    0.10367194  0.37397468  1.1042223
  -0.35428637 -0.2805928   1.1423652  -0.6477578  -0.25078753  0.2923322
  -0.10841522  2.4000547   0.3301377  -0.3313389  -2.077163    2.8197083
   0.28759784  0.28740317  0.36391523 -0.29079098  2.388711   -1.4459496
  -0.3392916  -0.7272123  -0.07370915  0.8779799   0.82496196  0.38604832
  -0.254069    0.53218275  0.73106337 -0.9874438   0.48598516  3.3468175
   0.37156555 -0.28495535  0.4084237   3.120525    0.26087978  0.3890118
  -0.21276917  0.8027668 ]]

Final Loss: 0.0595
Distance Metric: 58.5994
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.22994149  0.11604752  0.08788435 ... -0.00428383  0.01208857
   0.04432051]
 [ 0.08804171 -0.06264253  0.13902079 ...  0.10077392  0.10109268
  -0.04231133]
 [-0.01842755 -0.01936104  0.06468882 ...  0.01250899 -0.156011
  -0.1421105 ]
 ...
 [-0.12600155  0.2016564  -0.09299934 ...  0.01570464  0.13968503
   0.10077676]
 [-0.06502955 -0.14324006 -0.0254998  ...  0.16963853  0.2586143
   0.01150076]
 [ 0.03585164  0.11257405 -0.1667778  ...  0.09922567 -0.03577266
   0.24028884]]
layers.1.weight: [[ 0.0007373  -0.05396309 -0.02184066 ... -0.03143122  0.10246496
   0.10394243]
 [ 0.06905509 -0.05882571  0.06345046 ... -0.02826248  0.10192917
   0.04704134]
 [ 0.20453733  0.04898501 -0.13470809 ...  0.04505646 -0.03865876
  -0.14016862]
 ...
 [ 0.04753183 -0.06191842  0.12216259 ...  0.10653804 -0.16955873
  -0.06558063]
 [ 0.07039397 -0.05179164  0.18717211 ...  0.00768939  0.07357685
   0.07527618]
 [ 0.10742385 -0.09864123  0.05169757 ...  0.13987218 -0.09736168
   0.01878509]]
layers.2.weight: [[ 0.44780785  0.61795455  0.6216994   0.65535325  0.07548684 -0.85164046
  -1.597783    0.3372472  -2.5616333   0.01183549  1.4574441   3.390969
   0.20305811 -0.27112108 -1.5928959  -1.2400616   0.35722098 -1.2243567
   2.31321     3.3912      0.5400221   0.2660941   0.27046546 -1.3454839
  -1.171724   -0.55691254 -0.14355206  0.9569607   1.6285453   1.8156381
   0.2425003   0.5053745  -0.1450918  -2.1751564  -4.6695495   1.8351258
  -1.4693412  -0.01868    -0.95468163 -1.5191904   1.6225184  -1.0836195
   1.684383   -0.26512098  0.75451046 -0.49731407 -0.6021538   1.8904132
  -0.19064002  1.7345688   1.7654357  -0.13560738 -0.57040554  1.2585106
   0.99456894  3.5641165   1.2622716  -0.58660185 -0.71078944 -1.956468
  -0.7007827   0.0785906   0.5849984   0.18046173 -2.4236262  -1.2542118
   1.8195941  -1.0180779  -2.7865472  -0.40524954 -1.6328943  -0.98282665
   0.94116294 -1.1574489   0.8012304   1.3952447  -1.9735078   1.9701866
  -0.72951704  1.0787377  -0.66907424  0.7074781   1.280466   -0.48194706
   1.8232707   2.4448345  -0.09037422  0.16843805 -0.7634686   0.87793535
   0.49041396 -0.3226711   0.5987513  -0.3848389   0.7396998   0.6497745
   0.24107018  2.8266695  -0.56976914  0.8606236  -1.1203467  -0.5346808
   0.18661885 -0.94350976  0.44163844  0.67425877  1.3726132  -1.2118059
  -1.5034755  -1.1055506   2.2734258  -0.7170293  -1.4311659  -0.08171718
   1.2483109   2.543358   -1.9573424   0.5496593  -0.6886628  -0.5709291
   1.5151536  -0.03359071  1.7637624  -0.25382233  0.11438526  0.46624678
  -1.7514013  -1.8649081 ]]

Final Loss: 0.7438
Distance Metric: 39.0305
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 150

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.39823443 -0.0624289   0.23616461 ...  0.46043146 -0.508508
   0.6582176 ]
 [ 0.2108341   0.18588611 -0.03799663 ...  0.18574546  0.02972937
  -0.18020242]
 [ 0.14116049  0.01287548  0.11054678 ... -0.38987496  0.16619304
  -0.1535767 ]
 ...
 [ 0.22417448 -0.31129512 -0.10380616 ...  0.9193827  -0.77391547
  -0.62485445]
 [-0.22127411  0.11544587  0.19534062 ... -0.13376284 -0.31344512
  -0.2517975 ]
 [ 0.05570617 -0.07893021  0.26531115 ...  0.5604033  -0.55235374
   1.5088574 ]]
layers.1.weight: [[ 0.11311997 -0.05183721  0.05492907 ... -0.13197833 -0.1902052
   0.09504192]
 [-0.03466651 -0.00231968 -0.01613688 ...  0.11123446 -0.01526577
  -0.04328561]
 [ 0.20160997  0.02269988 -0.02875676 ... -0.12477173 -0.09502722
  -0.14517349]
 ...
 [-0.01324365  0.16820295  0.16732611 ...  0.0182697  -0.05334418
   0.12261565]
 [ 0.02715084  0.09628765 -0.15069431 ... -0.00124626 -0.03595601
   0.03868479]
 [-0.06484313 -0.11629061  0.1425619  ...  0.10130028  0.04817121
  -0.0561958 ]]
layers.2.weight: [[-1.0082892e-02 -9.5901167e-04  8.1247548e-03  1.0715643e-02
  -1.1725968e-04  1.9041537e-03 -1.7022705e-03 -9.5108468e-03
   2.1409000e-01  8.9441836e-02  1.1191568e-02 -2.8856683e-03
   1.0896918e-02  3.9371233e-03  4.2943831e-04  1.3829583e-03
   2.6022214e-02 -4.5363703e-03  6.3136686e-03  1.2052236e-01
   9.3445167e-02 -1.8882535e-02 -1.1215771e-02 -7.8659533e-03
   3.7187880e-03  3.7388522e-03  8.0706794e-03  1.2923696e-02
   7.4108373e-03 -7.6698638e-03  6.4617898e-03  4.2033475e-04
   1.0045456e-02 -4.2682025e-04 -4.1517192e-03 -2.0413987e-02
   8.1468644e-03  1.5339454e-02 -1.3562471e-02  1.9840242e-03
  -9.7449441e-03  1.7663857e-02 -5.2785128e-03 -2.6873727e-03
  -2.2902102e-03  1.3474789e-02 -1.2559171e-02 -2.7074155e-03
  -6.4855726e-03 -2.9966326e-03  1.1159294e-02 -2.7434626e-01
  -3.0233113e-02  2.1798385e-03 -8.7683741e-03  1.3387221e-02
   4.9254913e-03  2.3866666e-03  1.3459912e-03 -5.2954368e-03
   6.0729176e-04 -1.7074317e-02 -9.7022783e-03 -5.1971665e-03
  -1.5448288e-02  1.8394516e-01 -2.2509976e-01 -1.1351141e-03
   2.9672641e-02  8.8638300e-03  1.3780020e-03  1.1376023e-03
   2.1774504e-02 -1.2355489e-02 -6.5171504e-03  7.5451104e-04
  -3.4204729e-02  3.0909941e-04 -1.3182955e-02 -1.4918819e-01
  -5.4709767e-03 -1.3394664e-02  4.4342726e-03  5.6432798e-03
   5.3030802e-03  2.2576128e-03 -8.7490324e-03 -1.1931018e-02
  -1.5857536e-01  2.7841276e-02  9.1129495e-03 -1.6198191e-01
  -6.5445001e-03 -1.5969884e-03  2.8194787e-03  1.0655572e-02
  -2.6051905e-02 -1.2313807e-02  2.0225169e-03  8.4516937e-03
  -1.2832292e-02 -6.4216106e-04  8.8444240e-03  2.8962183e-03
  -1.1212603e-03 -1.3475290e-02 -2.4467087e-03 -1.6267527e-02
   1.3176843e-02  2.4799041e-03 -6.0686045e-03  8.2191359e-03
  -9.7585386e-03  1.4918113e-01  2.4570595e-04 -4.4597792e-03
   1.4472475e-02  1.1173171e-02  4.4961325e-03 -3.1527657e-02
  -1.9332783e-02 -5.6678820e-02 -8.7258192e-03 -3.6094472e-02
   1.8507055e-03  6.7791259e-03  1.3259978e-02 -6.8662358e-03]]

Final Loss: 0.0056
Distance Metric: 33.0891
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.6592411  -0.29989702  0.32054067 ...  0.0642486  -0.00701689
  -0.04988036]
 [-0.12141981 -0.06034913  0.11620869 ... -0.06980505  0.15549359
  -0.12840699]
 [ 0.25510123 -0.00554387 -0.15840968 ... -0.1313836  -0.08517502
  -0.0935057 ]
 ...
 [-0.8294843  -0.17064108  0.41895267 ...  0.06133442 -0.02044666
  -0.01396287]
 [-0.32092407 -0.07690063  0.15272969 ... -0.03958529 -0.00826993
   0.01918995]
 [-0.1599811  -0.19082478  0.13532262 ...  0.19941497  0.04346403
  -0.01673256]]
layers.1.weight: [[ 0.08071695  0.03179716  0.15149999 ... -0.13846408  0.14562255
  -0.02833059]
 [ 0.08809064  0.13467593 -0.08099389 ...  0.02784703 -0.16826455
   0.13092777]
 [-0.09881933 -0.11310665  0.13571988 ... -0.09135777  0.13691662
  -0.07019474]
 ...
 [ 0.07616743  0.11077174 -0.06875461 ...  0.04372744 -0.03780155
   0.06378052]
 [-0.02611199 -0.01174296 -0.12643056 ...  0.07976691 -0.76018083
  -0.15534237]
 [-0.12971437 -0.13911891  0.15527247 ... -0.02924228  0.20503338
   0.01816504]]
layers.2.weight: [[ 8.11918378e-02 -3.70867610e-01  2.85535365e-01  6.49936616e-01
   1.25626298e-02  5.54725267e-02 -5.63738346e-01  1.47738298e-02
  -4.26613778e-01  6.15101218e-01 -4.01285350e-01  2.00717263e-02
   9.62675214e-01 -3.91159207e-01 -1.02725208e+00  3.13496530e-01
  -1.44337431e-01  2.53423154e-01 -3.53589833e-01 -2.79227793e-01
  -6.29932284e-01 -8.32163334e-01 -9.95183885e-02  1.45939261e-01
   8.08166802e-01 -5.69377601e-01 -4.88172948e-01 -7.40117654e-02
   1.01290107e-01 -3.50738674e-01 -1.42816558e-01 -1.88967812e+00
  -2.73291379e-01  8.93517863e-03 -3.16028625e-01  3.49526429e+00
  -5.51203907e-01 -2.01803625e-01 -1.54227987e-01 -8.48542035e-01
  -1.53620169e-01 -2.58295774e-01  2.46557191e-01  4.55955356e-01
   5.03988704e-03 -1.73619807e-01 -1.03662145e+00 -3.06132883e-01
   2.03331504e-02 -6.43733799e-01 -1.66152894e-01 -3.11244100e-01
   1.36800325e+00 -4.81151938e-01  1.68563426e-02 -3.91259462e-01
  -4.94037092e-01 -5.58806241e-01  1.92569211e-01  1.39357358e-01
   7.19340742e-02 -6.25769317e-01 -3.76560599e-01 -1.57186896e-01
   5.03558636e-01 -4.35563058e-01  3.28392953e-01 -1.36927247e-01
   3.59241888e-02 -3.15651417e-01 -2.23856837e-01 -1.06785917e+00
  -3.71103215e+00  1.99502349e-01  1.05763823e-01 -1.64687663e-01
  -1.73886567e-01  3.52281749e-01 -8.35749954e-02  2.22168162e-01
  -1.97098687e-01 -1.58351865e-02  6.81092292e-02 -1.01067460e+00
  -1.37364015e-01  7.85760134e-02 -2.61678845e-01  5.07499836e-02
  -2.30012655e-01 -2.16675982e-01 -3.95540565e-01 -5.85563540e-01
  -6.00547016e-01  4.25049961e-02  2.34851584e-01 -7.91456652e+00
  -8.81268859e-01  9.07219276e-02 -5.90231121e-01 -5.29763758e-01
   1.08933784e-01 -2.51518488e+00  3.54071796e-01 -1.47527790e+00
  -1.05652344e+00 -9.60285485e-01  3.60292763e-01 -2.48574466e-01
  -2.56174624e-01  1.37549881e-02 -1.78268105e-01  9.26957950e-02
  -2.19439179e-01 -3.09039652e-01 -5.17250538e-01 -2.85141677e-01
   1.10211059e-01  6.35767132e-02 -1.16390362e-01 -9.42659616e-01
  -1.16551310e-01 -1.35418773e-01 -7.39790797e-01  2.56419182e-02
   2.14249700e-01 -1.39051434e-02 -1.23585010e+00 -2.35344917e-01]]

Final Loss: 0.1980
Distance Metric: 49.4124
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.07738416 -0.00724754 -0.15779771 ... -0.04795901  0.07656284
   0.03044126]
 [ 0.05187915  0.06767648 -0.18447547 ... -0.09479806 -0.28337312
   0.0192228 ]
 [-0.04222858  0.08845897  0.03937671 ... -0.00793822 -0.09064996
  -0.01893962]
 ...
 [ 0.14683527 -0.01123602 -0.31524813 ... -0.08148389  0.09165167
  -0.10124075]
 [ 0.02142324  0.05039516 -0.13766392 ...  0.14695182 -0.15308781
   0.26405334]
 [ 0.11915916  0.01193058 -0.16482283 ... -0.07430138  0.05799137
  -0.02786868]]
layers.1.weight: [[-0.06063353  0.00961102  0.32204068 ...  0.02347341 -0.05720817
   0.14682066]
 [-0.13745444  0.13443671  0.02532299 ...  0.01878439  0.06949978
  -0.20649068]
 [-0.00432275  0.01137867 -0.16060491 ...  0.05291387 -0.32958972
   0.27748886]
 ...
 [-0.14946023 -0.0691734  -0.00274159 ... -0.18466525  0.13951722
  -0.09312207]
 [ 0.13466908  0.11880653 -0.03970566 ... -0.04060308  0.15520683
   0.01475626]
 [-0.00233315 -0.02983012  0.27933416 ...  0.0502743  -0.00131515
  -0.05448088]]
layers.2.weight: [[ 1.9234006e+00  3.0571970e-01  1.6089722e+00 -3.7051089e+00
   1.1892120e+00  2.2025399e+00  1.6552232e-01 -4.3795520e-01
   4.1039553e-02 -8.7419039e-01  1.1296359e+00  1.9362675e-01
  -3.8659531e-01  2.6514080e-01  2.9568985e-01 -2.0657120e+00
   3.6521214e-01 -1.0645572e+00 -1.5812021e-01 -4.4248026e-02
   9.5026553e-01 -9.4097346e-01 -1.5720596e+00  1.9225850e+00
  -1.2302023e+00  2.2059555e+00  1.8228477e-01 -8.9916289e-01
  -2.8551161e-01 -1.9682643e-01 -8.1659877e-01  4.1127089e-01
  -1.3339326e+00 -2.5688944e+00  1.5012684e-01  4.3599668e+00
  -3.6301598e-01 -9.3397629e-01  9.6575677e-01 -3.0384311e-02
   5.1156211e-01 -3.4654835e-01 -1.8328384e+00 -7.2531655e-02
  -2.2616182e-01 -9.2138853e-03  2.9504234e-01  7.0998365e-01
  -1.5184242e+00 -6.3900584e-01 -1.5660101e+00 -1.2786977e+00
  -2.9948990e+00  5.9082788e-01 -1.1265213e+00 -6.8982792e-01
   1.8898599e+00  8.9421844e-01 -9.6825099e-01 -8.4517044e-01
  -2.5721986e+00  1.3451900e+00  2.0753431e+00  1.7473302e+00
   3.5607655e+00  4.1494459e-01 -3.4667377e+00 -2.1244240e+00
  -2.2760012e+00  1.9005202e+00 -1.3178982e-01 -6.1613357e-01
   1.9068496e-01 -1.0492166e+00 -2.2950082e-01 -2.6595564e+00
  -4.6504685e-01 -7.0822746e-01 -2.0313445e-01 -1.3550740e-01
  -9.8641753e-01  2.8338599e-01 -4.6508020e-01 -4.1231588e-01
   1.1790708e+00 -1.4351927e+00  2.4701999e-01  1.1471775e+00
   1.1025151e+00  1.4815366e+00  1.9018705e+00  3.1608680e-01
   5.4562283e-01 -1.2812927e+00 -1.1207311e+00  1.4079869e+00
   1.9091648e-01  1.8487740e+00  5.9747124e-01 -5.0467443e-01
  -1.5918733e+00 -4.2717934e-01 -1.3395872e+00 -1.2163661e-01
   6.6863239e-01  7.8529799e-01 -5.0112242e-01  1.4146064e-01
   1.0362003e+00 -1.6132370e-01  3.6702642e+00 -1.1134183e+00
  -9.3284237e-01 -3.6201394e-01  3.9807639e-01 -3.9812467e-01
   9.3929100e-01 -1.7185133e+00  1.5245048e+00 -9.3498135e-01
  -6.9526047e-01 -8.0348802e-01  1.6857398e-03 -1.1528759e+00
  -2.6259632e+00 -1.1134439e+00  1.4856771e+00 -4.5999360e-01]]

Final Loss: 0.3969
Distance Metric: 38.7465
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 165

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.13019562 -0.00850443 -0.12304032 ... -0.11925045  0.1644211
   0.08625537]
 [-0.16750908  0.1650659  -0.10079674 ... -0.1703731   0.0237781
   0.17874931]
 [ 0.2813306   0.32740828  0.32821122 ... -0.02390574 -0.03067363
   0.01757555]
 ...
 [-0.11502448 -0.12329429  0.07236078 ... -0.0372914  -0.14746141
   0.06002601]
 [-0.08255967 -0.02238391 -0.00540065 ... -0.04647429  0.04126066
   0.02524934]
 [-0.00772084 -0.1350231   0.04535825 ... -0.06169116  0.07440029
   0.00168601]]
layers.1.weight: [[ 0.08607764 -0.08091477  0.0091402  ...  0.14228669 -0.24513452
   0.17644171]
 [ 0.15872148 -0.19534045 -0.13705534 ...  0.01015641 -0.2432073
  -0.1852632 ]
 [-0.22993042 -0.14471824 -0.03175558 ... -0.03219207 -0.1817818
   0.1712606 ]
 ...
 [ 0.10809754  0.18935719 -0.20751613 ...  0.06312259  0.11785946
  -0.20583054]
 [-0.24868165  0.1821784   0.06621128 ...  0.17715028 -0.02912331
   0.1348108 ]
 [-0.02629815  0.11018937  0.07846872 ...  0.03027093 -0.16826983
  -0.17309259]]
layers.2.weight: [[ 2.4549087e-04  3.3922247e-03  3.5278747e-04 -5.9444238e-03
  -2.0365166e-03 -1.6329258e-03 -3.5405604e-03  8.3100359e-04
   0.0000000e+00 -2.5082657e-03  3.7539182e-03  5.2858540e-04
  -2.0944035e-04 -2.8661813e-03  2.7598182e-03  0.0000000e+00
   2.2736982e-02  1.5085935e-02  7.4404255e-03 -8.0316776e-04
   3.1972018e-03 -5.0957513e-04  2.2249548e-03  1.7478348e-03
  -2.4557065e-03  1.0317089e-02  6.5989868e-04  3.7078327e-03
  -2.4553157e-02  8.0356133e-01 -4.8044356e-04 -2.4350008e-03
   2.4050198e-04 -1.3096372e-02  0.0000000e+00 -1.9694387e-03
   2.6627576e-03 -4.6658292e-01  3.3354026e-03 -7.8052152e-03
  -5.9243473e-03 -2.6473149e-03  2.7524596e-03 -1.0609927e-03
   1.5281114e-03  4.8820366e-04 -6.3759140e-03 -1.9225430e-02
   6.2717253e-04 -8.4209405e-03  4.6377480e-03  6.9062919e-03
  -2.3960264e-03  2.4645743e-03  8.5264974e-04 -1.2355658e-03
  -2.1157071e-03 -7.6873635e-04 -7.8835181e-04 -1.3740055e-03
  -1.0913367e-04  1.0817427e-03 -3.9414791e-04  7.7012833e-03
   4.9157720e-04  2.1164692e-03 -2.2020517e-03 -1.1611033e-03
  -1.7732804e-01 -3.9634011e-03  1.4288872e-03 -8.9608571e-03
   0.0000000e+00  0.0000000e+00 -4.6275876e-02 -6.4445398e-04
  -1.9625637e-03  2.2135577e-03 -1.2211197e-02  1.3500509e-03
  -1.7254223e-04  1.6995814e-03 -1.6472569e-03 -3.2018574e-03
  -1.2864269e-01  4.6337908e-03  4.3416158e-03  4.1516591e-03
   2.8158484e-03  3.6896069e-03  2.1624363e-04  9.0481725e-04
   7.8119889e-02 -7.6671600e-02  1.0474419e-03  3.0031783e-04
   5.6361314e-03  8.1739895e-04 -1.4117340e-02  1.2646409e-02
  -2.0727297e-02 -1.1353238e-04  3.7308617e-03 -2.0097161e-03
  -1.3799054e-03  9.7129785e-04  3.7750205e-01 -2.0645754e-03
  -2.6052448e-03 -2.5906247e-01 -6.3468255e-03  1.2261160e-02
  -1.8739968e-03 -9.3598210e-04  7.4827939e-02 -1.8529074e-02
  -4.5601949e-03 -1.3110026e-03  7.1951147e-04 -2.2230158e-03
   6.7263586e-04  9.6391544e-02 -2.7672725e-03  1.2677623e-03
  -2.1516262e-03  1.8663522e-03  8.3405263e-04  1.2820395e-02]]

Final Loss: 0.0000
Distance Metric: 27.3220
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 3
stopped after epoch: 1999

================================================================================

