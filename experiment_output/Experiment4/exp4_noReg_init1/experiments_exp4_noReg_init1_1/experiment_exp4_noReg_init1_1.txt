Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 0.9693861   0.33497155 -0.7205057 ]]]
layers.1.weight: [[[ 0.1573832 -0.2538954]]]
layers.2.weight: [[[-0.7754827  -0.42895728]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.01869127 -0.19642049  0.01255918 ... -0.03624617  0.00465951
   0.07240256]
 [-0.16095458  0.0595198  -0.11315392 ... -0.15726615  0.19907458
  -0.0952625 ]
 [ 0.00036765  0.19856945  0.11926153 ...  0.15320608  0.06136017
   0.07371645]
 ...
 [-0.06918336 -0.11017339 -0.01025626 ... -0.01992575 -0.12264073
   0.19664094]
 [-0.18139434  0.17216495 -0.11353807 ... -0.04654939 -0.00235358
   0.01768136]
 [-0.14166422 -0.20801514  0.01356756 ...  0.08315022  0.1097432
   0.03628017]]
layers.1.weight: [[ 0.10038859  0.08828969  0.07394552 ... -0.10220136 -0.14543724
   0.14977081]
 [ 0.01658772 -0.01096277  0.04760536 ...  0.10654034 -0.06487748
  -0.11297286]
 [ 0.12763387  0.04701189 -0.04515978 ...  0.14218725 -0.1298507
   0.13238265]
 ...
 [-0.02367287 -0.04463791 -0.05749165 ... -0.09531483 -0.01440372
   0.02970696]
 [-0.00735198  0.0477031   0.12097763 ... -0.09964847  0.14957507
  -0.08840825]
 [-0.01411255  0.00805672 -0.1492646  ...  0.05904505 -0.10179576
   0.00831937]]
layers.2.weight: [[-0.11899013  0.07565849 -0.16236804  0.12133145 -0.11090691 -0.06567132
  -0.00085744  0.06906354 -0.23992287 -0.09449487  0.13360834 -0.22470173
  -0.03249742 -0.12730823  0.06831095 -0.08100411  0.00607194  0.0678345
  -0.21850783  0.1953349  -0.19889343  0.08036642  0.09918551  0.08360369
   0.14647962 -0.00495976 -0.0263034   0.1585658  -0.1337263  -0.09393077
  -0.11619456  0.15668103 -0.00957651 -0.01435537 -0.12779927  0.0019084
  -0.12656178 -0.02082181  0.16615197 -0.21988557 -0.07488883  0.07717582
   0.13707893  0.12412553  0.01540186  0.07453188  0.07276078 -0.00984614
  -0.1532491  -0.15480366  0.16374622  0.0138546  -0.18743801  0.00657787
  -0.1864751   0.07310025  0.11183571 -0.18602924  0.17374615 -0.09214767
   0.02850281  0.03777231 -0.08186907 -0.11349452 -0.14503384  0.08445568
  -0.02658149 -0.1029602   0.0100058   0.17345305 -0.00760368 -0.19006073
  -0.12837528 -0.040724   -0.06050464  0.14339311  0.1402054   0.02006516
  -0.02834397 -0.12305867 -0.09116442 -0.00948063 -0.11209892  0.03155743
   0.08900553 -0.13434558 -0.07081707  0.13922983  0.01169358 -0.17919543
  -0.12529424  0.15173721 -0.23542705  0.10968401  0.03729397  0.18416762
   0.08656245  0.17008235  0.16659872  0.18668723  0.09259143 -0.12150981
  -0.1407363   0.10150042 -0.01047196 -0.05048195 -0.11312502 -0.14291668
   0.10633146  0.16707237 -0.1476649  -0.14338778 -0.02511172  0.02575169
   0.12683107  0.02240165  0.05217627 -0.02265927  0.07853978  0.10572097
  -0.05686005  0.07635246  0.0809418   0.17547658 -0.05502537  0.18311793
   0.01271068  0.17875464]]

Final Loss: 0.0000
Distance Metric: 18.8852
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.23483631  0.06645735  0.06748122 ... -0.01249906  0.11854562
  -0.15798809]
 [ 0.08742125 -0.0022212  -0.05738249 ...  0.20479742  0.08488321
  -0.08541797]
 [ 0.2377911   0.02495653  0.09922501 ... -0.10277213  0.15666805
  -0.07401722]
 ...
 [ 0.20670132  0.23381579  0.1784439  ... -0.1409822  -0.01652966
  -0.00702385]
 [-0.38517365 -0.00087183  0.04920667 ... -0.20356782  0.22523136
  -0.01921095]
 [ 0.09717206  0.10691078  0.001634   ... -0.01125234  0.0040256
  -0.01905528]]
layers.1.weight: [[ 0.23240064  0.06525242  0.06255973 ...  0.07413574 -0.25267735
   0.10086758]
 [-0.06834666 -0.11235785  0.00721302 ... -0.16268629  0.03562651
   0.09670653]
 [-0.2146468  -0.10228647 -0.01214875 ...  0.12466027 -0.2136627
  -0.13124827]
 ...
 [ 0.09940707  0.09110389 -0.32605568 ... -0.14753696  0.22431481
   0.01874328]
 [ 0.05070103 -0.02101801  0.08604276 ...  0.05044322  0.20071086
   0.02942473]
 [-0.01601092  0.01350684 -0.12382445 ...  0.04790417 -0.3126754
   0.11376414]]
layers.2.weight: [[-2.3661308   1.0936465   3.9362447   1.3350724   0.22308455 -0.5351633
  -0.68828636 -1.4049369  -0.20359477  0.41621    -1.708014    1.0642332
   0.16756952  0.62062293 -0.40249458  2.3492084  -1.8061494  -3.049696
   2.7573166  -0.5024391   0.01321846  1.5494614   0.7318612  -0.31293264
  -1.3526889   2.463252    0.05262274  0.42111722  1.9778093   2.2590315
  -1.5116308   1.2219173  -0.51650095 -2.8327765   0.2853867   1.471364
   0.03612146  1.0750816   1.3257501   0.84281605  0.6439859  -1.2274911
   3.9982035  -0.49398318 -0.24609002  1.5289382  -0.86673874 -1.3792953
   0.43187764  1.8545973  -1.0553237   0.752592   -0.80433047 -0.08512672
  -1.5967948   1.18199    -0.18117902 -0.61688226 -1.7725214  -2.4943645
   1.1582166  -1.116954   -0.6561018  -3.7266417  -1.670758   -0.5540704
   0.9067543  -0.2295636   0.72483134  0.22145112 -2.969189   -0.7279032
   0.61463237  0.624302   -0.86904377  0.59413534 -0.06694189 -0.7870821
  -2.0803046  -1.1268778   2.518438   -0.5773294  -0.58625656 -1.1636819
  -0.4358684   2.311574   -0.3545877  -0.6157229  -0.5995328   1.9481691
   1.9092634  -0.29629093 -0.73894674  1.0513701   0.9731817  -0.7227364
  -1.8862185  -0.8766487   1.1620997  -0.02666616 -0.8141645   0.58047694
   1.6601858  -0.18237521 -0.80830324 -2.21064     2.6678033  -0.50287855
  -1.1329759   1.1439995  -0.14984857  0.45930183  0.2696228   2.4908445
   2.4716964   2.0737963   2.484262    0.5715321   0.03522778 -2.083085
   0.901937    0.49360728  0.42661035  0.12594822  1.9877607  -1.3184459
   0.1901972   1.0773009 ]]

Final Loss: 0.0573
Distance Metric: 40.0547
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 151

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-4.24215309e-02 -1.54555142e-02 -4.07331325e-02 ... -1.08157815e-02
  -4.75719944e-02 -8.90740007e-02]
 [ 2.00945158e-02  1.28850983e-02 -2.31916625e-02 ...  9.57549643e-03
  -1.14415074e-02  2.86527327e-03]
 [ 7.98800230e-01  2.78799206e-01 -1.36299551e+00 ... -2.11319923e+00
   1.24073446e+00 -3.16227734e-01]
 ...
 [ 2.65326142e+00  3.27851892e-01  1.77176666e+00 ...  1.52120805e+00
  -1.08357227e+00 -1.63323894e-01]
 [ 1.21039343e+00  7.59743333e-01 -3.51360464e+00 ...  2.09262681e+00
  -8.75855029e-01 -2.76019430e+00]
 [-1.15707353e-01  1.46996295e-02  1.16228402e-01 ...  1.54775053e-01
   1.05614819e-01  2.15879187e-01]]
layers.1.weight: [[ 9.9036932e-02 -4.9641170e-02 -2.1016103e-04 ...  6.3432655e-03
  -5.1288274e-03 -1.0389241e-01]
 [ 1.4319246e-01  8.2886398e-02 -1.3498040e-02 ...  1.1710845e-02
  -5.1935720e-03  4.0249474e-02]
 [-6.4518237e-03 -1.1700920e-01  9.6526677e-03 ...  9.2536220e-03
  -1.2466884e-02  1.6560355e-02]
 ...
 [-4.1161865e-02  2.7297314e-02  2.9293228e-02 ... -7.5074178e-03
   1.5449857e-02 -1.0971200e-01]
 [-1.5912016e-01  2.1195208e-01  9.4133886e-03 ... -3.8153450e-03
  -3.5912087e-03 -1.9288890e-02]
 [ 7.7951662e-02  4.8191514e-02 -1.2820579e-02 ... -8.4052952e-03
  -1.2987457e-03  9.6602021e-03]]
layers.2.weight: [[ 0.00140552  0.00307768  0.0018073   0.00920427  0.00591128  0.00264495
  -0.00382822 -0.00876725  0.00408903 -0.00800023 -0.01459665  0.009305
  -0.0034216  -0.01595596  0.00328091  0.00476775  0.00475616 -0.00223172
   0.00651678  0.00904744  0.01096245  0.0133728   0.00111737 -0.00271567
   0.00290089  0.00332869  0.00651608  0.13090274 -0.00827489 -0.00992037
   0.00265018 -0.01291158 -0.00130248  0.00483979 -0.00151616 -0.00241366
  -0.00135825 -0.00228217 -0.00404611 -0.00339737  0.00930316  0.00683234
   0.00593456 -0.00055599  0.00139385  0.00060837  0.00358179  0.00074632
   0.00610438 -0.00592027 -0.0008976  -0.00518267 -0.00404163  0.0113928
  -0.00514929 -0.00688647  0.00386232  0.00614455 -0.00487304 -0.00346658
  -0.00145812 -0.00329676  0.00250465 -0.00828152  0.00688229 -0.00678365
  -0.00472028 -0.0088929  -0.0048469   0.00105155  0.00640388  0.0065834
  -0.00017556 -0.01261628 -0.01723092  0.         -0.00214283  0.00572482
  -0.00252606  0.0031733  -0.00754279 -0.0014559   0.00732318 -0.00432998
   0.00647471  0.00238665  0.00074109  0.00900856 -0.00029744  0.0023386
   0.01074553  0.00468729  0.00639822 -0.00019923 -0.0142131  -0.00040344
   0.01081357 -0.00475472 -0.00561769  0.01383555  0.00282855 -0.00739749
   0.00026203  0.01012918 -0.13713065  0.00224136 -0.002748   -0.00206912
   0.00403453  0.00587942 -0.00714374  0.00365618 -0.00192955 -0.00780851
   0.00079815 -0.00080235  0.00651409  0.00392847 -0.00394609  0.00360271
  -0.00718672 -0.00651276 -0.00032179  0.00451719 -0.00398751 -0.0041508
   0.00349662 -0.00431814]]

Final Loss: 0.2029
Distance Metric: 48.8053
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.02841065 -0.07004736  0.13084169 ...  0.01883556 -0.01747745
   0.14604555]
 [-0.03297798 -0.1104975   0.0156647  ... -0.4204962   1.0756317
   0.30336785]
 [-0.050164    0.07879075 -0.04604424 ...  0.1622473  -0.5563522
  -0.15051058]
 ...
 [-0.05373114  0.03512209 -0.16411784 ... -0.16903138  0.13871488
   0.19278687]
 [-0.11528245  0.3645319   0.06816768 ...  0.04503267  0.2729486
  -0.02214452]
 [-0.01967535  0.19699128  0.21669237 ...  0.04081225 -0.3630016
  -0.2134394 ]]
layers.1.weight: [[-0.07105919 -0.0459832   0.05358651 ... -0.087547   -0.15027787
  -0.1113501 ]
 [ 0.01575586 -1.0194855   0.734912   ... -0.09832121  0.0174642
   0.20910719]
 [ 0.04613192 -0.02648208 -0.12960619 ...  0.06430697 -0.13757305
  -0.03973107]
 ...
 [-0.08641673  0.08117322 -0.08003096 ... -0.03428841 -0.06381621
  -0.08815407]
 [ 0.08145651 -0.06468672 -0.04446962 ...  0.14537983  0.03129854
  -0.14933214]
 [-0.09481765  0.04618222  0.12963207 ... -0.09450579  0.17333366
   0.05037601]]
layers.2.weight: [[-0.34796023  7.4555745   0.18663281 -1.1756047  -2.1653693   0.07899073
   0.31328383 -0.1301735   1.3123279   0.5102972  -0.35873938  0.10602975
   0.8408297  -0.28792495 -0.15957698  0.93145704 -0.07491767 -1.0916417
  -0.5694481  -0.3753132  -0.7775251   1.9708112  -0.683602   -0.54593575
   0.41584206  0.9067189  -0.05321373 -0.5837766   0.7269296  -0.46415544
   0.43146247 -0.31606838 -0.23373045 -0.92210895  0.83292913  0.20538016
  -0.80951434  1.7594978   0.86519706 -0.81474686  0.29759967 -0.30638164
   1.428525    0.48355198 -0.68753904  0.28459153  0.6893861   1.4462427
   1.4004625   0.47759297  2.2777345  -0.5945139   0.60953104  0.22248456
   1.1151086  -0.35610834 -1.3880152  -0.11845919 -1.1805755  -0.2196116
  -0.43038055  0.9815206  -0.2971787  -0.40645474 -1.3174567   0.80523676
  -0.46770856  2.9544249   0.20777838  0.28530177 -0.1161058   0.5676783
   0.5362616   0.839717    0.06173318 -0.6231177   0.6029701  -0.55496776
  -0.6003421   1.9759922  -0.14670043 -0.43376568  0.35972914  0.7955703
  -0.57239425 -0.47317022  0.48881966  0.3155564   0.47787854  0.7889937
  -3.3402956   0.34063205  0.42799827  0.51589155 -0.9715414   0.14274617
   0.70657766 -0.64330655  0.6349194   2.2236943  -0.83287835  2.5308368
   0.04930138 -0.6021815   0.50521666  3.288131    0.48845476  1.1513292
   0.02323024 -0.6951525   0.95807546 -0.47824955 -1.7960328  -0.39079764
  -0.7865136  -0.40159217  1.2343496  -0.51743925  0.20879793 -0.18392782
  -1.434042    0.61896104  0.18751225  0.801622    1.3848284  -0.38712278
  -1.3062767   0.48374668]]

Final Loss: 0.4209
Distance Metric: 57.8090
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.04732804 -0.19040646 -0.37257224 ...  0.08013653 -0.31580648
   0.2139272 ]
 [ 0.04848346  0.031177    0.05369323 ... -0.17651299  0.09939998
   0.14240481]
 [ 0.11754546  0.01013117  0.08961596 ... -0.03631584 -0.01224
   0.00263675]
 ...
 [-0.00790379 -0.04722184  0.0860144  ...  0.055282    0.06353792
   0.20244688]
 [-0.11722305 -0.15010443  0.06702881 ... -0.3795015  -0.03117106
   0.05599576]
 [-0.21654601 -0.08084461  0.36531955 ...  0.27587518  0.01554662
   0.05183361]]
layers.1.weight: [[ 0.05330655  0.11235825  0.06219879 ...  0.16828194  0.10524042
   0.14388092]
 [ 0.06602994 -0.0372566   0.19800813 ... -0.05317483 -0.0987122
  -0.23639126]
 [-0.1293948   0.04242648 -0.05179145 ... -0.2317025   0.07442651
   0.09145257]
 ...
 [-0.05856502 -0.04192812 -0.06890041 ...  0.06063585  0.09749326
   0.03240753]
 [-0.15383527 -0.03906721 -0.15908785 ... -0.11706885 -0.07371032
   0.02693623]
 [ 0.0400931   0.11121383 -0.20956439 ...  0.13954656 -0.05560321
  -0.00734625]]
layers.2.weight: [[ 0.47240207  2.0244586   0.52585214 -0.46929145  0.7041305  -1.8677691
  -0.8092556  -1.8399068  -0.19803229 -2.6233795   0.35631773 -0.642967
  -1.0393866  -0.22752376  1.0397736  -1.4320209   1.1081663   0.13287075
  -1.334057    0.99349004 -0.51914674 -2.615288   -1.1948789  -0.53710854
  -0.84703386 -0.09073187  0.26903963  0.04737372 -0.7198372   0.5231743
   0.21869949  0.6847001  -1.3477252  -0.7878494   1.1909696  -1.8650479
  -0.14081    -3.1742804   2.6708865   0.46649826 -1.2419372  -0.69929856
  -3.277568    1.67344     0.81150323  0.15208833 -0.4222077  -0.19494395
   1.6808475  -0.9336052   2.1100793   0.5812573   1.7058405   1.7342613
  -0.6988948  -1.746066    3.092259    1.8565412   1.3666581   1.1136838
   1.5875384  -0.55311     2.4044263   0.72186273 -0.771886   -1.6497762
  -0.19848894  2.3425462   0.8310179  -0.12177289  0.84815073  2.4212706
  -1.1254247  -2.4701054   1.116412    3.6885788  -2.4560688   1.9024785
   1.8597822  -0.6042103  -0.06717753  0.84373546  1.6866517  -1.156231
  -0.46681884 -1.0744325   1.7030532  -1.8857398  -1.1947392  -0.8527106
   0.2106961   1.7417804   0.28878397 -0.4452917   0.7836017  -0.81319815
   0.9406085   0.37219542 -1.1870064   1.7991936  -0.82320386  4.215185
   0.01187497  0.2737409  -1.9458681  -0.84674156 -2.0959187  -0.89340067
   0.36007968  0.28107747 -1.3018006   0.8840214   0.26110324 -0.51903147
  -0.01893917 -0.14307736  1.5834621   0.4958715   0.73028314  0.70078856
   0.1697472  -0.04720461 -0.193679   -0.66324186 -2.0959408   0.7959173
   1.1019064  -0.38588828]]

Final Loss: 1.1425
Distance Metric: 39.5184
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 199

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.09995591 -0.14963995  0.17993526 ...  0.16049477  0.18913649
  -0.03232358]
 [-0.15411691 -0.00932166  0.17532693 ...  0.20043491  0.02758268
  -0.06693464]
 [ 0.03350974  0.1251598  -0.01128591 ... -0.11261722  0.10442211
  -0.06591016]
 ...
 [-0.06987122 -0.0225194   0.19153672 ... -0.06349465  0.02298981
   0.06102673]
 [ 0.14406534 -0.00589789  0.25397605 ... -0.12447961  0.17449075
   0.16170914]
 [-0.05306035 -0.13785024 -0.18505654 ... -0.1021586  -0.0667422
  -0.0463377 ]]
layers.1.weight: [[-0.15176252 -0.1489917  -0.15248726 ...  0.13731618  0.10290589
   0.1232415 ]
 [-0.07466064 -0.09591656 -0.13748692 ... -0.01642444 -0.06318468
   0.06641534]
 [-0.0069032   0.06132653 -0.01297754 ...  0.01345481 -0.03432144
  -0.02099492]
 ...
 [ 0.01068567 -0.14351067  0.01968785 ... -0.10968316  0.04281925
   0.14093582]
 [ 0.0911475   0.08375448 -0.10445164 ... -0.1425609  -0.10358208
  -0.14202861]
 [ 0.13467407  0.05470615  0.05296715 ... -0.07071932  0.09633815
   0.14999782]]
layers.2.weight: [[-0.04359455  0.01439965  0.03857255  0.0567816   0.14772463 -0.11754796
   0.01230431  0.20411383  0.05624826  0.27835232 -0.1824063  -0.210567
   0.16294776 -0.1496743   0.1530981   0.18457527 -0.04878009 -0.19517791
   0.22861476 -0.16744083  0.04365207  0.23828213  0.05646033  0.1034575
   0.12379752  0.16880769 -0.27809137  0.2719644  -0.17516612 -0.19852105
  -0.18191713  0.05812446 -0.07730598 -0.16049881  0.04318934 -0.01387855
   0.16819616 -0.27642035 -0.02223164  0.13584025  0.15118377  0.25899583
  -0.13853218 -0.23627953 -0.16745478  0.22245668  0.233135    0.12641369
  -0.12082432 -0.19878396 -0.16211116  0.12227902 -0.11386362 -0.24535994
  -0.21118967  0.02158844 -0.17454295  0.08705039  0.1735261  -0.11184015
   0.15771796 -0.12104293  0.07024641 -0.0694508   0.12151573  0.12507644
  -0.12904659  0.17468226  0.1722583   0.06210783  0.23198226 -0.066336
   0.064946   -0.19194353 -0.07445     0.20728388  0.29911804 -0.22744846
   0.15455155  0.16847098 -0.06806246  0.07599301  0.19558346  0.22350644
   0.2587747   0.0382076   0.22795604  0.03795284 -0.22171028 -0.17650601
  -0.18474728  0.10038511 -0.18030103 -0.19514629 -0.12295231 -0.12043951
  -0.01281922  0.11638544 -0.09736358 -0.03392932  0.02000101  0.18858899
  -0.08679406 -0.23149344  0.16551682 -0.05460038  0.17875245 -0.08825076
   0.19544373  0.15407991 -0.00649301  0.22521567 -0.125479   -0.07903098
   0.09529031 -0.08088738 -0.23483746 -0.15116264 -0.09616186  0.13453208
  -0.1363178  -0.01783742 -0.04986172 -0.16121915 -0.25136653 -0.00393653
  -0.22095963  0.21314844]]

Final Loss: 0.0000
Distance Metric: 20.6107
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.18241493  0.02282472 -0.17294319 ... -0.003103    0.05055652
  -0.28658643]
 [ 0.01807398  0.2405026  -0.13763134 ...  0.11673795 -0.16914043
   0.05719421]
 [ 0.21813853  0.05858028  0.15952604 ... -0.09170619  0.22111596
  -0.06070794]
 ...
 [ 0.1857577   0.2197855   0.13304633 ... -0.14598958 -0.08212632
  -0.02375174]
 [-0.36697862  0.03255747  0.09123701 ... -0.04411993  0.22161183
  -0.15677504]
 [ 0.01434381  0.22675236 -0.29096055 ...  0.1374181  -0.23152754
  -0.07939215]]
layers.1.weight: [[ 0.26398987  0.14561692  0.11836302 ...  0.07035027 -0.16468643
   0.15153539]
 [-0.08709185 -0.13986199 -0.01972329 ... -0.1574367   0.01246812
   0.04996372]
 [-0.24041688 -0.12438116 -0.01214875 ...  0.12911683 -0.23953992
  -0.19177926]
 ...
 [ 0.13554868  0.12944382 -0.33081496 ... -0.1598289   0.25387052
   0.05193017]
 [ 0.05115663 -0.0214395   0.08605789 ...  0.05492316  0.19872926
   0.02515429]
 [-0.0109179   0.0099201  -0.16766332 ...  0.04322277 -0.33094564
   0.08523469]]
layers.2.weight: [[-2.3966439   1.034316    3.9316664   1.350309    0.2074706  -0.5552437
  -0.7198267  -1.4188846  -0.23815653  0.4159651  -1.7138914   1.0231293
   0.1817822   0.58542675 -0.4028589   2.3205779  -1.7798463  -3.0111701
   2.7265503  -0.5130429  -0.03883929  1.541203    0.7105662  -0.3208602
  -1.3583106   2.4639719   0.0543921   0.42198968  1.979889    2.295401
  -1.5311754   1.2047533  -0.49985382 -2.8481743   0.2903522   1.5078511
   0.02351742  1.043549    1.3231285   0.85052186  0.6453361  -1.2637603
   3.9971113  -0.48402834 -0.30772188  1.4832329  -0.8943821  -1.354114
   0.36945578  1.8530087  -1.0907912   0.7778045  -0.78388846 -0.07016157
  -1.5823963   1.1499104  -0.23934178 -0.5795254  -1.8260239  -2.5419567
   1.1723369  -1.1284758  -0.63882333 -3.7164643  -1.6780664  -0.56560415
   0.90582144 -0.21015668  0.6771868   0.2518058  -2.9735193  -0.794625
   0.61345255  0.5607217  -0.8741282   0.57179177 -0.06223576 -0.8313454
  -2.0602965  -1.1209893   2.532313   -0.5678912  -0.58925545 -1.2063411
  -0.4381382   2.2881866  -0.38207412 -0.64213365 -0.58835953  1.9450611
   1.9030784  -0.2931026  -0.7522805   1.0356456   0.88907605 -0.7086655
  -1.8889947  -0.90271753  1.1582617  -0.03844606 -0.8022677   0.57303905
   1.7067991  -0.19090909 -0.8034638  -2.2001297   2.6631281  -0.55124176
  -1.1101531   1.1067815  -0.17013507  0.444617    0.23780002  2.4579859
   2.4890223   2.0693219   2.5405605   0.5448129   0.03013536 -2.091453
   0.87207735  0.51141137  0.444137    0.12229398  1.8966153  -1.3261554
   0.18309872  1.0637065 ]]

Final Loss: 0.0573
Distance Metric: 40.8494
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 132

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.05400671 -0.04446014 -0.00767352 ...  0.00149992 -0.09723321
  -0.00457142]
 [ 0.073363   -0.06591944  0.00330131 ... -0.00296226 -0.05669035
   0.01429065]
 [ 1.4688509   0.44514424 -1.2496355  ... -1.4073411   0.73932755
   0.23425889]
 ...
 [ 0.06825787 -0.01504402  0.02006044 ... -0.0511134   0.01126399
   0.02000847]
 [-0.05250863  0.06062675  0.00847093 ... -0.00656642  0.04628149
   0.00791701]
 [ 0.08447149 -0.04471551  0.00522582 ... -0.01775184 -0.0711653
   0.00442964]]
layers.1.weight: [[ 0.03982091 -0.00391512 -0.0028562  ...  0.04718374 -0.03644018
  -0.15417534]
 [-0.06342378  0.12323889  0.00606475 ... -0.1065155   0.138793
  -0.05356066]
 [ 0.01873087 -0.02121422 -0.01157724 ...  0.10336757 -0.0017015
  -0.04327509]
 ...
 [ 0.08116817  0.05768843 -0.00372051 ... -0.16471258 -0.02597415
   0.09655825]
 [-0.06491545  0.11342999  0.0027517  ...  0.03923294 -0.10596126
  -0.03951297]
 [-0.00416802  0.10399332 -0.00467933 ... -0.03044038 -0.05196301
   0.03657747]]
layers.2.weight: [[-4.23391815e-03 -2.00979388e-03 -7.47611979e-03  1.12104710e-04
   2.91719357e-03 -2.05179956e-03 -2.28526117e-03  1.96973071e-03
  -3.48203321e-04  4.11566254e-03  3.30170686e-03 -1.58164883e-03
   5.46530774e-03  1.98192871e-03  1.23693980e-03  0.00000000e+00
  -6.43264875e-03  6.14451198e-03  5.39270369e-03  3.14894714e-03
   1.86652702e-03 -2.41123652e-03  1.42263877e-03 -1.53851148e-03
   0.00000000e+00  1.52932946e-03  7.59148085e-03 -9.45630446e-02
   7.13077257e-04  3.93468048e-03  1.18772564e-02  6.40677987e-04
  -8.74127168e-03 -1.39565521e-03 -4.65543941e-03 -8.32864782e-04
   2.83022411e-03  2.63576792e-03 -2.72434601e-03  1.07742322e-03
   4.82252054e-03 -1.07807275e-02  1.57954986e-03 -4.78462130e-03
  -1.63357303e-01 -4.06266376e-03 -2.17431178e-03  1.59882987e-03
  -1.53060583e-03  3.33662634e-03  1.04653707e-03  2.83288979e-03
  -5.53065538e-03 -1.38231122e-03  3.69651173e-03  4.44801711e-03
  -1.36778248e-03  6.87609520e-03 -1.20633678e-03  2.91641918e-04
   2.40542716e-03 -7.13569112e-04 -1.86225108e-03  6.66606938e-03
   7.66981300e-03 -3.21468455e-03  6.33228512e-04 -5.02154324e-03
   6.31476706e-03  3.48190987e-03  4.95912973e-03 -3.18710762e-03
  -5.14872558e-03 -3.14138108e-03 -4.56317002e-03 -9.02278256e-03
  -1.21795470e-04 -9.21674888e-04  4.57067479e-04 -2.01453338e-03
   3.38858063e-03  7.24532269e-03  3.45815113e-03  0.00000000e+00
   3.58333229e-03  4.63767583e-03  5.10619767e-03  3.82829737e-03
   1.14789419e-03  0.00000000e+00  2.45144474e-03 -1.64297549e-03
  -3.22418148e-03 -6.52242079e-03 -2.46841949e-03 -1.97888166e-03
  -3.55170551e-03 -4.94112819e-03  6.93011144e-03 -5.99863566e-03
   6.16522552e-03  1.27305218e-03  4.28083306e-03  9.77207557e-04
  -9.83590074e-03 -8.70083086e-03  1.18066644e-04 -8.37473664e-03
   2.00127112e-03 -2.40198057e-03  1.25827519e-02  3.73419956e-03
  -4.17716196e-03 -3.05238995e-03 -7.80372089e-03 -1.06095122e-02
   6.18434604e-03 -3.17583606e-03  6.55588834e-03  4.30292543e-03
  -5.59859956e-03  1.63848337e-04 -6.01717131e-03  5.79400873e-03
  -3.25507298e-03  9.56867822e-03  5.50290430e-03  6.19804440e-03]]

Final Loss: 0.2329
Distance Metric: 50.0133
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-1.88831799e-02 -1.16491839e-01  1.32030740e-01 ...  2.03574467e-02
   2.18911041e-02  1.56060010e-01]
 [-4.72774438e-04 -1.18042566e-01 -3.21587287e-02 ... -4.27804440e-01
   1.06156743e+00  3.00178975e-01]
 [-1.71952751e-02  5.78138269e-02 -3.99290472e-02 ...  1.58939302e-01
  -4.93991941e-01 -1.74583018e-01]
 ...
 [-6.65790662e-02  5.17732427e-02 -1.35835275e-01 ... -1.60380155e-01
   1.45705938e-01  2.04856381e-01]
 [-1.29525632e-01  3.41654360e-01  2.80133393e-02 ...  2.88261436e-02
   2.34276697e-01 -2.83945687e-02]
 [-9.12219379e-03  1.22390278e-01  1.90095767e-01 ...  5.98931536e-02
  -4.64958549e-01 -2.31553152e-01]]
layers.1.weight: [[-0.06912396 -0.05007421  0.05211034 ... -0.08815236 -0.15977664
  -0.11026809]
 [ 0.00404724 -0.9967792   0.7972425  ... -0.08229075  0.03200842
   0.21943527]
 [ 0.04549002 -0.02349693 -0.13135567 ...  0.06619713 -0.13048093
  -0.04426978]
 ...
 [-0.09620593  0.05475156 -0.07255384 ... -0.04499774 -0.08152029
  -0.08627336]
 [ 0.11149828 -0.02677969 -0.12137953 ...  0.15656555  0.08802307
  -0.12798168]
 [-0.10001507  0.04035385  0.12255692 ... -0.08982848  0.18973516
   0.03098218]]
layers.2.weight: [[-3.44422847e-01  7.63460016e+00  1.96427211e-01 -1.23306811e+00
  -2.25381446e+00  1.10841811e-01  2.73501843e-01 -1.19466506e-01
   1.27476370e+00  5.28519511e-01 -3.76159102e-01  7.55619407e-02
   8.71264100e-01 -2.85389543e-01 -1.63275316e-01  9.62137282e-01
  -7.85605833e-02 -1.09089923e+00 -6.60851419e-01 -3.88506889e-01
  -7.73773432e-01  2.02630138e+00 -7.52037168e-01 -5.26141346e-01
   5.69468141e-01  8.86286199e-01 -3.77727859e-03 -5.32833159e-01
   7.22834349e-01 -4.42330718e-01  3.91421586e-01 -3.52240652e-01
  -2.21171156e-01 -9.33804870e-01  8.22273970e-01  2.89495796e-01
  -7.55811036e-01  1.73290920e+00  7.60460794e-01 -8.15971732e-01
   3.46116483e-01 -3.06620657e-01  1.42382336e+00  4.97799933e-01
  -6.82040513e-01  3.16477478e-01  6.35657966e-01  1.50059533e+00
   1.30627728e+00  5.24282753e-01  1.82239568e+00 -5.23860693e-01
   6.11146331e-01  2.99478441e-01  1.14608777e+00 -3.57753724e-01
  -1.53823483e+00 -1.06319994e-01 -1.22548616e+00 -1.54211015e-01
  -3.96277726e-01  9.73011136e-01 -3.12483102e-01 -4.13161576e-01
  -1.36136329e+00  8.83289754e-01 -4.79620337e-01  2.85018015e+00
   2.34260574e-01  2.84207791e-01 -1.34151563e-01  6.19010270e-01
   5.31358540e-01  8.92670095e-01  5.85249066e-02 -6.12585127e-01
   6.03168070e-01 -3.05605233e-01 -5.69447577e-01  1.94499171e+00
  -1.77365825e-01 -4.27224427e-01  3.61677885e-01  7.72314072e-01
  -5.10990679e-01 -5.19590080e-01  5.61089456e-01  3.22652012e-01
   5.10600150e-01  7.49525428e-01 -3.45510530e+00  3.64368200e-01
   4.47591811e-01  5.61881661e-01 -8.92105520e-01  1.30559847e-01
   6.62440360e-01 -6.42570615e-01  6.71958923e-01  2.29011083e+00
  -8.22576225e-01  2.52301002e+00  2.00704895e-02 -6.45036638e-01
   4.87047672e-01  3.21267390e+00  6.93635583e-01  1.11345506e+00
   5.03442623e-02 -6.44288361e-01  9.33378518e-01 -4.17862862e-01
  -1.82069623e+00 -4.10080492e-01 -7.27567554e-01 -4.15635854e-01
   1.15260088e+00 -4.77770299e-01  2.14834690e-01 -1.77850112e-01
  -1.34200037e+00  6.84466779e-01  2.33249292e-01  7.42557347e-01
   1.31283998e+00 -3.07272494e-01 -1.46231616e+00  5.10604501e-01]]

Final Loss: 0.4143
Distance Metric: 57.9790
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.01234867 -0.13215323 -0.21399738 ...  0.17270893 -0.24063788
   0.0776251 ]
 [ 0.02181429  0.05116517  0.07169537 ... -0.14839432  0.06893305
   0.15626958]
 [ 0.03418167 -0.13423426  0.10189918 ... -0.04235781  0.03683272
  -0.00453781]
 ...
 [ 0.07388172 -0.06363481  0.03870705 ...  0.09484275  0.08769142
   0.16003323]
 [-0.09123945 -0.25266454  0.0691512  ... -0.2958775  -0.12002361
   0.06140977]
 [-0.1592678  -0.15475306  0.2960546  ...  0.3357243  -0.02605728
   0.13981713]]
layers.1.weight: [[ 0.05146405  0.11251357  0.06219319 ...  0.17048232  0.11187579
   0.14683826]
 [ 0.05487809 -0.01032003  0.2234364  ... -0.00375125 -0.0949882
  -0.220926  ]
 [-0.13385786  0.04028986 -0.05062457 ... -0.2341324   0.07370925
   0.10382545]
 ...
 [-0.0652636  -0.03981163 -0.0538988  ...  0.08263636  0.10482109
   0.05008143]
 [-0.17941067 -0.02524244 -0.15009406 ... -0.09021984 -0.04959086
   0.04885482]
 [ 0.03766747  0.11197058 -0.20691179 ...  0.12788783 -0.05990101
  -0.01089614]]
layers.2.weight: [[ 0.4768349   2.0618646   0.52774173 -0.470463    0.73726034 -1.8715383
  -0.7958663  -1.7864903  -0.08552612 -2.61398     0.3555076  -0.59691554
  -1.0185938  -0.17335778  1.0387552  -1.3804997   1.1002722   0.16237225
  -1.3389673   1.0217952  -0.5049463  -2.6181512  -1.1541953  -0.5088442
  -0.84268594 -0.05557267  0.2562895   0.0429924  -0.6992771   0.5254973
   0.21903425  0.6812121  -1.3268194  -0.76879287  1.1750401  -1.8576468
  -0.11208498 -3.1559827   2.6798542   0.4727166  -1.2457893  -0.701451
  -3.2715433   1.683884    0.80927676  0.14553139 -0.40388596 -0.18994369
   1.6830198  -0.92017925  2.111796    0.6251334   1.721362    1.737548
  -0.69825083 -1.7060488   3.1010559   1.8688688   1.3579111   1.1173807
   1.5591424  -0.5291074   2.409571    0.7038299  -0.7505034  -1.6287686
  -0.17340851  2.3595448   0.83088297 -0.08486214  0.90203446  2.5181873
  -1.1288557  -2.4617345   1.1095674   3.707732   -2.4467137   1.9104013
   1.8455994  -0.5850624  -0.02721776  0.8536634   1.7053795  -1.1572084
  -0.45068645 -1.0743246   1.7859746  -1.83584    -1.1914612  -0.8444349
   0.2905207   1.7402227   0.29984692 -0.4484843   0.81451154 -0.8154895
   0.9473407   0.3653549  -1.1595176   1.821019   -0.81521094  4.215185
   0.03921609  0.2975721  -1.9452583  -0.83065706 -2.0948007  -0.8738229
   0.41013712  0.2994285  -1.2552743   0.8744053   0.3023991  -0.44093534
  -0.01455884 -0.13567227  1.6324441   0.49728462  0.7353209   0.70534146
   0.17726474 -0.04282421 -0.22070879 -0.66855216 -2.0917284   0.8300796
   1.1691946  -0.36151993]]

Final Loss: 1.1329
Distance Metric: 39.3130
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 117

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.16245261  0.09016245 -0.16428533 ... -0.16675399  0.27855116
   0.20482394]
 [-0.32565784  0.2367841  -0.08472865 ...  0.30884352 -0.31473085
  -0.3232466 ]
 [ 0.07476398  0.32871562 -0.00742687 ...  0.27070805 -0.15429738
  -0.06222826]
 ...
 [ 0.14235252  0.03309404 -0.2410008  ...  0.2229788  -0.16290854
  -0.2664534 ]
 [-0.05434804 -0.0174039   0.08022477 ... -0.24822684  0.13347082
   0.10552358]
 [ 0.2433752  -0.19902319 -0.05652    ... -0.12676136 -0.06902551
  -0.06897057]]
layers.1.weight: [[-0.01705097  0.15398452  0.18867259 ... -0.24959604  0.05052584
   0.1258966 ]
 [ 0.01594186  0.17365561  0.26088414 ...  0.2264782  -0.13092092
   0.17353694]
 [ 0.19278775 -0.23837861  0.03630028 ...  0.10088725  0.07289247
  -0.03956947]
 ...
 [ 0.07961538 -0.08315606 -0.11535461 ...  0.12273956 -0.25281683
  -0.1752967 ]
 [ 0.17905545 -0.12479753  0.18285933 ...  0.0181402  -0.14901929
  -0.17992242]
 [ 0.22963512  0.09301459  0.04990804 ... -0.23116215  0.04008401
   0.10059553]]
layers.2.weight: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0.]]

Final Loss: 0.0000
Distance Metric: 31.4858
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 214

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.66842276  0.09339261  0.5018026  ...  0.12357558  0.03305184
   0.10747052]
 [ 0.25355387 -0.2072072  -0.23984922 ... -0.08785657 -0.03226591
  -0.26496905]
 [ 0.20701957 -0.03378028 -0.12048552 ...  0.11004236 -0.02375149
  -0.14284006]
 ...
 [-0.17283843  0.06166619 -0.0616198  ... -0.16354321  0.18466373
  -0.0595515 ]
 [-0.954084    0.25474828  0.63948995 ... -0.09915133 -0.08830054
   0.01612977]
 [-0.00331517 -0.02818469  0.06370176 ... -0.01754821  0.09146249
  -0.11124783]]
layers.1.weight: [[-0.08057715 -0.06205645  0.05939729 ...  0.04435526 -0.11846486
  -0.14498398]
 [ 0.04286435 -0.12383527 -0.02592068 ... -0.0757274   0.12733215
  -0.23321112]
 [-0.04059913  0.11169085 -0.12644248 ...  0.08962088 -0.08514882
   0.11133312]
 ...
 [ 0.40353796 -0.01699478 -0.23395081 ...  0.15279587  0.20059428
   0.13511685]
 [-0.02836665 -0.1391791   0.02600679 ... -0.078484   -0.11438916
   0.02671014]
 [ 0.12759082 -0.0526838   0.01637941 ... -0.06996937  0.12217746
   0.08012806]]
layers.2.weight: [[ 1.13466904e-02 -5.83152175e-01  1.28941238e-02  6.46917045e-01
  -4.72493060e-02  1.53136909e-01 -5.11045933e-01 -2.24745646e-01
   2.25935072e-01 -7.59272635e-01  2.17966929e-01  6.22337982e-02
  -1.02899396e+00 -1.87079355e-01  1.31726190e-01 -3.45037371e-01
  -3.09944868e-01 -2.28256509e-01 -9.76850837e-02 -2.53505766e-01
   2.46068627e-01 -2.40484506e-01  9.49809924e-02 -4.09274012e-01
  -2.77163424e-02 -5.12343049e-01 -3.44015092e-01 -2.85199076e-01
  -1.51238397e-01 -1.72805339e-01  3.98161449e-02 -5.19222856e-01
  -2.95065880e-01  4.52550203e-02 -5.24559975e-01 -1.62452161e-01
   2.47518361e-01 -4.38782513e-01 -1.19562340e+00  1.18657686e-01
  -5.69356740e-01 -5.84669895e-02  1.17120579e-01  1.18189968e-01
  -3.25773746e-01 -4.80031163e-01 -2.18290612e-01 -3.86671662e-01
  -1.21575341e-01 -2.22103104e-01 -6.75593764e-02 -1.63732082e-01
   8.65546390e-02 -2.99794316e-01 -7.15474665e-01  6.96250573e-02
  -8.19136575e-02 -7.35527694e-01 -4.79989588e-01 -6.82140291e-02
  -6.44629061e-01 -1.49716079e-01  1.91875243e+00 -2.85554290e-01
  -6.98175967e-01  1.68808237e-01 -8.40048566e-02  1.37359962e-01
  -1.28978118e-01 -9.02780984e-03 -4.94666040e-01 -1.03089154e-01
  -5.72621869e-03 -9.57263634e-02 -6.46707594e-01 -8.81615937e-01
  -1.70324370e-01 -7.49565423e-01 -5.64744547e-02 -7.04538465e-01
  -4.78415489e-01 -2.84232378e-01 -1.51216574e-02 -8.27804506e-01
  -3.62467051e-01 -7.50498027e-02  2.27921620e-01  3.01359802e-01
  -3.54522079e-01 -3.58795434e-01 -1.12418607e-01 -1.21003844e-01
   1.42045483e-01 -1.83758661e-01 -3.30689549e-01  5.89551963e-02
  -5.96283853e-01  4.94249254e-01 -8.30092251e-01  4.34645802e-01
   8.39635849e-01 -3.15533549e-01  1.42891169e-01  7.41230190e-01
  -2.99782187e-01 -1.29055172e-01  7.85469413e-02 -1.33708194e-01
   2.10088968e+00  3.49539876e-01 -4.48310852e-01 -2.01099694e-01
  -6.76125407e-01  2.08244085e-01  1.38666667e-02 -2.98495352e-01
   9.78896916e-02 -6.25070143e+00  2.79004663e-01 -2.58613229e-01
   6.41169250e-01 -2.46583804e-01  1.78089533e-02 -5.38108587e-01
  -6.64282963e-02 -3.59501433e+00 -3.12623799e-01  2.34944597e-01]]

Final Loss: 0.0649
Distance Metric: 40.8901
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.1808092   0.05522442 -0.2429095  ...  0.01672669 -0.01084083
  -0.29789722]
 [-0.00868393  0.27888808 -0.17728402 ...  0.1287942  -0.2661374
   0.07201637]
 [ 0.2238808   0.04981085  0.17912844 ... -0.08691527  0.2472867
  -0.05599059]
 ...
 [ 0.19021754  0.22372392  0.12360311 ... -0.14009136 -0.08299156
  -0.02674686]
 [-0.34551752  0.02057588  0.11136195 ... -0.06129613  0.2806311
  -0.17726216]
 [-0.01956269  0.310617   -0.36198395 ...  0.16945511 -0.3172129
  -0.11588366]]
layers.1.weight: [[ 0.29032105  0.16158478  0.1795902  ...  0.08561865 -0.15689614
   0.18219043]
 [-0.10003743 -0.14824627 -0.05252826 ... -0.16524939  0.00829412
   0.03179638]
 [-0.2383401  -0.11916296 -0.01214875 ...  0.12911683 -0.23582496
  -0.19990808]
 ...
 [ 0.1323744   0.13145779 -0.32751566 ... -0.1598289   0.2557139
   0.05483987]
 [ 0.05115663 -0.02234806  0.08469459 ...  0.05495702  0.19822164
   0.024017  ]
 [-0.02601411  0.00173776 -0.19875695 ...  0.03582238 -0.33325362
   0.06708937]]
layers.2.weight: [[-2.4125853   1.0243804   3.931922    1.3494824   0.19719712 -0.5667893
  -0.7235028  -1.4177138  -0.2463007   0.41378453 -1.7237463   0.99084204
   0.17297618  0.5745157  -0.4186927   2.3088367  -1.7847854  -3.023934
   2.718249   -0.5180855  -0.03678907  1.5422144   0.6941233  -0.31875923
  -1.3564771   2.4639719   0.03706494  0.40839475  1.9774052   2.292821
  -1.529818    1.1957854  -0.5017149  -2.8620768   0.282006    1.5082783
   0.02081333  1.0157285   1.3147876   0.8451958   0.63620585 -1.2789851
   3.9871662  -0.48479086 -0.30813953  1.4626446  -0.90680134 -1.3583021
   0.35415697  1.8528883  -1.1003432   0.7709781  -0.7910184  -0.07818899
  -1.5914559   1.130483   -0.2508221  -0.5848064  -1.8426431  -2.561257
   1.1726683  -1.1276954  -0.64091164 -3.7249444  -1.677078   -0.57189375
   0.90597904 -0.22081606  0.67812365  0.24933006 -2.9720364  -0.81920964
   0.58999044  0.5504884  -0.87530345  0.56972444 -0.06963218 -0.84682477
  -2.0647607  -1.1231459   2.529645   -0.5790814  -0.5904736  -1.2233407
  -0.44286188  2.282757   -0.40893438 -0.6574499  -0.58814013  1.9341301
   1.894462   -0.2907545  -0.7588913   1.0371884   0.85069674 -0.71249014
  -1.8925008  -0.9108406   1.1562958  -0.05370459 -0.80180573  0.57046145
   1.6800426  -0.19089408 -0.80407614 -2.201738    2.661025   -0.5751168
  -1.1101108   1.0873332  -0.16903056  0.446525    0.23602861  2.4585376
   2.485585    2.0616693   2.5438235   0.5190406   0.02493741 -2.0967207
   0.86645126  0.50190824  0.43231958  0.12258181  1.8720886  -1.3269311
   0.17759842  1.0469147 ]]

Final Loss: 0.5863
Distance Metric: 42.3477
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 120

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.02338808  0.02108719  0.01407869 ... -0.22658134  0.34975848
  -0.49643242]
 [ 0.11979513 -0.01561375 -0.0456409  ... -0.05201906  0.254228
  -0.1225424 ]
 [ 0.21964522  0.00381671 -0.0417043  ... -0.15829691  0.11894342
  -0.02819836]
 ...
 [ 0.03511814 -0.11581038  0.1366246  ... -0.04892295 -0.00721771
   0.03208311]
 [ 0.00747256  0.01647323  0.01755282 ...  0.1394014  -0.21848516
   0.24269721]
 [-0.17534631 -0.26381785 -0.00405561 ... -0.03446725  0.10053071
   0.25728345]]
layers.1.weight: [[ 0.01498113  0.18337463  0.08116316 ...  0.01767931  0.19735347
   0.2614378 ]
 [-0.06255314 -0.12797982 -0.18983528 ...  0.19493105 -0.2405462
   0.02887371]
 [-0.02358465 -0.1990575   0.0548384  ... -0.18327448  0.02355236
  -0.22755504]
 ...
 [ 0.27775523 -0.00737082  0.01639502 ... -0.0078062  -0.15394495
   0.05512381]
 [-0.01704387 -0.20181008  0.11926606 ...  0.24304685  0.08512566
   0.05969168]
 [-0.05140468  0.11733824 -0.17549674 ...  0.10744232  0.24828137
  -0.04769052]]
layers.2.weight: [[ 1.1935211e-03  2.2899506e-03 -1.2779455e-03  2.8083262e-03
  -3.4961402e-03  2.6565020e-03  3.0987409e-03  3.8225472e-04
  -4.8107252e-04  1.8253442e-03  5.7276431e-03 -6.2906840e-03
   1.8506218e-04  1.6875105e-04 -3.1479392e-03  0.0000000e+00
  -8.4187108e-04  2.7214570e-03 -1.3493844e-02  5.0345347e-03
   2.3055991e-02 -7.1260310e-03  0.0000000e+00 -8.3363953e-04
  -3.0727708e-03  5.2941395e-03 -1.5097280e-03 -1.4689503e-03
  -1.5192959e-01 -2.6030017e-03  1.2421819e-02  1.8483574e-03
   1.0689333e-03  9.9435309e-03 -1.4721436e-03  1.6981042e-03
  -1.8424514e-03 -4.3498171e-03 -1.9258505e-03 -4.7000946e-04
   1.7523159e-03  5.9331227e-03 -4.5082606e-03  2.0430055e-03
   2.8930299e-04 -1.9887779e-03 -6.9919354e-03  5.1663662e-03
   0.0000000e+00  7.9696532e-03  3.3158103e-03 -1.0185650e-03
   1.5738966e-03  0.0000000e+00  3.4943123e-03  1.6432094e-03
   6.6733680e-04  5.5678212e-03 -2.7898481e-04  3.0143775e-02
  -5.4455902e-03 -3.7620282e-03 -1.6973128e-04  4.8967903e-03
   3.8232503e-04  9.5188810e-04  2.4411993e-03 -1.9398505e-02
   2.2915283e-03 -1.7275026e-02 -2.0946711e-02  4.1836116e-04
   3.4222973e-03  4.0330429e-02  5.1026912e-03 -3.3089174e-03
   2.7520227e-04  5.8561459e-02 -9.6733932e-04 -3.7351874e-01
  -2.7464019e-04 -8.4171147e-04 -3.3034724e-03  9.2356786e-04
   2.8750959e-03  1.1756172e-01 -8.5877784e-04  3.9582743e-04
  -1.7997982e-01  3.4675171e-04  5.7057310e-03  8.3133590e-04
  -4.3155937e-03  0.0000000e+00  2.1149939e-02 -7.9951616e-04
  -1.3589964e-02 -1.3368017e-03 -1.9766756e-03  1.3762265e-03
  -2.2320952e-03  7.4734702e-03 -1.1374307e-03 -2.6677799e-04
   4.6951119e-03 -1.5004009e-02  1.4793951e-03  4.1753508e-04
  -3.9111031e-04 -2.3229069e-03  4.8207408e-03  5.6584919e-04
  -2.7075809e-04  1.8004121e-03 -5.8857042e-01  8.2209408e-03
   9.1862294e-04  1.8275091e-02  7.7791663e-04 -4.0888897e-04
   8.3724177e-04 -1.6366370e-03 -5.1903405e-04  4.5827762e-03
   0.0000000e+00 -4.9617568e-01  3.6522390e-03 -7.0233624e-03]]

Final Loss: 0.0000
Distance Metric: 26.2738
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1999

================================================================================

