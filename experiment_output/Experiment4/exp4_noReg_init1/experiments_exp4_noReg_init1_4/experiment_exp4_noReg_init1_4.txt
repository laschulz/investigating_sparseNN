Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.1087358  -0.5448544  -0.00490475]]]
layers.1.weight: [[[-0.9195585  0.5580388]]]
layers.2.weight: [[[ 1.0112202 -1.1653095]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.15937276 -0.06738863  0.12221298 ... -0.11815449  0.00864569
   0.0987625 ]
 [-0.08800153  0.21169503  0.09901825 ... -0.13941516  0.20895466
   0.12526692]
 [-0.01498311  0.04189128 -0.0511442  ...  0.08557309  0.10227472
  -0.06869015]
 ...
 [ 0.16520962  0.0607068  -0.0190013  ... -0.00644981  0.05366455
   0.09387555]
 [-0.05376458  0.10991216  0.02793113 ...  0.01559433 -0.03730353
  -0.00327805]
 [-0.12874867 -0.22695693 -0.02225434 ... -0.08071246 -0.16479854
   0.16770163]]
layers.1.weight: [[-0.12144671  0.11090909  0.06197333 ... -0.02932817  0.13148011
  -0.03564198]
 [ 0.10533281 -0.12737809 -0.11647686 ... -0.11846433  0.10828175
  -0.08135679]
 [-0.02503828  0.15016381  0.01461762 ... -0.14275752 -0.11026058
   0.13447335]
 ...
 [ 0.0871671  -0.03420445  0.14398399 ... -0.12466256  0.06320611
  -0.04752042]
 [ 0.0094904  -0.10615387  0.12379695 ...  0.1164131   0.02359582
   0.00127697]
 [-0.03196097  0.13578524  0.01680632 ...  0.1486345  -0.02315162
   0.08355299]]
layers.2.weight: [[ 0.12114731  0.17409366  0.15817213  0.17508031  0.09324247 -0.07981693
  -0.15114477  0.12534876  0.03977551  0.07224318  0.0049352  -0.06102121
   0.04473012  0.15963908 -0.11546536 -0.17389168  0.14466172 -0.12714408
   0.161665   -0.06110775 -0.17980754  0.17977367  0.04326152  0.11446173
  -0.17143565 -0.20585936  0.14013343  0.1877682  -0.16615878  0.09842741
   0.03383955  0.05191725  0.14246123  0.05199917 -0.06857266 -0.07009017
   0.06185744 -0.06158858 -0.06341462  0.14595859 -0.09875578  0.02118035
   0.00851632 -0.03565655  0.10670493 -0.20762624 -0.19285597  0.06991655
   0.19971909  0.13139409  0.03149556 -0.09761444  0.00328514 -0.011966
   0.11992689  0.07589126  0.12096551 -0.19501546 -0.12605377 -0.21486107
  -0.09861156 -0.01657398  0.07646298  0.0518667  -0.2052739  -0.13966134
  -0.01907986 -0.07191231  0.13957348 -0.05942358  0.1635263  -0.18683006
  -0.10673504 -0.01626509  0.09016665 -0.0172923   0.00372137 -0.00904934
  -0.18624504 -0.1870883   0.14688754 -0.16261205 -0.14197071 -0.04547471
   0.15920275 -0.07871027 -0.05183041  0.08433343  0.09696034  0.20662864
  -0.12781698  0.15769918  0.13312325 -0.22331168  0.03922965 -0.04915396
   0.0574428   0.11633247 -0.21711485 -0.1669436   0.08058896 -0.20500061
   0.07285425  0.12673987 -0.02056474  0.12465589 -0.12916149 -0.10714962
  -0.19001742 -0.00638541 -0.11299165 -0.1749218   0.17497894  0.17706373
  -0.01513277 -0.00054741  0.06784735 -0.08175898 -0.05277504  0.14648135
  -0.05705401  0.12712695 -0.20384099  0.09489789  0.02129725  0.0648914
   0.14532006  0.01010088]]

Final Loss: 0.0000
Distance Metric: 19.7641
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.237593    0.0959095   0.1436721  ...  0.2451356   0.13557468
  -0.05725796]
 [-0.04893773  0.00948657  0.16827145 ... -0.18333289 -0.01312699
   0.0613693 ]
 [-0.14427595  0.03172105 -0.23036264 ...  0.07422873 -0.05780241
   0.10170311]
 ...
 [-0.04963292 -0.15177122 -0.08526406 ...  0.16307247 -0.05214014
   0.14992633]
 [ 0.31975347  0.09415028  0.26099783 ... -0.01492229 -0.05315049
  -0.03231093]
 [ 0.02098766 -0.15225832 -0.01434569 ...  0.07710987 -0.0719943
  -0.11645496]]
layers.1.weight: [[-0.1714722  -0.03542988 -0.11774268 ... -0.25212106  0.01188156
   0.00068754]
 [-0.00437473 -0.14751677  0.0730052  ... -0.02663467 -0.03233337
   0.04619922]
 [ 0.09400728 -0.18971378  0.04342525 ...  0.07133745  0.01802534
  -0.16290408]
 ...
 [ 0.24335493  0.03063946 -0.20559482 ...  0.07136241 -0.23404625
  -0.24436511]
 [ 0.00435133  0.02063596  0.23640881 ...  0.09599571 -0.16837336
  -0.1165321 ]
 [ 0.08738998  0.14911129 -0.01280793 ...  0.15062156  0.19015895
  -0.00086484]]
layers.2.weight: [[-0.51441014 -1.6156512  -0.17118795  0.0360168   2.1396282  -2.502194
   1.5940534   0.5119242  -1.6050823  -0.1575248   1.1245546  -0.10734552
   1.915716   -0.37134814 -0.43222675  0.02570834  0.14073268  0.27911946
   1.057949    2.7103703   1.0880469  -2.7357962  -2.0410578  -1.3611922
  -0.8040006  -1.7349023   1.2448071  -0.31728384 -1.5312061  -2.9543667
  -3.3572533  -0.8241641  -0.9589296  -0.27854744  0.379846    2.657087
  -0.28554454 -0.72711796 -0.39062497  0.03270716  0.9186997  -0.71968484
   1.5483828   0.47651342  1.2202244  -1.4481752   0.69426405  1.7024112
  -1.1046474  -1.6902841  -1.1242837  -0.13734266  1.3709453  -0.12966162
  -2.3806353  -0.15652381 -1.1670805  -0.84574276 -0.86785924 -0.9427463
   0.07664683  0.96531546  0.85141414 -0.07790606 -0.48805502  0.03068259
   0.13030098 -2.0159738  -0.73532367 -2.8525007  -0.405088    0.5121952
  -2.0582678   0.23325863 -1.8458581  -2.3347585   1.1032834  -1.196494
   0.6299456  -0.38790402  1.9611295  -0.89164555  0.12959701 -0.76018625
   1.0348907  -2.189275   -2.7251165  -4.088112   -1.4638342  -1.1795744
   2.764409    0.18946058  0.76579595  0.24306248 -1.9450105   0.38314238
   2.7800193   0.30262277 -0.14673284 -2.321197   -0.99679047  1.5075338
   0.6320087   0.3962973  -0.3528124   0.31787598 -1.0965257  -1.2729201
  -3.0351994  -0.94656503 -0.6093429   0.7623023   0.8385217  -0.09382711
   0.59464115 -2.14225     0.9742219  -2.268424    1.5943946  -0.59264773
   0.98183596 -0.12132588  1.9448436  -0.14947663  0.624683    2.227933
   1.7439148  -0.40968612]]

Final Loss: 0.2276
Distance Metric: 37.9017
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 176

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.08923739  0.02044169  0.0275853  ... -0.04929623 -0.0161565
   0.08679461]
 [ 0.01714052 -0.03214357  0.02375328 ...  0.05423747  0.03952687
   0.07955706]
 [-0.07292566 -0.00140592  0.01218105 ...  0.02785061 -0.02842539
   0.11483309]
 ...
 [-0.04420503 -0.00387522  0.01672553 ...  0.0118386  -0.00091894
   0.08245704]
 [-0.03874446  0.0093428  -0.0004144  ...  0.01770029  0.00324191
   0.10553993]
 [-0.0094473   0.01307014 -0.01311877 ... -0.03824789 -0.03652481
  -0.06514841]]
layers.1.weight: [[ 0.16885331 -0.01258965 -0.09216519 ... -0.09293843 -0.02608262
  -0.08254258]
 [ 0.06102477 -0.13224041  0.06083924 ...  0.1409014   0.01672867
   0.03786608]
 [ 0.06159044 -0.16366737 -0.1109945  ...  0.13152227 -0.17125112
  -0.00879396]
 ...
 [ 0.05900179  0.07168877  0.06301279 ...  0.09837279 -0.00574845
   0.14359981]
 [-0.01213727 -0.11417573  0.05992359 ...  0.02494993  0.1252494
   0.06545055]
 [ 0.02926426  0.02499343 -0.00918105 ... -0.1333793   0.00859119
   0.02442889]]
layers.2.weight: [[ 0.00512736  0.00013613  0.00282345 -0.00428461  0.00482214  0.01112832
  -0.00805559 -0.01123051 -0.00292464 -0.01061181 -0.00210823  0.00027053
   0.00301773 -0.0061219  -0.00640819 -0.00159966  0.00766379  0.01020015
  -0.00608599 -0.00017628  0.00395135  0.00448188  0.00721985  0.00034514
   0.0013689   0.00491724 -0.00634573  0.00018894  0.00390743  0.0057496
   0.00639571 -0.02099327 -0.00143461 -0.00015399 -0.0002102  -0.00071698
  -0.00224933 -0.01760423  0.0001327  -0.00261238  0.00773184  0.00402405
  -0.00686208 -0.00076387 -0.00182125 -0.00319828  0.0031959  -0.00019296
   0.00147441  0.00013638  0.00243674 -0.01409377  0.00332324 -0.0017597
   0.00567011  0.00361095  0.00168508  0.00437403 -0.00322967  0.00817572
   0.00469973 -0.00254376  0.00684152 -0.00213449  0.00126763 -0.00910713
   0.00225562 -0.00050051 -0.00239639 -0.00628191  0.00438682  0.0029717
   0.0050777  -0.00543969 -0.00179261 -0.00031354 -0.00505358  0.00316317
   0.00787207 -0.00380225 -0.00438227 -0.00297387 -0.01101176 -0.00567869
  -0.01204822  0.00519799 -0.00579134  0.00640774  0.00203059 -0.08281072
   0.00162247  0.          0.00148371  0.0009879   0.01116971 -0.02362901
  -0.00235533  0.01159495  0.00169551  0.00013294 -0.01184781 -0.00626513
   0.00248266 -0.00447579 -0.00338684  0.00114323 -0.00754321 -0.01497747
   0.00052637  0.00763596  0.00809834  0.00342016 -0.00367949  0.00019013
   0.00209471  0.00686621 -0.00226191 -0.00240336 -0.00787876  0.00874977
  -0.00254745 -0.11594294 -0.00747488 -0.00297258 -0.00270708 -0.01171099
  -0.00042072 -0.00149918]]

Final Loss: 0.1877
Distance Metric: 38.9098
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.15535511 -0.38746735 -0.04440447 ...  0.79391015 -1.5850264
  -0.10401559]
 [ 0.6196718  -1.1993467  -0.0627512  ...  0.7473397  -1.4285322
  -0.0971007 ]
 [-0.03489722  0.03776576  0.06685739 ...  0.2625533  -0.6103372
   0.02197229]
 ...
 [ 1.4581873  -2.755721   -0.16077845 ... -0.3362166   0.6505426
   0.01817539]
 [-0.06472851  0.01354711 -0.01494861 ...  0.40920398 -0.8153582
  -0.126814  ]
 [-0.1192914   0.19782749  0.00638601 ...  0.7173219  -1.3297569
  -0.04557575]]
layers.1.weight: [[ 0.07226343  0.02233343 -0.09913381 ...  0.0985124  -0.03348622
  -0.02182529]
 [-0.02378648 -0.07864007  0.03825446 ...  0.12454943 -0.09257814
   0.1383947 ]
 [ 0.01800282 -0.03142725 -0.07481053 ...  0.12373276  0.1400447
  -0.04255361]
 ...
 [-0.08764987 -0.04281961  0.12422547 ...  0.08775482 -0.04780008
  -0.08796635]
 [-0.41246426  0.4645766   0.00062385 ...  0.56721485 -0.08341731
  -0.18216415]
 [-0.15182434 -0.06398474 -0.05372629 ... -0.00549978 -0.14115341
   0.05845169]]
layers.2.weight: [[ 0.24914226  0.03146886  0.47330004 -1.0299908   0.14598581  0.5718397
  -0.5923464  -0.20991145  0.18187125 -0.2895692   1.6093372   0.1502051
   2.7608128   1.7865363  -0.02107815  0.91653556  1.0242559  -0.33325908
   1.5355532   0.7337675   0.4573979  -0.1600345   0.0765314  -0.50147223
  -1.5955434  -0.06809566  1.4114301  -0.02103144 -0.06809283 -0.60618895
  -0.05570223 -0.14455567  0.17024717 -0.3296165  -2.643807   -0.32279295
   0.13684727 -0.27403384 -0.06920783  0.02825471  0.11369383  0.18128192
  -1.7637297   0.22160992 -1.0270127   0.0462536  -1.1082363   0.10089077
  -0.1272351  -0.02930207  1.3118403  -0.4864357  -0.49512953  0.39792183
  -0.3089063   0.33920485 -0.1295325   0.09296971  0.5178974  -0.35487866
   0.26138645 -0.42272586 -0.43572295  0.07453717 -0.92881864  0.47749135
   0.37458104 -0.9655269   0.6887129  -0.11216722  0.27338403  1.7527314
  -0.03388986  7.0991893  -2.2137017  -0.01304103  0.02858687  1.451606
   0.98474854  0.458967    0.2087502  -0.33210942 -0.16237012 -1.6276947
  -0.0618807   0.47627664 -0.11654745  0.82239264 -3.2959325  -0.3098107
   0.7280182   5.5765452   0.2161857  -0.5211673  -0.33249563  3.6443374
  -0.8517828   1.0643305   0.59389764  0.9963535   1.8589418   0.88491625
  -1.5953331  -0.14885527 -0.5564095   0.46178687  0.35240823 -0.21033385
   1.0243465   0.21440451 -2.256323   -0.44666123  0.5024179  -0.15782428
  -0.545314   -0.5944947   0.03277262 -0.39819372  0.01475074 -0.9521396
  -0.38112774 -0.12337288  2.392906    0.6599608   0.01600545 -0.15348768
   2.6153069   0.03574444]]

Final Loss: 0.0815
Distance Metric: 57.0963
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.21149361  0.17028888  0.32804096 ...  0.20754403  0.09374912
  -0.08388664]
 [-0.03027141 -0.05076451  0.02440329 ...  0.02441697  0.01714844
   0.04751131]
 [-0.00148999 -0.07672777 -0.07790854 ... -0.06139446  0.01424138
  -0.33557326]
 ...
 [-0.01542477  0.10646991 -0.16083017 ... -0.1341142  -0.24127597
   0.00178495]
 [ 0.07226107  0.09333991  0.06280084 ...  0.0521034   0.02772097
  -0.00763461]
 [ 0.1542709   0.0111491  -0.0899767  ... -0.2398345   0.21002543
  -0.07482418]]
layers.1.weight: [[ 0.23065408  0.21780501 -0.03275246 ...  0.11220018  0.02889857
   0.03414769]
 [-0.0886151   0.03823824 -0.10286655 ...  0.04775154  0.04954465
  -0.03652256]
 [-0.14260782 -0.14959739 -0.08868879 ... -0.10670132 -0.09772117
   0.02958093]
 ...
 [ 0.07840768 -0.02983385  0.2668869  ... -0.15295507  0.1547997
   0.2065635 ]
 [-0.05196922  0.22976553 -0.03894459 ... -0.20698021  0.02051482
   0.11428508]
 [-0.01248256  0.07964985  0.1051342  ...  0.02311868  0.18028808
  -0.13902685]]
layers.2.weight: [[-1.7595211  -1.1671425   1.4260725  -0.6175674   1.7049522   3.482004
  -0.19721769  0.09159871 -0.47059852  0.06075364 -2.1288514  -0.4799734
  -0.20114218 -0.518223    0.03453569 -0.8043712   1.5214589  -0.79148054
   2.056339   -1.3304008  -1.9923806   1.2322454  -1.8704244  -0.97616524
   1.7864444   0.11604995 -3.4454415   2.3976076   0.24641311  1.6503276
  -0.19885291  0.88524747 -1.865024   -0.7714577   0.03034296 -0.9093011
   0.45304057  0.3716306   1.049497   -1.106346   -1.2049866   0.59640986
   0.2724797   0.741702    2.020957   -1.2593476   0.0380365  -1.2039138
   1.8173127  -1.727055    1.8830491  -0.13756539  0.82669926  0.9904332
   1.4239721  -0.49156964  0.15402842 -0.875466   -1.3353103   0.12745868
  -0.61806595  1.1044222   1.0693976  -2.1085532  -0.9566152   1.4243431
   1.2276064  -0.71842563 -0.5369369   1.1819042   2.908929    1.9540977
   0.48994792 -1.917373    1.5442802   1.0684342  -1.7391119  -0.4910874
  -0.3224159  -1.5340124   1.8787899  -0.14221548  0.72552484  0.57045525
  -1.5687493   0.8279442   0.73721665  2.7711623   0.5860798   1.0028864
  -1.4096215  -0.32165512 -0.58267236  1.6865045   1.4256272  -0.41109994
   0.6627545  -1.4381552  -0.8379041  -0.5327028   2.7195234   0.05582579
   0.63670236  0.85058546  0.01339737 -1.8352422  -1.7002455   2.0951824
  -1.5402062  -2.9013991   0.58488876  0.95610356 -0.09608176 -0.16095185
   1.5933232   0.29003817 -1.0157337   0.35292512 -1.887012   -0.83018136
   1.9551274   0.9341483  -0.4534548  -0.5381616  -1.022761    0.6497296
  -0.59918517 -0.36624214]]

Final Loss: 0.4495
Distance Metric: 37.8839
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 312

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.25293165  0.09359109  0.11470682 ...  0.02803284 -0.29331544
   0.0978635 ]
 [ 0.12229669  0.18095791 -0.01697237 ... -0.20547111  0.08946943
   0.00760849]
 [-0.05229945  0.31212097 -0.15219656 ...  0.18122765 -0.06490914
   0.04202095]
 ...
 [-0.27812192  0.18041773  0.04679479 ...  0.2907527   0.12771939
  -0.16759479]
 [-0.32450253 -0.0177137  -0.17830108 ... -0.265204   -0.1795543
   0.03008546]
 [ 0.07813122  0.2553133   0.04235848 ... -0.10623636  0.13403195
   0.1336814 ]]
layers.1.weight: [[-0.24806361  0.22919628  0.08697562 ... -0.01460731 -0.03829541
   0.20826732]
 [ 0.2041201   0.21881975  0.10539071 ...  0.17574759 -0.18151002
  -0.03203697]
 [ 0.15589792 -0.17135404 -0.20457675 ...  0.08170809 -0.08691581
  -0.17390318]
 ...
 [-0.01504693 -0.06310397  0.13691473 ...  0.04679781 -0.11562075
  -0.21603239]
 [ 0.11811709 -0.08883929  0.18751818 ... -0.05177195 -0.1808005
   0.22753955]
 [-0.22533578 -0.03203082  0.06620754 ... -0.09221855  0.03090457
   0.23769678]]
layers.2.weight: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0.]]

Final Loss: 0.0000
Distance Metric: 28.6553
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 243

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.10450305 -0.10892142  0.12310722 ... -0.09882814 -0.11016636
  -0.13909851]
 [-0.01149147  0.695198    0.04064827 ...  0.14712605  0.02614487
  -0.00206138]
 [-0.04283516  0.40847713  0.10703228 ... -0.133491    0.00574823
   0.02080001]
 ...
 [-0.04892214 -0.25882715  0.06409791 ...  0.09728803  0.01251898
   0.16820848]
 [-0.0905355   0.17578684 -0.13488679 ...  0.14797565  0.06425297
   0.05534102]
 [ 0.04306172 -0.4870604  -0.26349127 ... -0.06846762  0.10191255
  -0.06053577]]
layers.1.weight: [[ 0.13924704 -0.15587878 -0.03460595 ...  0.14482757  0.08790895
   0.08218289]
 [ 0.09340853  0.00245508 -0.15594412 ... -0.14879785 -0.14728329
   0.04440375]
 [-0.05404947 -0.11985639  0.1400951  ... -0.06499171 -0.14053053
   0.11816414]
 ...
 [-0.03158697  0.0301532   0.03456792 ... -0.00672069 -0.06060232
  -0.16274714]
 [ 0.27956963  0.22328547  0.07752317 ... -0.0477946   0.09192656
   0.05905399]
 [ 0.08608002 -0.03063642  0.04976833 ...  0.13088274  0.08071014
   0.09135623]]
layers.2.weight: [[ 9.08133462e-02  1.67696610e-01 -4.22077626e-02 -1.32800579e-01
  -1.56500173e+00  1.91890523e-02 -2.69534606e-02 -1.31213963e-01
  -6.93608403e-01 -1.01075485e-01 -2.16018975e-01 -6.17114566e-02
  -9.57379341e-02  3.64535779e-01  1.36271611e-01 -3.19274276e-01
  -1.79565609e-01  3.26519728e-01  4.23011303e-01 -1.62525618e+00
  -3.26739460e-01  2.07819492e-01 -1.41737103e-01 -2.80491561e-01
  -2.89587467e-03  6.71696007e-01  1.72634244e-01 -1.88279554e-01
  -2.65810341e-01  1.07487880e-01 -4.41745967e-01  3.49032164e-01
   9.58953239e-03 -5.73189735e-01 -2.84927249e+00 -3.94523561e-01
  -6.76823109e-02 -1.88509345e-01 -2.13987494e+00  1.17934950e-01
   4.31013018e-01  2.42812447e-02 -2.20940873e-01 -4.88877892e-01
  -1.68695793e-01  3.14643055e-01 -2.06533760e-01 -5.18747509e-01
   9.38305855e-02 -1.14500545e-01 -2.39292860e-01  6.67481065e-01
   1.36377160e-02 -3.59363079e-01 -3.80897343e-01 -3.99847150e-01
  -1.43742859e-01 -1.12634575e+00 -1.24360573e+00 -6.79531544e-02
  -5.26584005e+00 -1.45011857e-01 -4.13726836e-01  5.67896217e-02
   3.04927766e-01 -4.65372443e-01 -3.15206274e-02 -3.88761103e-01
  -5.92529893e-01 -1.69959739e-02 -5.89066803e-01 -4.17777777e-01
  -1.74047679e-01 -2.34070614e-01 -1.26396380e-02 -9.84536856e-02
   9.55759734e-03  1.97069198e-01  1.24180742e-01 -1.02300674e-01
  -3.54932278e-01 -1.55512416e+00 -3.62872124e-01  1.27363145e-01
  -2.97497809e-02 -3.99310321e-01  5.39460294e-02 -3.64721626e-01
  -1.14999436e-01 -4.85416263e-01 -2.34102383e-01 -3.99181902e-01
  -7.18647242e-02  4.91118401e-01 -5.30610979e-02 -3.86829287e-01
   3.05322140e-01  2.70376474e-01 -1.43338084e+00 -3.61357927e-01
  -1.23433568e-01 -1.15575030e-01 -2.15814412e-01  2.93394595e-01
  -3.18059176e-02 -1.73914444e+00 -5.03492475e-01 -8.02706122e-01
  -1.94759727e-01 -3.86814140e-02  3.00971139e-02 -1.42840415e-01
   3.44856866e-02 -7.47413874e-01  3.52929711e+00 -1.26552570e+00
  -2.30252311e-01 -2.07495451e-01 -3.95951301e-01 -1.92057177e-01
   1.24145404e-01  1.42762393e-01  1.26250654e-01  2.56927069e-02
  -2.48200595e-01 -6.67989075e-01 -1.20457923e+00 -5.99744618e-01]]

Final Loss: 0.1135
Distance Metric: 41.9220
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[-0.2355274   0.10591581  0.13931009 ...  0.24858603  0.13553914
  -0.05364986]
 [-0.04405677  0.00588428  0.16332458 ... -0.18675156 -0.01847838
   0.05449013]
 [-0.15982592  0.04956748 -0.22730665 ...  0.05574324 -0.06739508
   0.07786646]
 ...
 [-0.04054688 -0.17156097 -0.08502237 ...  0.16426814 -0.03189973
   0.14768904]
 [ 0.31574613  0.09926915  0.26708087 ... -0.011193   -0.05199154
  -0.02623832]
 [ 0.02236721 -0.1510154  -0.02043328 ...  0.07551861 -0.07902262
  -0.11367598]]
layers.1.weight: [[-0.17141987 -0.03306125 -0.11758143 ... -0.25553557  0.0150987
   0.00067247]
 [-0.00437473 -0.14751677  0.0730052  ... -0.02663467 -0.03233337
   0.04619922]
 [ 0.09400728 -0.18971378  0.04342525 ...  0.07133745  0.01802534
  -0.16290408]
 ...
 [ 0.24137145  0.02442068 -0.20731787 ...  0.0817439  -0.23945406
  -0.24317503]
 [ 0.00418871  0.01990092  0.23575838 ...  0.10057711 -0.16613233
  -0.11730366]
 [ 0.08638985  0.14997908 -0.01520138 ...  0.14848588  0.1909847
  -0.00094049]]
layers.2.weight: [[-0.51575285 -1.6156512  -0.17118795  0.04142897  2.1390154  -2.5018804
   1.5961269   0.5104302  -1.6020558  -0.15883043  1.1245546  -0.10809743
   1.9135907  -0.37056848 -0.43268904  0.02759528  0.14047831  0.27923045
   1.0575019   2.7079592   1.0899162  -2.7350047  -2.0408032  -1.3647752
  -0.8022632  -1.7363551   1.2445198  -0.32379732 -1.5333678  -2.951776
  -3.357689   -0.8249539  -0.95853776 -0.27630302  0.3770006   2.6594636
  -0.28503683 -0.7255691  -0.39039245  0.0314945   0.921713   -0.7186169
   1.5502121   0.4783117   1.2200818  -1.4470102   0.69450223  1.7018178
  -1.1047608  -1.6902841  -1.1218902  -0.13761382  1.369109   -0.12966162
  -2.3783066  -0.15595108 -1.1676215  -0.84333515 -0.8667486  -0.94264543
   0.07666416  0.9658367   0.85322964 -0.07502093 -0.48640817  0.03155479
   0.13468337 -2.0165067  -0.7341544  -2.8525007  -0.40495116  0.515337
  -2.0602307   0.23148607 -1.8455685  -2.3360465   1.1095425  -1.1932982
   0.63008165 -0.3879053   1.9591826  -0.89396274  0.1299019  -0.75721866
   1.0375637  -2.188068   -2.726482   -4.088112   -1.4636077  -1.1800008
   2.7655182   0.18960181  0.76688844  0.24440226 -1.9435508   0.38383433
   2.7821023   0.30262277 -0.14673284 -2.3208568  -0.99261457  1.5082227
   0.6350784   0.39361784 -0.3554568   0.3173929  -1.0974891  -1.2717727
  -3.0349271  -0.94656503 -0.60924834  0.7645602   0.8416428  -0.09353826
   0.5955656  -2.139337    0.97956765 -2.2683303   1.597614   -0.59175026
   0.98164785 -0.12071409  1.9469538  -0.14881699  0.6261673   2.229642
   1.7476192  -0.40767246]]

Final Loss: 0.4084
Distance Metric: 39.2326
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 215

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[-0.13203488  0.19120198  0.23731646 ... -0.10898203 -0.19005029
  -0.10027023]
 [-0.00264214  0.00633842  0.00207638 ... -0.00637235 -0.00298481
  -0.00340027]
 [-0.08057652  0.09956076  0.01056743 ...  0.30101326 -0.13989593
   0.00807721]
 ...
 [ 0.25251108  0.06955615  0.219674   ... -0.16778818  0.051376
   0.07415661]
 [-0.24469183 -0.22619666  0.09388363 ...  0.12513255  0.23678127
   0.25176913]
 [-0.05341954  0.2309223  -0.18700233 ...  0.13916142  0.1536447
   0.04048553]]
layers.1.weight: [[-0.12590994  0.05138206  0.14074458 ... -0.21810125 -0.11585862
   0.08926287]
 [ 0.10346129 -0.11769955  0.2174862  ...  0.07240599  0.05950986
  -0.05628119]
 [-0.06250751  0.24054597 -0.16066733 ...  0.08327805 -0.21466051
  -0.13257223]
 ...
 [ 0.00763907 -0.10216513 -0.01376433 ... -0.05250241 -0.05536216
   0.00259564]
 [ 0.02641284  0.10100824  0.2519366  ...  0.15333423  0.11446002
   0.1300013 ]
 [-0.17912966 -0.1787079  -0.10996807 ... -0.16117847  0.1256014
  -0.23067415]]
layers.2.weight: [[-4.2541143e-03  8.7675033e-03 -3.5319719e-04 -8.1449496e-03
  -8.8282814e-04  1.0335087e-03  3.3406576e-03 -3.1477518e-04
   2.8032117e-04  1.7385293e-03  0.0000000e+00 -5.0638271e-03
   6.3786581e-03 -1.3040840e-03 -2.6240249e-04  3.0628184e-04
  -2.0506860e-04 -5.5315788e-04 -2.3099829e-03 -7.4245594e-04
   0.0000000e+00 -3.0984038e-03 -1.4068809e-03 -7.2616893e-03
   4.3352597e-04 -1.4530084e-03  3.0610135e-02 -3.6687627e-02
   1.1469482e-03 -3.7037066e-04 -6.4164732e-04  2.0010881e-03
  -8.6599076e-04 -3.6588418e-03 -3.6902266e-04  6.7251553e-03
  -3.4983363e-03  1.6763338e-03  1.1510058e-04  3.2768457e-03
   0.0000000e+00  2.6807009e-04 -2.0488201e-02  1.3143665e-04
  -1.8771484e-03  6.0144019e-01  2.0925789e-01 -1.8145776e-03
  -6.6569988e-03  0.0000000e+00  6.3833608e-03 -2.5192255e-02
   1.8571791e-03  2.6128883e-03  8.8895840e-04  4.2313766e-02
   3.5791434e-03 -2.0980965e-02  4.0691404e-04 -1.2182330e-01
  -6.5510203e-03  3.0212663e-02 -6.2792473e-03 -6.3849881e-04
   1.7704170e-02  3.6759931e-03 -1.8248252e-03  5.7853543e-04
  -3.9417753e-03 -4.5231686e-04  1.3541951e-04 -2.2969609e-02
   9.8287044e-03  3.3079401e-01  1.0909634e-04  1.7459202e-03
   5.6367177e-03 -8.4950193e-04  1.6736424e-03  1.0780692e-03
   8.9883490e-04  4.8184025e-04  8.1589734e-03 -3.5903421e-03
  -5.7813772e-03  6.0005113e-03  4.8112287e-03 -8.5313234e-04
  -1.4403593e-03 -3.2415055e-03 -1.4201467e-03  8.6577429e-04
  -4.2577703e-03 -1.3961903e-04  9.2351846e-03  2.2494132e-04
   1.2319046e-03 -1.4774242e-01  2.7381089e-03 -4.5359749e-03
  -6.0025994e-03 -4.9840653e-04  7.0969656e-04 -1.6716906e-04
   2.2699448e-01  7.4373267e-04  7.5710309e-04  0.0000000e+00
  -2.4814152e-03 -7.5787638e-04 -5.3062914e-03  9.8489365e-04
   3.2316463e-04  6.9293537e-04  1.4393432e-02  4.7362214e-03
  -9.9616479e-03 -3.5739643e-04 -3.9707520e-04 -5.2557630e-04
   6.8955804e-04 -1.2084709e-03  1.4489292e-03 -5.9446075e-04
  -1.3086800e-03  3.5706931e-01 -2.1362568e-04 -3.0739099e-04]]

Final Loss: 0.0000
Distance Metric: 27.0883
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 4
stopped after epoch: 1999

================================================================================

