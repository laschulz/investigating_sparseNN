Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[-0.6215098  -0.81731844 -0.466581  ]]]
layers.1.weight: [[[0.0390192  0.02942358]]]
layers.2.weight: [[[0.4453551  0.76907396]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.03103721 -0.06414997 -0.04152171 ...  0.13562816 -0.14486845
   0.02726232]
 [ 0.03369081 -0.06926701 -0.15366895 ... -0.17577061 -0.08593093
  -0.10167892]
 [-0.10997726 -0.03339209 -0.17444386 ...  0.20612021 -0.13783187
   0.03666729]
 ...
 [ 0.01307854  0.12579384 -0.11952268 ...  0.17669305  0.08348583
  -0.14344732]
 [ 0.1995769  -0.05724885 -0.17143956 ...  0.10094348  0.09072959
  -0.04345384]
 [-0.05047089 -0.08110539  0.15744781 ...  0.05712673 -0.12500297
   0.06126361]]
layers.1.weight: [[ 0.03608666  0.03221745 -0.03147018 ...  0.09635151  0.04496834
  -0.00428543]
 [ 0.05776503 -0.03958196  0.01985406 ...  0.00135927 -0.01346992
   0.09354725]
 [-0.00851775  0.1428927  -0.11861522 ... -0.10278928  0.01325132
  -0.08552781]
 ...
 [-0.12651117  0.12407023 -0.01765449 ... -0.05701008 -0.14830142
   0.04900433]
 [ 0.00599265 -0.13547051 -0.07253887 ... -0.08098855  0.02178311
  -0.08283584]
 [-0.01910499  0.14485854 -0.0924429  ...  0.11990918 -0.01735859
  -0.00174709]]
layers.2.weight: [[-0.0832794   0.01985738  0.0348395   0.03011563 -0.1594569   0.00996547
  -0.01732236 -0.14565781 -0.09788223 -0.01666604 -0.06286968  0.00822679
   0.16322042  0.10526734 -0.02013905 -0.0005013  -0.18595651  0.12818767
  -0.03646238 -0.10350659  0.01430069  0.04896068  0.16899392  0.1520361
   0.1690385  -0.08075134 -0.04117307 -0.18647338  0.08391129  0.16291767
   0.03649165  0.06123972  0.21571174  0.1534989  -0.09127694  0.00121047
  -0.16024843  0.17006284 -0.11585667  0.14574184 -0.02921865 -0.1731202
  -0.12711525 -0.11916078  0.01269034 -0.11172705  0.16009612  0.11202975
   0.09770335  0.18813263 -0.13424076  0.20577589 -0.1853789  -0.01635327
   0.12320966  0.03624683  0.07247264 -0.08800956 -0.16662095 -0.07532791
  -0.15080392 -0.06067074  0.16513164  0.12188666  0.13968232  0.11764782
  -0.00581687  0.04729098 -0.19496718  0.18764827  0.17780656 -0.19348913
  -0.02986686 -0.00654826 -0.16818176  0.0255899  -0.08909063 -0.05827244
   0.1433055  -0.04843666  0.01471178 -0.04812751 -0.17202444 -0.09012058
   0.1436604   0.18288033 -0.09936996 -0.19621071 -0.04678211 -0.19399598
  -0.11198229  0.18988867  0.07817514 -0.10563946 -0.01915905 -0.03422833
   0.08148491  0.02617082  0.0671271   0.09321877  0.00171361  0.07537162
   0.02942531 -0.09392575  0.00980352  0.08331644  0.08841062 -0.00659873
   0.18052386  0.12094018 -0.03298495 -0.07556912  0.12767646  0.07209645
  -0.06737005 -0.15580024  0.00615353  0.13913895 -0.16596027  0.00946712
   0.19281286  0.13311683  0.05983474  0.07914715  0.07083138 -0.01816612
  -0.07466664 -0.16087689]]

Final Loss: 0.0000
Distance Metric: 18.5546
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.10513364 -0.05218481  0.0842529  ...  0.08148552 -0.06404971
  -0.23327121]
 [ 0.06552741 -0.01716982  0.00936678 ...  0.02851463  0.27771375
   0.24841866]
 [-0.18879177  0.06138289 -0.12908821 ... -0.04504949  0.2754766
  -0.0941952 ]
 ...
 [ 0.02387173  0.0999855   0.11315248 ... -0.20873977 -0.14538622
  -0.06567343]
 [ 0.22452979  0.00749321 -0.31183973 ...  0.19388556  0.14390668
  -0.1473978 ]
 [-0.02124912 -0.0471754   0.27697176 ... -0.08289984 -0.1040795
  -0.07041217]]
layers.1.weight: [[-0.07264192 -0.1140503   0.05473055 ... -0.10050427  0.069962
   0.05460728]
 [-0.04765549 -0.32913202 -0.01013914 ... -0.0992619  -0.20520028
   0.09020127]
 [-0.22542435 -0.0322584  -0.271693   ... -0.03025716  0.06285365
  -0.09269425]
 ...
 [-0.33551547 -0.00175823  0.1439694  ...  0.109616   -0.04394435
  -0.30377632]
 [ 0.10823325  0.03431097  0.2175854  ...  0.14130367  0.00281053
   0.12700221]
 [-0.14959818 -0.2469827  -0.2158084  ... -0.0638503  -0.20946948
   0.16373965]]
layers.2.weight: [[-0.14620617  0.20363972 -0.90405023 -0.51645434  1.8327812   0.17628871
  -0.25339907 -1.3950499  -0.09425732 -1.3155826  -1.2516278  -2.0642586
   0.65652657  0.5939581  -0.5361361   1.7066541   2.413085    1.7084748
  -0.26580232  0.6207749  -1.0479898   0.8936315   0.07409421 -0.36942232
  -0.13896264  1.3543092  -1.7442086  -0.1758389   0.15792087  1.603767
  -2.0803337  -0.05240385  1.8186244  -0.17125948  1.2672967  -2.9876187
  -0.4186283   0.06391733 -1.7076591   0.13764784  0.69433993  0.636185
  -0.22932102 -0.6834483   0.78578115  1.658286    0.63598627 -0.45276263
  -0.4634151   3.182878   -0.3332924  -0.16197528  1.103873    0.23528878
  -1.0472476  -0.65674937  1.6392679   0.40149596  0.75758755  0.58822644
   0.67842174 -0.17567423 -1.2884595   0.9288743  -1.0124061   1.8548989
   1.0391277   1.5186783   0.06097886 -0.31555602 -0.5136976   0.9463446
   0.26989785 -1.7717413  -0.44760793  1.8335481   1.9130577  -2.048313
   0.7831643  -0.34948325  0.7965156   0.5246868  -2.1716087   0.0839141
   0.01447053  0.28565815  1.6692028  -0.70767194 -0.6430934   1.7103003
   0.3310753   0.21110931  1.3576959  -0.24543165  1.2556012  -1.4055291
   1.4686524   0.8910849   1.6688777   0.13971211  1.3775114  -2.2154377
  -1.1158594   1.3466305  -0.3049524  -0.75162    -0.55657434 -1.8890456
   1.7592084   0.7580411  -4.260257   -0.6753136   1.4358966   1.207941
  -1.0288732   0.2221077  -0.71575475  0.6210444  -3.458702    1.0556573
  -0.906298    0.9532037   1.0219775   0.7109268  -3.4150288   1.3232105
   0.51305145  1.362327  ]]

Final Loss: 0.2464
Distance Metric: 36.6805
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 165

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 4.6777181e-02 -5.5197727e-02 -9.5334593e-03 ...  2.2016190e-02
  -1.2907895e-02  3.1144994e-03]
 [-8.9811675e-02  2.8116360e-01  3.7254867e-01 ... -8.1069809e-01
  -1.6405312e+00 -2.6083106e-01]
 [-5.6602663e-01  8.7025738e-01 -8.3234888e-01 ... -2.0956302e-01
  -2.0780873e+00  2.2613835e-01]
 ...
 [-4.3146465e-02  3.3305097e-02 -6.7560049e-04 ... -4.3216371e-03
  -5.8299163e-03  1.5518869e-03]
 [-1.4492525e-01  3.4193028e-02  3.9596606e-02 ... -8.1077047e-02
   2.5755987e-02  3.2530610e-02]
 [-1.3920015e-01 -2.0620275e+00 -5.5890894e-01 ...  1.5810162e+00
   2.8409078e+00  1.8090155e+00]]
layers.1.weight: [[-0.02446059 -0.02103422  0.00256856 ...  0.00451875  0.04386331
   0.00941075]
 [ 0.01411451  0.0008575   0.00454646 ... -0.05417458  0.03052436
   0.00082613]
 [-0.01040959  0.0057468   0.0027962  ...  0.03182139 -0.01005742
   0.00501678]
 ...
 [ 0.03146378 -0.00735978  0.02844404 ... -0.01333634  0.0110734
   0.00127676]
 [-0.01253775 -0.00331891 -0.00554745 ...  0.05577084 -0.03580832
  -0.00275379]
 [-0.04997576  0.01010238 -0.02140643 ...  0.02320739 -0.01103896
  -0.00075402]]
layers.2.weight: [[-9.22977459e-03  2.42661429e-03 -6.13399455e-03 -4.69202176e-03
  -3.16794118e-04 -2.57450214e-04  5.57295466e-03 -6.70167990e-03
  -1.90960267e-03 -1.64236093e-03  5.35462890e-03 -8.88980087e-03
   5.65731991e-03  3.94196110e-03  3.21445870e-03  9.65196081e-03
  -2.90687731e-03  1.04323635e-02 -3.32932291e-03 -8.41504522e-03
  -1.79324731e-01  4.56595561e-03  3.67794331e-04 -4.12748894e-04
   3.37597029e-03  0.00000000e+00 -5.13469428e-03  1.81613374e-03
   1.18310913e-03  2.82316282e-03  3.86063731e-03 -3.18691228e-03
  -8.36258195e-03  7.80323520e-04  1.40384980e-03  1.00234174e-03
  -3.90837155e-03  5.06808981e-03 -7.34120607e-03  1.34980753e-02
  -1.69451311e-02  8.21191911e-03 -4.38752631e-03 -2.27721664e-03
   3.68012604e-03 -1.02751900e-03  2.50262348e-03 -5.91229089e-03
   1.23988924e-04 -6.49619848e-04  5.88589523e-04 -8.91839806e-03
   5.35620516e-03 -2.69100326e-03 -1.38800486e-03  2.48772442e-01
   4.27681487e-03 -1.45479268e-03  6.04092982e-03  1.51783053e-03
  -3.02540185e-03 -2.30194512e-03  3.04777664e-03 -9.14515869e-04
   9.81558301e-03 -2.80527282e-03 -2.21533910e-03 -1.77291420e-03
   0.00000000e+00 -5.67615265e-03 -5.55341830e-03  2.85872433e-04
  -3.06060887e-03  2.77271937e-03  7.29102176e-03 -9.77854524e-03
  -3.02667893e-03  3.75582371e-03 -4.91119735e-03 -6.32012775e-03
   5.90172643e-03  1.49824854e-03  1.35133590e-03  6.94234436e-03
  -2.26017181e-03  1.62065565e-03 -1.83992204e-03  3.06340633e-03
  -5.26964571e-03  8.44772905e-03  4.83496208e-03  6.92135515e-03
  -1.72812154e-03 -2.60476419e-03  3.39060812e-03 -9.29024420e-04
  -1.52395840e-03  1.97566417e-03  7.02175684e-03  3.11092404e-03
   4.31297487e-03  1.57487532e-03  5.69072925e-03 -2.48484151e-03
   7.71480333e-03 -2.65414524e-03  1.42117881e-03  6.87668379e-03
   2.57167662e-03  2.76932959e-03  1.77107356e-03 -3.34689510e-04
   2.19346979e-03 -5.08910045e-03 -1.38181949e-03 -1.98612036e-03
   4.24133148e-03  5.01940446e-03  5.62649360e-03 -3.85915325e-03
  -3.73915024e-03  3.17615643e-03  2.21329130e-04 -1.86886953e-03
   9.38976824e-04 -9.05862823e-03  6.83828769e-03  1.78404001e-03]]

Final Loss: 0.5611
Distance Metric: 77.3680
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[-0.07340379 -0.20428643  0.09751261 ... -0.01757841  0.04505284
   0.07289858]
 [ 0.03197357  0.15490587  0.1570957  ...  0.20183988  0.18364583
   0.00991174]
 [-0.0407703   0.06803036 -0.13763435 ...  0.20366071  0.03167
  -0.06430008]
 ...
 [ 0.1704777   0.17161423 -0.10876656 ...  0.04758384 -0.13278748
   0.02704273]
 [ 0.15817535  0.10653488 -0.04408044 ...  0.14535479 -0.03472062
   0.04529258]
 [ 0.08948188  0.19629951  0.02289757 ...  0.20522647  0.09715472
  -0.08527521]]
layers.1.weight: [[-0.00747414 -0.09157968 -0.1283873  ...  0.07395051  0.06241031
  -0.0023318 ]
 [-0.01260415  0.05927131  0.00506388 ... -0.03432584 -0.05712759
  -0.00355835]
 [-0.04510554  0.00182984  0.15090208 ...  0.10482697  0.03301523
  -0.04504076]
 ...
 [ 0.09505568 -0.13054642  0.06287359 ... -0.04415728 -0.09518126
  -0.12079073]
 [-0.1404724  -0.13307805  0.08081091 ... -0.00704835 -0.00195312
  -0.05016196]
 [-0.00801344 -0.02009209  0.11311361 ... -0.12453824  0.065012
   0.03523101]]
layers.2.weight: [[-0.01989099 -0.09598317 -0.0625     -0.29109603 -0.22213638 -0.04393087
  -0.15644737 -0.01139761 -0.0625     -0.0625     -0.23507725 -0.24750943
  -0.07251824 -0.0625     -0.29561996  0.1477433  -0.29665083 -0.0625
   0.14348851 -0.11557999 -0.24691148 -0.03915269 -0.07776143 -0.09325895
  -0.29469994 -0.15317465  0.09007653 -0.23630519 -0.10735279 -0.0766416
  -0.31612563 -0.24146767 -0.1503765   0.0928139  -0.125      -0.18656123
  -0.18635485 -0.15230836  0.01503903 -0.14848925  0.09746102 -0.27021343
  -0.26989338 -0.19069497 -0.326335   -0.11711478 -0.17654204 -0.17771932
  -0.34820426 -0.22495374 -0.19810133 -0.30130005  0.06749396 -0.03343462
  -0.16775446 -0.15619683  0.00407025 -0.19766861  0.05530325 -0.06668722
  -0.03284487 -0.21808013 -0.22478294 -0.08656193  0.07794749  0.08124599
  -0.25486347 -0.21179503  0.0047032  -0.1466363   0.07085458 -0.2222242
  -0.07828181 -0.03639515 -0.2739285  -0.18998837 -0.29574808 -0.03125004
  -0.10637448  0.02067459 -0.2250905  -0.11191546 -0.25678015 -0.18830533
   0.02655235 -0.24160138 -0.00205111 -0.13089852 -0.21797724 -0.06250007
  -0.0625      0.01128769 -0.05949776  0.14851704 -0.14505967  0.065947
   0.02518478  0.13360202  0.01488471 -0.25       -0.08042496 -0.02514356
   0.02698082 -0.11574949 -0.14178656 -0.18158898 -0.24261867  0.0290069
  -0.18889152  0.1334053  -0.0819207  -0.01414332 -0.28307864 -0.14943974
  -0.25080475 -0.0625     -0.25861853 -0.15352355 -0.25       -0.15833181
  -0.13011931  0.07135262 -0.1943531  -0.0625     -0.10542192 -0.18769783
   0.09469483 -0.25      ]]

Final Loss: 0.0000
Distance Metric: 20.6536
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 8.02710876e-02  1.44915674e-02 -1.63922131e-01 ...  2.93910940e-04
  -1.02461979e-01 -1.24583766e-01]
 [-6.63318336e-02 -7.57955983e-02 -1.52743638e-01 ... -1.21586792e-01
   1.24920644e-01  6.30331784e-02]
 [ 3.00073177e-02 -2.41982192e-01  8.15854445e-02 ...  1.10815272e-01
  -5.38569279e-02 -9.22459140e-02]
 ...
 [-6.50016740e-02  3.00456993e-02  1.80467069e-01 ...  3.16609055e-01
   5.17415591e-02  3.51708196e-02]
 [-5.79835996e-02 -2.30914533e-01  9.90576595e-02 ... -1.10977076e-01
  -1.04912348e-01  1.26327828e-01]
 [ 2.59108126e-01  3.28011602e-01 -3.43044586e-02 ... -1.13611864e-02
  -1.20521016e-01  2.46130913e-01]]
layers.1.weight: [[-0.03843059 -0.07411095  0.03302361 ... -0.05769965 -0.03324822
   0.01214757]
 [ 0.09732711  0.06899348  0.2546644  ...  0.13806234  0.13227463
   0.12436928]
 [ 0.08541242 -0.08378508  0.13356328 ... -0.06206346 -0.20679866
  -0.02940246]
 ...
 [ 0.18087235 -0.00436882 -0.21015404 ...  0.29682666  0.21679008
   0.05875359]
 [ 0.04019242  0.01313201 -0.16690396 ...  0.01105545 -0.00459289
   0.19250928]
 [ 0.01418847  0.10172563 -0.04880941 ...  0.10808396 -0.11432967
  -0.04705658]]
layers.2.weight: [[-5.6854379e-01 -2.0505667e+00  1.5312713e+00  2.5843006e-02
  -9.2452675e-01 -1.1736201e+00  1.2998176e+00  2.3891308e+00
  -2.4105966e+00 -3.8541007e-01  1.4248177e+00  4.9038297e-01
  -4.7354972e-01 -1.8506351e+00  2.6794752e-01 -1.8328421e-01
   5.1814443e-01 -1.2735959e+00  2.6788979e+00  5.2016622e-01
  -9.1558760e-01 -2.2674806e+00  1.5050116e+00  1.8569579e+00
   4.0738800e-01  1.8582045e+00 -2.3283710e+00  1.4503611e+00
  -8.6242825e-02 -8.4035450e-01  5.7557166e-01  7.7085578e-01
   9.6720880e-01  2.0746753e+00  1.4707708e+00 -8.1052566e-01
   9.8560601e-01 -1.5302047e+00 -5.1343971e-01 -4.7925800e-01
  -5.7562947e-01  3.1542659e+00  9.3478763e-01  9.5794678e-01
   1.2951881e-01  7.6931320e-02  2.3999388e+00  1.6087615e+00
   8.0118112e-02 -2.3425831e-01  1.1651181e-01 -4.4344446e-01
  -1.7416042e+00  3.8052610e-01 -1.0777746e-03 -1.0126833e+00
  -3.0719700e-01  1.1925547e+00 -1.2207901e+00 -4.4829074e-01
  -2.6121531e+00  1.9126848e+00  1.2670147e+00  2.6504570e-01
  -2.6747062e+00 -1.2416482e+00  7.1408242e-01 -8.1544596e-01
   2.1557260e+00 -7.0749789e-01 -3.3067129e+00  7.6858455e-01
  -1.2291423e+00 -3.1758366e+00  3.8914572e-02  6.9412982e-01
   4.1083810e-01 -1.1951942e+00  1.0223559e+00 -6.4381629e-01
  -3.3990359e-01 -1.5265205e+00 -7.8317636e-01 -1.9493817e-01
  -2.1084969e-01  6.3869554e-01 -2.0272067e+00  1.4063612e+00
   8.8469285e-01 -5.1586330e-01 -4.4509637e-01 -1.0754803e+00
  -3.0963655e+00 -1.0025340e-01  6.6054600e-01 -3.0638558e-01
  -2.9722760e+00 -3.0171704e+00  1.6165661e+00  7.3295546e-01
  -1.3931130e+00  2.0392914e+00 -3.1568289e+00  1.3261520e+00
   6.6911894e-01  1.7502115e+00  1.7203968e+00 -2.5771925e-01
   7.2916001e-01 -8.0843490e-01 -8.0973154e-01 -9.3398297e-01
  -6.1037332e-01  1.0113212e+00  3.5550509e-02 -6.7591912e-01
  -8.2991117e-01 -9.6112639e-01  6.9566715e-01  1.2504160e+00
   1.8935022e-01 -9.7236490e-01  9.1789567e-01 -2.1529224e+00
  -3.5604575e-01  1.2747890e+00 -2.9609665e-01  8.5529399e-01]]

Final Loss: 0.0000
Distance Metric: 38.4735
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 101

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.13937828 -0.04018909 -0.18278165 ... -0.06732511  0.26085028
  -0.18188997]
 [ 0.21418521  0.19298464  0.19374938 ... -0.28291535  0.25509533
  -0.16009901]
 [-0.31683815  0.02504805  0.26634532 ... -0.01097285 -0.2236121
   0.24830873]
 ...
 [ 0.30505908  0.00497594  0.02375509 ...  0.08428364  0.25263795
   0.15622312]
 [-0.31327537 -0.13231298  0.06402562 ...  0.19312793 -0.14195396
   0.14269106]
 [ 0.20472182  0.00854245  0.01359031 ...  0.25361463  0.34066376
   0.12804602]]
layers.1.weight: [[-0.18607211 -0.10766141  0.12502721 ...  0.18087667 -0.15602453
   0.2229189 ]
 [ 0.09529556  0.09102985  0.2715429  ... -0.25478378  0.0988283
   0.0367255 ]
 [-0.27222502 -0.00333104  0.24831606 ...  0.08562901 -0.17284864
   0.17324683]
 ...
 [ 0.20934476  0.18966241 -0.17232095 ...  0.14124168  0.17929643
  -0.08336439]
 [ 0.06944627  0.1175018  -0.03206765 ... -0.24132138  0.24862234
   0.0499942 ]
 [-0.10301612  0.14698641  0.2544397  ... -0.15016069  0.04279526
  -0.21691747]]
layers.2.weight: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
  0. 0. 0. 0. 0. 0. 0. 0.]]

Final Loss: 0.0000
Distance Metric: 26.9673
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 228

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[ 0.27141023  0.0485522  -0.04456174 ... -0.14636523 -0.15993197
   0.17986527]
 [-0.6273097  -0.06461746  0.50625986 ... -0.24487108 -0.07903804
   0.08027519]
 [ 1.7099063   0.07631083 -1.0873492  ...  0.13083185 -0.04128087
  -0.08760711]
 ...
 [ 0.07354385 -0.08160374  0.05698039 ...  0.12289955 -0.05529373
   0.20981875]
 [ 0.13008176  0.01036379 -0.04689338 ... -0.406319   -0.02922826
   0.2373656 ]
 [ 0.6786947   0.12670675 -0.45439142 ...  0.02870951 -0.08118116
  -0.12413323]]
layers.1.weight: [[ 0.03983689 -0.10620716  0.02068548 ... -0.10164041 -0.25178123
   0.14711058]
 [ 0.0401036   0.06618892 -0.11302287 ...  0.07192508  0.05168752
  -0.12069032]
 [ 0.07461897  0.08709685 -0.13039304 ... -0.09631968  0.054237
   0.13298182]
 ...
 [-0.09884264  0.1946198  -0.22720292 ...  0.12321935 -0.5173811
  -0.0493405 ]
 [-0.11440703  0.11734185 -0.06589068 ...  0.08203491 -0.04402935
  -0.01533981]
 [-0.10936779  0.01635952 -0.01518115 ...  0.14665173 -0.13469489
   0.11517408]]
layers.2.weight: [[ 1.97770923e-01  9.99066755e-02  2.10579447e-02 -3.15055192e-01
  -7.84813821e-01  3.36583287e-01  1.80951685e-01  2.26768112e+00
  -2.31185853e-02 -2.89066762e-01 -5.69306672e-01  8.25157166e-02
  -4.77014661e-01 -4.74055588e-01 -4.25790399e-02 -6.19835377e-01
   1.67200878e-01  1.63008302e-01 -1.90058008e-01  6.65481329e-01
  -1.06791034e-02 -7.02220201e-01 -7.04302549e-01 -1.05640435e+00
  -1.26231599e+00 -5.51456988e-01 -7.25119591e-01 -6.34745955e-01
  -4.80076015e-01 -1.48158371e-01 -1.85152039e-01 -1.25749901e-01
  -3.78007829e-01 -3.07246089e-01 -4.17208999e-01 -1.40913129e-01
  -2.28637362e+00  6.04488142e-02 -1.18670464e-02  5.93336403e-01
  -3.82858902e-01 -4.34249192e-01 -3.83402199e-01 -3.50148439e-01
  -1.85410941e+00  1.62989691e-01 -1.09596002e+00 -3.30024898e-01
   8.12963173e-02 -4.05261695e-01  5.52825153e-01 -3.49557579e-01
  -4.06147033e-01 -8.70436847e-01  8.06104913e-02  1.72217742e-01
  -4.82823789e-01 -1.88490525e-01  2.44411051e-01 -4.18517977e-01
  -3.23249042e-01  2.26879027e-02 -7.35404968e-01  2.52064735e-01
  -1.38338313e-01  1.62969738e-01 -2.48661280e-01 -5.58783591e-01
  -2.67646578e-03 -7.08484530e-01 -3.05212259e-01 -2.47401327e-01
  -3.13485980e-01 -1.22988451e+00 -8.21151510e-02 -4.19392824e+00
   3.56072396e-01 -1.40756917e+00 -3.43286425e-01 -1.58104241e-01
   2.67426938e-01 -8.16189468e-01  1.13444865e-01 -7.27120489e-02
   9.93771255e-02  8.51375461e-01  4.84802067e-01  1.75469339e-01
   5.47018684e-02 -2.80757934e-01 -2.45598650e+00 -1.83674604e-01
  -2.16411158e-01  1.07399356e+00 -8.00218773e+00  1.20106831e-01
   2.87330568e-01 -1.61264467e+00 -7.75975823e-01 -4.68694896e-01
   5.80720365e-01  2.62081921e-01 -7.63511419e-01  8.32560599e-01
  -7.69292951e-01 -5.80603108e-02 -7.89749980e-01 -1.04212336e-01
   1.83039561e-01 -1.75410852e-01  4.03653711e-01  1.26913294e-01
  -1.80070445e-01 -9.06183794e-02 -4.83429283e-01  4.89823557e-02
   1.66313887e-01 -5.98293662e-01  4.18157689e-02 -9.64276940e-02
  -4.98536170e-01 -1.09501943e-01  7.19936430e-01  1.08865686e-01
  -1.03796087e-02  1.01831973e+00 -3.90192896e-01 -4.75792214e-02]]

Final Loss: 0.2405
Distance Metric: 47.0407
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.11655267 -0.0474504   0.09075455 ...  0.08717899 -0.05307843
  -0.21926317]
 [ 0.06453905 -0.01253031  0.00089773 ...  0.01114031  0.28500584
   0.24973339]
 [-0.17568468  0.05666981 -0.119994   ... -0.03849337  0.2811662
  -0.09995499]
 ...
 [ 0.03735263  0.08768856  0.09535676 ... -0.24634077 -0.16565588
  -0.08907364]
 [ 0.23938106 -0.00570354 -0.30776864 ...  0.19800591  0.147364
  -0.16115661]
 [-0.02435207 -0.04464683  0.28194532 ... -0.07704394 -0.0855812
  -0.08079359]]
layers.1.weight: [[-0.07207768 -0.11437273  0.05469375 ... -0.1000428   0.07099084
   0.05474677]
 [-0.04706342 -0.3276072  -0.00954898 ... -0.0992619  -0.20518106
   0.09052828]
 [-0.22682266 -0.03459144 -0.27237228 ... -0.02548444  0.06314423
  -0.09269425]
 ...
 [-0.34764802 -0.00741244  0.14231908 ...  0.10056052 -0.06199341
  -0.30711743]
 [ 0.10741888  0.03296656  0.21903126 ...  0.1355762  -0.00266409
   0.12857099]
 [-0.1451927  -0.2455334  -0.21250491 ... -0.07650696 -0.22248763
   0.16775407]]
layers.2.weight: [[-0.1420083   0.20626125 -0.9102273  -0.51638246  1.8340696   0.16467428
  -0.2597297  -1.3985081  -0.09706987 -1.3385583  -1.2547364  -2.0630069
   0.65017396  0.5867499  -0.5374739   1.6998805   2.405929    1.6889793
  -0.26700774  0.5970228  -1.0540577   0.89352125  0.073957   -0.37041795
  -0.14075819  1.3421618  -1.7532972  -0.17526418  0.14806822  1.6022577
  -2.0860887  -0.05462964  1.8136657  -0.17533785  1.2627465  -3.006881
  -0.41926172  0.06561262 -1.7211852   0.13234422  0.6714423   0.6310847
  -0.23031716 -0.68168163  0.782615    1.6556461   0.6300081  -0.44955194
  -0.4708182   3.1812484  -0.33325884 -0.16226164  1.0891219   0.2363954
  -1.0504599  -0.6673818   1.6395624   0.3919223   0.75588256  0.5900995
   0.66004634 -0.18781358 -1.2960174   0.923322   -1.0158391   1.8588316
   1.0372831   1.511052    0.06013213 -0.3185033  -0.5253451   0.929178
   0.2781797  -1.7757609  -0.44827506  1.8367338   1.9014843  -2.0500307
   0.769358   -0.35218117  0.79619217  0.51738995 -2.1758738   0.07085492
   0.01611228  0.2774738   1.6666098  -0.71745574 -0.64275604  1.6971792
   0.33137178  0.20952614  1.3423984  -0.26195064  1.2497357  -1.4102322
   1.4684883   0.8673877   1.6728199   0.1405125   1.3532486  -2.2222548
  -1.1180273   1.3463783  -0.31515238 -0.7685825  -0.5612257  -1.8916469
   1.7606097   0.7559965  -4.262054   -0.6742304   1.4226549   1.2044629
  -1.0287582   0.21215327 -0.7221177   0.6112748  -3.4685433   1.0494862
  -0.9178454   0.9473018   1.026123    0.7101884  -3.4284759   1.3159125
   0.49849582  1.357866  ]]

Final Loss: 0.6091
Distance Metric: 38.0889
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 163

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.13219108 -0.22954416  0.1390733  ... -0.10843293 -0.00048903
   0.16234995]
 [ 0.18259238 -0.37890297  0.48638055 ...  0.01941292 -0.00686579
  -0.00986803]
 [ 0.17373718 -0.324931    0.39948142 ... -0.03434701 -0.02322017
  -0.03223474]
 ...
 [ 0.15306714 -0.07912783  0.07401331 ... -0.13045593 -0.11677534
  -0.04917333]
 [-0.02911222 -0.01817658 -0.0060507  ... -0.00203911 -0.04697286
   0.0073286 ]
 [-0.02839422  0.01515059 -0.22777714 ... -0.00280886 -0.04291252
  -0.09677036]]
layers.1.weight: [[-0.2294509  -0.10319566  0.2059812  ... -0.00963759  0.2303992
   0.13111348]
 [-0.2090177  -0.18418743 -0.03537584 ...  0.2511549  -0.1541001
  -0.0840436 ]
 [ 0.02432026 -0.062427   -0.13419062 ...  0.00519072  0.22373626
  -0.06774246]
 ...
 [-0.02257591  0.17492558  0.14447333 ... -0.00340105 -0.10162745
  -0.16868146]
 [ 0.01969456 -0.06643499 -0.01688265 ...  0.1332076  -0.2286908
   0.15659168]
 [ 0.12250759 -0.07196812 -0.22722307 ...  0.14484556 -0.09381828
   0.18231782]]
layers.2.weight: [[ 3.5941057e-04 -6.2314619e-04 -6.4309915e-03  1.1028986e-03
  -8.7532038e-03  1.3078493e-03 -2.4857044e-03  1.5147825e-03
  -1.1808237e-03 -1.4548912e-03 -2.2898575e-03 -6.2291150e-04
  -2.9068018e-04 -5.8842860e-03  2.4357077e-03  1.7217953e-04
   7.4393710e-04  2.1457344e-03  8.4715504e-03  1.7104034e-01
   6.7587439e-03  4.3208469e-03  2.5726363e-01 -3.1460135e-03
   2.0229404e-03 -9.9259906e-04  7.7180745e-04  0.0000000e+00
   0.0000000e+00  9.9392550e-04 -2.1279257e-04 -1.6346628e-02
  -1.0328572e-03  1.3633381e-03 -6.6575040e-03  2.1784910e-04
  -1.6356690e-03  4.0325909e-03 -1.0951079e-03 -2.7177608e-01
   2.5769714e-03 -6.1616418e-04  4.4298288e-03 -4.0123981e-04
  -7.7340910e-03 -1.7927436e-03  1.9266887e-04  7.0398632e-03
   2.2535028e-03 -6.2645879e-04 -8.0962507e-03  1.7037375e-03
  -3.2594118e-03 -8.1206131e-04  3.0719689e-03  4.4141589e-03
  -5.8294092e-03  1.3381729e-03 -9.3622546e-04 -1.5257660e-03
   1.0740538e-01 -9.9352738e-03 -5.5855152e-04 -2.7493061e-04
   2.0303298e-03  3.1252010e-03  2.1287873e-02 -1.2392887e-02
   8.8172834e-03  2.7245369e-03  4.3957835e-04 -9.9633113e-03
  -6.0733658e-04  2.2688261e-03 -1.7785449e-02  7.1657787e-04
   7.2645856e-04  1.8720101e-03  6.5935552e-03 -1.5128677e-01
   2.1445140e-02 -4.6258145e-03 -1.0575667e-02 -4.5689463e-04
   3.4453224e-03 -1.7555227e-03 -5.9028333e-03  0.0000000e+00
   6.9037899e-02 -6.4612837e-03 -2.8109602e-03  3.2895785e-03
  -7.1859831e-04 -8.6797046e-04  1.8257217e-02 -9.0935326e-04
   4.6285968e-03  4.2939833e-03  0.0000000e+00 -2.8829326e-04
   3.2128415e-03  2.1032176e-03  4.5787383e-04  0.0000000e+00
   6.6433346e-04  5.9851822e-02 -1.8506538e-02  7.3916244e-04
   2.3334490e-03  5.8768839e-03 -4.3581533e-03  2.7873646e-03
  -4.5714807e-04  1.4263230e-03 -1.5760937e-01 -2.1299429e-02
   4.1295195e-01 -9.7447232e-04 -7.6642120e-04  1.3470388e-03
   4.7102217e-03 -3.2545670e-03  1.2100436e-03 -8.8367105e-04
  -1.5496650e-01  0.0000000e+00  2.4809134e-03 -4.3918699e-04]]

Final Loss: 0.0000
Distance Metric: 26.6230
L1 norm: 0
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 5
stopped after epoch: 1999

================================================================================

