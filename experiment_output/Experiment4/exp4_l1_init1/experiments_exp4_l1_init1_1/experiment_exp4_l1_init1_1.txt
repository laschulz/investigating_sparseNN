Experiment Summary:
================================================================================

Teacher Model Parameters:
layers.0.weight: [[[ 2.59 -2.83  0.87]]]
layers.1.weight: [[[-1.38  1.29]]]
layers.2.weight: [[[ 0.86 -0.84]]]

================================================================================

baselineCNN_sigmoid -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.        -1.4842379
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   1.6567944  0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.        -1.3356997  0.         0.         0.
   0.         0.       ]]

Final Loss: 0.0004
Distance Metric: 6.5298
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1026

================================================================================

baselineCNN_sigmoid -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [-0.02741959  0.04641664  0.05464554 ...  0.03201418  0.06318928
   0.01986424]
 [ 0.06388114  0.00222856  0.06042385 ... -0.02905862  0.05604905
   0.02166045]
 ...
 [-0.02576774 -0.031088   -0.12265    ...  0.00063794  0.06252552
   0.01074728]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.54398364 0.
  0.         0.         0.         0.         0.40940964 0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.47399226 0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.39542994 0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.43137002
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.93185085 0.         0.
  0.         0.         0.         0.459108   0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.38459188 0.         0.         0.
  0.47305316 0.         0.         0.         0.         0.
  0.         0.53227264 0.         0.         0.         0.
  0.         0.        ]]

Final Loss: 0.0110
Distance Metric: 9.7387
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1898

================================================================================

baselineCNN_sigmoid -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.         0.         0.         0.         0.         0.
  0.11007563 0.        ]]

Final Loss: 0.2525
Distance Metric: 8.2571
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 299

================================================================================

baselineCNN_relu -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 2.2222687e-04  2.2223232e-04  2.2214405e-04  2.2152145e-04
   2.2194561e-04  1.0593561e+00  2.2260932e-04  2.2280241e-04
   2.2241488e-04  2.2193923e-04  2.2187596e-04  2.2156198e-04
   2.3365164e+00  2.2233686e-04  2.2209386e-04  2.2215123e-04
   2.2229832e-04  2.2223617e-04  2.2241780e-04  2.2209101e-04
   2.2211469e-04  2.2200713e-04  2.2217711e-04  2.2232808e-04
   2.2219890e-04  2.2285451e-04  2.2218878e-04  2.2219286e-04
   2.2193557e-04  2.2203162e-04  2.2159347e-04  2.2153828e-04
   2.2150908e-04  2.2242131e-04  2.2190112e-04  2.2261566e-04
   2.2220466e-04  2.2246048e-04  2.2239507e-04  2.2192944e-04
   2.2223579e-04  2.2265765e-04  2.2217700e-04  2.2203577e-04
   2.2216763e-04  2.2150455e-04  2.2276190e-04  2.2274186e-04
  -1.9467524e+01  2.2201157e-04  1.0944250e+01  2.2265605e-04
   2.2262384e-04  2.2246251e-04  2.2231365e-04  2.2224695e-04
   2.2227544e-04  2.2201310e-04  2.2246182e-04  2.2274625e-04
   2.2159850e-04  2.2266446e-04  2.2262643e-04  2.2201570e-04
   2.2275263e-04  2.2153727e-04  2.2196781e-04 -1.3442640e+01
   2.2243420e-04  2.2147830e-04  2.2214721e-04  2.2209718e-04
   2.2236040e-04  2.2282620e-04  2.2269532e-04  2.2149408e-04
   2.2153325e-04  2.2280446e-04  2.2159322e-04  2.2221087e-04
   2.2266191e-04  2.2283134e-04  2.2147992e-04  2.2239811e-04
   2.2204289e-04  2.2151350e-04  2.2216888e-04  2.2211144e-04
   2.2267425e-04  2.2154245e-04  2.2263854e-04  2.2186074e-04
   2.2189989e-04  2.2193922e-04  2.2283384e-04  2.2186604e-04
   2.2148117e-04  2.2282079e-04  2.2238417e-04  2.2224417e-04
   2.2285787e-04  2.2245677e-04  2.2196170e-04  2.2233049e-04
   2.2212548e-04  2.2223308e-04  2.2189927e-04  2.2225613e-04
   2.2200498e-04  2.2158009e-04  2.2269040e-04  2.2230933e-04
   2.2262246e-04  2.2280996e-04  2.2235706e-04  2.2221166e-04
   7.7980537e+00  2.2188944e-04  2.2225421e-04  2.2283748e-04
   2.2239207e-04  2.2230562e-04  2.2240763e-04  2.2224143e-04
   2.2273975e-04  2.2190226e-04  2.2199235e-04  2.2233416e-04]]

Final Loss: 3.3189
Distance Metric: 61.2364
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 3796

================================================================================

baselineCNN_relu -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.        -2.1633205  0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   2.8276446  0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.       ]]

Final Loss: 0.0002
Distance Metric: 11.7292
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1618

================================================================================

baselineCNN_relu -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.1170496   0.15752001  0.19683251 ...  0.1553919  -0.1926928
   0.00567743]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 ...
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [ 0.12739989 -0.36037603 -0.13428047 ...  0.20189679  0.396329
  -0.24201529]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.          0.          0.          0.          0.          0.
   1.0732273   0.          0.29416552  0.         -0.2575487   0.4085578
   0.          0.          0.         -0.4051275  -0.34515816  0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
  -0.47910684  0.         -1.0443773   0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
  -0.20320305  0.          0.          0.          0.2922605   0.
   0.3419786   0.          0.          0.          0.46885404  0.
   0.          0.          0.          0.          0.          0.
   0.         -0.33355787  0.          0.          0.          0.
  -0.3620631   0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.46195477  0.
   0.          0.50029457  0.          0.57023734  0.          0.
   0.          0.          0.          0.          0.          0.
   0.         -0.667072    0.          0.          0.          0.
   0.          0.        ]]

Final Loss: 3.5593
Distance Metric: 34.0173
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1347

================================================================================

baselineCNN_tanh -> fcn_128_128_tanh

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[ 0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         1.886776   0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
   0.         0.         0.         0.         0.         0.
  -0.5968183  0.       ]]

Final Loss: 0.0001
Distance Metric: 3.3208
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1283

================================================================================

baselineCNN_tanh -> fcn_128_128_relu

Student Model Parameters:
layers.0.weight: [[ 0.          0.          0.         ...  0.          0.
   0.        ]
 [-0.05518591  0.02182916 -0.12442584 ...  0.00540818  0.06998082
  -0.13210404]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 ...
 [-0.11790115 -0.07653963  0.0836812  ... -0.01786366  0.02405663
   0.00299244]
 [ 0.          0.          0.         ...  0.          0.
   0.        ]
 [-0.0254953   0.04511011 -0.0954234  ... -0.198248    0.45093474
  -0.741393  ]]
layers.1.weight: [[0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.00321499 0.         ... 0.07225826 0.         0.06791671]
 ...
 [0.         0.         0.         ... 0.         0.         0.        ]
 [0.         0.003488   0.         ... 0.04678638 0.         0.00212755]
 [0.         0.         0.         ... 0.         0.         0.        ]]
layers.2.weight: [[ 0.          0.         -0.72305095  0.          0.          0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.49187714  0.         -0.7530011   0.
   0.          0.42410392  0.          0.          0.          0.
   0.          0.          0.         -1.0161415   0.59332985  0.
   0.          0.         -0.63443017  0.          0.          0.
   0.          0.          0.38646907  0.          0.          0.
   0.          0.          0.          0.2733406   0.42761263  0.
   0.          0.          0.          0.427159    0.          0.
   0.          0.          0.          0.         -0.3992425   0.6013275
   0.          0.         -0.38128185  0.          0.          0.
   0.          0.          0.          0.          0.4704219  -0.87665427
   0.         -1.2763817   0.          0.          0.          0.
   0.          0.          0.          0.          0.          0.
  -0.9174893   0.          0.          0.          0.27734667  0.
   0.          0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.35903904  0.
  -0.66261625  0.          0.          0.          0.          0.
   0.          0.          0.          0.          0.42533514  0.
  -1.1413918   0.          0.          0.          0.          0.
   0.4841237   1.3449955   0.         -1.149195    0.          0.
  -0.6480361   0.        ]]

Final Loss: 0.2907
Distance Metric: 13.6535
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 1148

================================================================================

baselineCNN_tanh -> fcn_128_128_sigmoid

Student Model Parameters:
layers.0.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.1.weight: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]
layers.2.weight: [[  0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.         -6.374919    0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          3.7272217
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.        -11.686847    0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.          0.          0.          0.          0.
    0.          0.       ]]

Final Loss: 0.1459
Distance Metric: 36.2939
L1 norm: 1e-05
L2 norm: 0
Batch size: 16
Clipping: 0
Learning rate: 0.005
data size: 100000
init: 1
seed: 1
stopped after epoch: 6289

================================================================================

